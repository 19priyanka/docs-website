{
  "/docs/infrastructure/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-10-25T00:44:10Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.17523,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-10-24T20:51:20Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.73492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    },
    {
      "sections": [
        "ECS integration troubleshooting: Generate verbose logs",
        "Problem",
        "Solution",
        "Using task definition environment variable",
        "Retrieve logs via SSH (EC2 launch type only)",
        "Forward logs to CloudWatch and download them with awscli",
        "From running container"
      ],
      "title": "ECS integration troubleshooting: Generate verbose logs",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "06198f1b2e0faa69bd8a7dfb93f18c8955fea83b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs/",
      "published_at": "2021-10-24T22:06:42Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting the on-host ECS integration, you can generate verbose logs for a few minutes to find and investigate errors. This can be useful for conducting your own troubleshooting or when providing information to New Relic support. Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. You can automate this process by using the newrelic-infra-ctl command. For more information, see Troubleshooting a running agent. Solution Generating verbose log files requires editing your task definition file. For a sample config file that includes all applicable settings, see Infrastructure configuration settings. You have several options for implementing verbose logs: Change the task definition environment variable and do a task restart For EC2 launch type: Retrieve logs via SSH Forward to CloudWatch and download with awscli Run a command from the running container Using task definition environment variable To enable verbose logs by changing the environment variable and doing a task restart: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. Save your task definition. Update your service to use the newly registered task definition. If you chose NRIA_VERBOSE=3 and you're not sending the logs directly to New Relic, you have two options for viewing and downloading the logs: For EC2 launch type: you can retrieve the logs via SSH, or Forward logs to CloudWatch Return your settings to default: Disable verbose logging by editing your task definition and setting NRIA_VERBOSE to 0. Save your task definition. Update your service to the latest version of your task. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file. Retrieve logs via SSH (EC2 launch type only) To get logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your container instances. Find the container ID of the New Relic integration container, by running the command docker ps -a. The name of the container should be nri-ecs. Save the logs from the container with the command docker logs NRI_ECS_CONTAINER_ID > logs.txt. Leave the command running for about three minutes to generate sufficient logging data. Continue with the instructions in the enable verbose logs section. Forward logs to CloudWatch and download them with awscli To get logs via CloudWatch: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. We use a CloudWatch log group called /newrelic-infra/ecs to forward the logs to. To see if it already exists, run: aws logs describe-log-groups --log-group-name-prefix /newrelic-infra/ecs Copy If a log group exists with that prefix, you'll get this output: { \"logGroups\": [ { \"logGroupName\": \"/newrelic-infra/ecs\", \"creationTime\": 1585828615225, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:YOUR_REGION:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:*\", \"storedBytes\": 122539356 } ] } Copy Because this command matches log groups with prefixes, ensure the log group name returned is exactly /newrelic-infra/ecs. If the log group doesn't exist, the output will be: { \"logGroups\": [] } Copy If the log group doesn't exist, create it by running: aws logs create-log-group --log-group-name /newrelic-infra/ecs Copy Edit your task definition. In the container definition for the newrelic-infra container, add the following logConfiguration: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"/newrelic-infra/ecs\", \"awslogs-region\": \"AWS_REGION_OF_YOUR_CLUSTER\", \"awslogs-stream-prefix\": \"verbose\" } } Copy Register the new task version and update your service. Next you'll look for the relevant log stream. If you have multiple instances of the task running, they'll all send their logs to the same log group but each will have its own log stream. Log streams names follow the structure AWSLOGS_STREAM_PREFIX/TASK_FAMILY_NAME/TASK_ID. In this case, it will be verbose/newrelic-infra/TASK_ID. To get all the log streams for a given log group, run this command: aws logs describe-log-streams --log-group-name /newrelic-infra/ecs Copy The following is an example output of a log group with two streams: { \"logStreams\": [ { \"logStreamName\": \"verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"creationTime\": 1586166741197, \"firstEventTimestamp\": 1586166742030, \"lastEventTimestamp\": 1586173933472, \"lastIngestionTime\": 1586175101220, \"uploadSequenceToken\": \"49599989655680038369205623273330095416487086853777112338\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"storedBytes\": 0 }, { \"logStreamName\": \"verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"creationTime\": 1586166745643, \"firstEventTimestamp\": 1586166746491, \"lastEventTimestamp\": 1586173037927, \"lastIngestionTime\": 1586175100660, \"uploadSequenceToken\": \"49605664273821671319096446647846424799651902350804230514\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"storedBytes\": 0 } ] } Copy From the previous list of log streams, identify the one with the task ID for which you want to retrieve the logs and use the logStreamName in this command: aws logs get-log-events --log-group-name /newrelic-infra/ecs --log-stream-name \"LOG_STREAM_NAME\" --output text > logs.txt Copy Continue with the enable verbose logs instructions. From running container To enable verbose logs by running a command from the running container: SSH into one of your container instances. Find the container ID of the New Relic integration container by running the command docker ps -a. The name of the container should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec INTEGRATION_CONTAINER_ID /usr/bin/newrelic-infra-ctl Copy For more details, see Troubleshoot the agent. Save the logs from the container with the command docker logs INTEGRATION_CONTAINER_ID > logs.txt Copy Leave the command running for about three minutes to generate sufficient logging data. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.64684,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "sections": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Retrieve logs via SSH (EC2 launch type only) To <em>get</em> logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your <em>container</em> instances. Find"
      },
      "id": "604507f9196a67c1ae960f5e"
    }
  ],
  "/docs/infrastructure/elastic-container-service-integration/installation/install-ecs-integration": [
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-10-24T20:51:20Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.62321,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-10-25T00:44:10Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.17523,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "ECS integration troubleshooting: Generate verbose logs",
        "Problem",
        "Solution",
        "Using task definition environment variable",
        "Retrieve logs via SSH (EC2 launch type only)",
        "Forward logs to CloudWatch and download them with awscli",
        "From running container"
      ],
      "title": "ECS integration troubleshooting: Generate verbose logs",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "06198f1b2e0faa69bd8a7dfb93f18c8955fea83b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs/",
      "published_at": "2021-10-24T22:06:42Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting the on-host ECS integration, you can generate verbose logs for a few minutes to find and investigate errors. This can be useful for conducting your own troubleshooting or when providing information to New Relic support. Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. You can automate this process by using the newrelic-infra-ctl command. For more information, see Troubleshooting a running agent. Solution Generating verbose log files requires editing your task definition file. For a sample config file that includes all applicable settings, see Infrastructure configuration settings. You have several options for implementing verbose logs: Change the task definition environment variable and do a task restart For EC2 launch type: Retrieve logs via SSH Forward to CloudWatch and download with awscli Run a command from the running container Using task definition environment variable To enable verbose logs by changing the environment variable and doing a task restart: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. Save your task definition. Update your service to use the newly registered task definition. If you chose NRIA_VERBOSE=3 and you're not sending the logs directly to New Relic, you have two options for viewing and downloading the logs: For EC2 launch type: you can retrieve the logs via SSH, or Forward logs to CloudWatch Return your settings to default: Disable verbose logging by editing your task definition and setting NRIA_VERBOSE to 0. Save your task definition. Update your service to the latest version of your task. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file. Retrieve logs via SSH (EC2 launch type only) To get logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your container instances. Find the container ID of the New Relic integration container, by running the command docker ps -a. The name of the container should be nri-ecs. Save the logs from the container with the command docker logs NRI_ECS_CONTAINER_ID > logs.txt. Leave the command running for about three minutes to generate sufficient logging data. Continue with the instructions in the enable verbose logs section. Forward logs to CloudWatch and download them with awscli To get logs via CloudWatch: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. We use a CloudWatch log group called /newrelic-infra/ecs to forward the logs to. To see if it already exists, run: aws logs describe-log-groups --log-group-name-prefix /newrelic-infra/ecs Copy If a log group exists with that prefix, you'll get this output: { \"logGroups\": [ { \"logGroupName\": \"/newrelic-infra/ecs\", \"creationTime\": 1585828615225, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:YOUR_REGION:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:*\", \"storedBytes\": 122539356 } ] } Copy Because this command matches log groups with prefixes, ensure the log group name returned is exactly /newrelic-infra/ecs. If the log group doesn't exist, the output will be: { \"logGroups\": [] } Copy If the log group doesn't exist, create it by running: aws logs create-log-group --log-group-name /newrelic-infra/ecs Copy Edit your task definition. In the container definition for the newrelic-infra container, add the following logConfiguration: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"/newrelic-infra/ecs\", \"awslogs-region\": \"AWS_REGION_OF_YOUR_CLUSTER\", \"awslogs-stream-prefix\": \"verbose\" } } Copy Register the new task version and update your service. Next you'll look for the relevant log stream. If you have multiple instances of the task running, they'll all send their logs to the same log group but each will have its own log stream. Log streams names follow the structure AWSLOGS_STREAM_PREFIX/TASK_FAMILY_NAME/TASK_ID. In this case, it will be verbose/newrelic-infra/TASK_ID. To get all the log streams for a given log group, run this command: aws logs describe-log-streams --log-group-name /newrelic-infra/ecs Copy The following is an example output of a log group with two streams: { \"logStreams\": [ { \"logStreamName\": \"verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"creationTime\": 1586166741197, \"firstEventTimestamp\": 1586166742030, \"lastEventTimestamp\": 1586173933472, \"lastIngestionTime\": 1586175101220, \"uploadSequenceToken\": \"49599989655680038369205623273330095416487086853777112338\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"storedBytes\": 0 }, { \"logStreamName\": \"verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"creationTime\": 1586166745643, \"firstEventTimestamp\": 1586166746491, \"lastEventTimestamp\": 1586173037927, \"lastIngestionTime\": 1586175100660, \"uploadSequenceToken\": \"49605664273821671319096446647846424799651902350804230514\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"storedBytes\": 0 } ] } Copy From the previous list of log streams, identify the one with the task ID for which you want to retrieve the logs and use the logStreamName in this command: aws logs get-log-events --log-group-name /newrelic-infra/ecs --log-stream-name \"LOG_STREAM_NAME\" --output text > logs.txt Copy Continue with the enable verbose logs instructions. From running container To enable verbose logs by running a command from the running container: SSH into one of your container instances. Find the container ID of the New Relic integration container by running the command docker ps -a. The name of the container should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec INTEGRATION_CONTAINER_ID /usr/bin/newrelic-infra-ctl Copy For more details, see Troubleshoot the agent. Save the logs from the container with the command docker logs INTEGRATION_CONTAINER_ID > logs.txt Copy Leave the command running for about three minutes to generate sufficient logging data. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.64684,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "sections": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": " the <em>container</em> ID of the New Relic <em>integration</em> <em>container</em>, by running the command docker ps -a. The name of the <em>container</em> should be nri-ecs. Save the logs from the <em>container</em> with the command docker logs NRI_ECS_<em>CONTAINER</em>_ID &gt; logs.txt. Leave the command running for about three minutes to generate sufficient"
      },
      "id": "604507f9196a67c1ae960f5e"
    }
  ],
  "/docs/infrastructure/elastic-container-service-integration/installation/uninstall-ecs-integration": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-10-25T00:44:10Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.17523,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "ECS integration troubleshooting: Generate verbose logs",
        "Problem",
        "Solution",
        "Using task definition environment variable",
        "Retrieve logs via SSH (EC2 launch type only)",
        "Forward logs to CloudWatch and download them with awscli",
        "From running container"
      ],
      "title": "ECS integration troubleshooting: Generate verbose logs",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "06198f1b2e0faa69bd8a7dfb93f18c8955fea83b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs/",
      "published_at": "2021-10-24T22:06:42Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting the on-host ECS integration, you can generate verbose logs for a few minutes to find and investigate errors. This can be useful for conducting your own troubleshooting or when providing information to New Relic support. Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. You can automate this process by using the newrelic-infra-ctl command. For more information, see Troubleshooting a running agent. Solution Generating verbose log files requires editing your task definition file. For a sample config file that includes all applicable settings, see Infrastructure configuration settings. You have several options for implementing verbose logs: Change the task definition environment variable and do a task restart For EC2 launch type: Retrieve logs via SSH Forward to CloudWatch and download with awscli Run a command from the running container Using task definition environment variable To enable verbose logs by changing the environment variable and doing a task restart: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. Save your task definition. Update your service to use the newly registered task definition. If you chose NRIA_VERBOSE=3 and you're not sending the logs directly to New Relic, you have two options for viewing and downloading the logs: For EC2 launch type: you can retrieve the logs via SSH, or Forward logs to CloudWatch Return your settings to default: Disable verbose logging by editing your task definition and setting NRIA_VERBOSE to 0. Save your task definition. Update your service to the latest version of your task. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file. Retrieve logs via SSH (EC2 launch type only) To get logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your container instances. Find the container ID of the New Relic integration container, by running the command docker ps -a. The name of the container should be nri-ecs. Save the logs from the container with the command docker logs NRI_ECS_CONTAINER_ID > logs.txt. Leave the command running for about three minutes to generate sufficient logging data. Continue with the instructions in the enable verbose logs section. Forward logs to CloudWatch and download them with awscli To get logs via CloudWatch: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. We use a CloudWatch log group called /newrelic-infra/ecs to forward the logs to. To see if it already exists, run: aws logs describe-log-groups --log-group-name-prefix /newrelic-infra/ecs Copy If a log group exists with that prefix, you'll get this output: { \"logGroups\": [ { \"logGroupName\": \"/newrelic-infra/ecs\", \"creationTime\": 1585828615225, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:YOUR_REGION:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:*\", \"storedBytes\": 122539356 } ] } Copy Because this command matches log groups with prefixes, ensure the log group name returned is exactly /newrelic-infra/ecs. If the log group doesn't exist, the output will be: { \"logGroups\": [] } Copy If the log group doesn't exist, create it by running: aws logs create-log-group --log-group-name /newrelic-infra/ecs Copy Edit your task definition. In the container definition for the newrelic-infra container, add the following logConfiguration: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"/newrelic-infra/ecs\", \"awslogs-region\": \"AWS_REGION_OF_YOUR_CLUSTER\", \"awslogs-stream-prefix\": \"verbose\" } } Copy Register the new task version and update your service. Next you'll look for the relevant log stream. If you have multiple instances of the task running, they'll all send their logs to the same log group but each will have its own log stream. Log streams names follow the structure AWSLOGS_STREAM_PREFIX/TASK_FAMILY_NAME/TASK_ID. In this case, it will be verbose/newrelic-infra/TASK_ID. To get all the log streams for a given log group, run this command: aws logs describe-log-streams --log-group-name /newrelic-infra/ecs Copy The following is an example output of a log group with two streams: { \"logStreams\": [ { \"logStreamName\": \"verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"creationTime\": 1586166741197, \"firstEventTimestamp\": 1586166742030, \"lastEventTimestamp\": 1586173933472, \"lastIngestionTime\": 1586175101220, \"uploadSequenceToken\": \"49599989655680038369205623273330095416487086853777112338\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"storedBytes\": 0 }, { \"logStreamName\": \"verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"creationTime\": 1586166745643, \"firstEventTimestamp\": 1586166746491, \"lastEventTimestamp\": 1586173037927, \"lastIngestionTime\": 1586175100660, \"uploadSequenceToken\": \"49605664273821671319096446647846424799651902350804230514\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"storedBytes\": 0 } ] } Copy From the previous list of log streams, identify the one with the task ID for which you want to retrieve the logs and use the logStreamName in this command: aws logs get-log-events --log-group-name /newrelic-infra/ecs --log-stream-name \"LOG_STREAM_NAME\" --output text > logs.txt Copy Continue with the enable verbose logs instructions. From running container To enable verbose logs by running a command from the running container: SSH into one of your container instances. Find the container ID of the New Relic integration container by running the command docker ps -a. The name of the container should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec INTEGRATION_CONTAINER_ID /usr/bin/newrelic-infra-ctl Copy For more details, see Troubleshoot the agent. Save the logs from the container with the command docker logs INTEGRATION_CONTAINER_ID > logs.txt Copy Leave the command running for about three minutes to generate sufficient logging data. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.64684,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "sections": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": " the <em>container</em> ID of the New Relic <em>integration</em> <em>container</em>, by running the command docker ps -a. The name of the <em>container</em> should be nri-ecs. Save the logs from the <em>container</em> with the command docker logs NRI_ECS_<em>CONTAINER</em>_ID &gt; logs.txt. Leave the command running for about three minutes to generate sufficient"
      },
      "id": "604507f9196a67c1ae960f5e"
    },
    {
      "sections": [
        "Understand and use ECS data",
        "View data",
        "Query your data"
      ],
      "title": "Understand and use ECS data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "16689cc080d4a8482e802b404df9ae45c4283db2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data/",
      "published_at": "2021-10-25T00:45:09Z",
      "updated_at": "2021-03-29T20:30:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this integration. View data To view the ECS integration dashboard: Go to one.newrelic.com and select Explorer. On the left, search for ECS clusters, or type the name of your ECS cluster in the search bar. To view a dashboard, select the entity name corresponding to your ECS cluster. In addition to the pre-built dashboards, you can also create your own custom queries and charts using the query builder. To learn how to query this data, see Understand data. Query your data Data reported by this integration is displayed in its dashboards and is also available for querying and the creation of custom charts and dashboards. This integration reports an EcsClusterSample event, with attributes clusterName and arn. Other types of data that may be available for querying: Infrastructure agent-reported events, including Docker All the events reported from an ECS cluster contain the attributes ecsClusterName and ecsClusterArn. Here's an example NRQL query that returns the count of containers associated with each Docker image in an ECS cluster named MyClusterName created in us-east-1: SELECT uniqueCount(containerId) FROM ContainerSample WHERE awsRegion = 'us-east-1' AND ecsClusterName = 'MyClusterName' FACET imageName SINCE 1 HOUR AGO Copy To learn more about creating custom queries and charts: How to query New Relic data Introduction to NRQL",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.46759,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this <em>integration</em>. View data To view the ECS <em>integration</em> dashboard: Go to one.newrelic.com and select Explorer"
      },
      "id": "603e9eb664441fbaad4e889f"
    }
  ],
  "/docs/infrastructure/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-10-25T00:44:10Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.81125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> <em>troubleshooting</em>: No data appears",
        "sections": "ECS <em>integration</em> <em>troubleshooting</em>: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". <em>Troubleshoot</em> in the UI To use the UI to <em>troubleshoot</em>: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-10-24T20:51:20Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 146.7054,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    },
    {
      "sections": [
        "Understand and use ECS data",
        "View data",
        "Query your data"
      ],
      "title": "Understand and use ECS data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "16689cc080d4a8482e802b404df9ae45c4283db2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data/",
      "published_at": "2021-10-25T00:45:09Z",
      "updated_at": "2021-03-29T20:30:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this integration. View data To view the ECS integration dashboard: Go to one.newrelic.com and select Explorer. On the left, search for ECS clusters, or type the name of your ECS cluster in the search bar. To view a dashboard, select the entity name corresponding to your ECS cluster. In addition to the pre-built dashboards, you can also create your own custom queries and charts using the query builder. To learn how to query this data, see Understand data. Query your data Data reported by this integration is displayed in its dashboards and is also available for querying and the creation of custom charts and dashboards. This integration reports an EcsClusterSample event, with attributes clusterName and arn. Other types of data that may be available for querying: Infrastructure agent-reported events, including Docker All the events reported from an ECS cluster contain the attributes ecsClusterName and ecsClusterArn. Here's an example NRQL query that returns the count of containers associated with each Docker image in an ECS cluster named MyClusterName created in us-east-1: SELECT uniqueCount(containerId) FROM ContainerSample WHERE awsRegion = 'us-east-1' AND ecsClusterName = 'MyClusterName' FACET imageName SINCE 1 HOUR AGO Copy To learn more about creating custom queries and charts: How to query New Relic data Introduction to NRQL",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.99885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this <em>integration</em>. View data To view the ECS <em>integration</em> dashboard: Go to one.newrelic.com and select Explorer"
      },
      "id": "603e9eb664441fbaad4e889f"
    }
  ],
  "/docs/infrastructure/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears": [
    {
      "sections": [
        "ECS integration troubleshooting: Generate verbose logs",
        "Problem",
        "Solution",
        "Using task definition environment variable",
        "Retrieve logs via SSH (EC2 launch type only)",
        "Forward logs to CloudWatch and download them with awscli",
        "From running container"
      ],
      "title": "ECS integration troubleshooting: Generate verbose logs",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "06198f1b2e0faa69bd8a7dfb93f18c8955fea83b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs/",
      "published_at": "2021-10-24T22:06:42Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting the on-host ECS integration, you can generate verbose logs for a few minutes to find and investigate errors. This can be useful for conducting your own troubleshooting or when providing information to New Relic support. Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. You can automate this process by using the newrelic-infra-ctl command. For more information, see Troubleshooting a running agent. Solution Generating verbose log files requires editing your task definition file. For a sample config file that includes all applicable settings, see Infrastructure configuration settings. You have several options for implementing verbose logs: Change the task definition environment variable and do a task restart For EC2 launch type: Retrieve logs via SSH Forward to CloudWatch and download with awscli Run a command from the running container Using task definition environment variable To enable verbose logs by changing the environment variable and doing a task restart: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. Save your task definition. Update your service to use the newly registered task definition. If you chose NRIA_VERBOSE=3 and you're not sending the logs directly to New Relic, you have two options for viewing and downloading the logs: For EC2 launch type: you can retrieve the logs via SSH, or Forward logs to CloudWatch Return your settings to default: Disable verbose logging by editing your task definition and setting NRIA_VERBOSE to 0. Save your task definition. Update your service to the latest version of your task. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file. Retrieve logs via SSH (EC2 launch type only) To get logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your container instances. Find the container ID of the New Relic integration container, by running the command docker ps -a. The name of the container should be nri-ecs. Save the logs from the container with the command docker logs NRI_ECS_CONTAINER_ID > logs.txt. Leave the command running for about three minutes to generate sufficient logging data. Continue with the instructions in the enable verbose logs section. Forward logs to CloudWatch and download them with awscli To get logs via CloudWatch: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. We use a CloudWatch log group called /newrelic-infra/ecs to forward the logs to. To see if it already exists, run: aws logs describe-log-groups --log-group-name-prefix /newrelic-infra/ecs Copy If a log group exists with that prefix, you'll get this output: { \"logGroups\": [ { \"logGroupName\": \"/newrelic-infra/ecs\", \"creationTime\": 1585828615225, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:YOUR_REGION:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:*\", \"storedBytes\": 122539356 } ] } Copy Because this command matches log groups with prefixes, ensure the log group name returned is exactly /newrelic-infra/ecs. If the log group doesn't exist, the output will be: { \"logGroups\": [] } Copy If the log group doesn't exist, create it by running: aws logs create-log-group --log-group-name /newrelic-infra/ecs Copy Edit your task definition. In the container definition for the newrelic-infra container, add the following logConfiguration: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"/newrelic-infra/ecs\", \"awslogs-region\": \"AWS_REGION_OF_YOUR_CLUSTER\", \"awslogs-stream-prefix\": \"verbose\" } } Copy Register the new task version and update your service. Next you'll look for the relevant log stream. If you have multiple instances of the task running, they'll all send their logs to the same log group but each will have its own log stream. Log streams names follow the structure AWSLOGS_STREAM_PREFIX/TASK_FAMILY_NAME/TASK_ID. In this case, it will be verbose/newrelic-infra/TASK_ID. To get all the log streams for a given log group, run this command: aws logs describe-log-streams --log-group-name /newrelic-infra/ecs Copy The following is an example output of a log group with two streams: { \"logStreams\": [ { \"logStreamName\": \"verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"creationTime\": 1586166741197, \"firstEventTimestamp\": 1586166742030, \"lastEventTimestamp\": 1586173933472, \"lastIngestionTime\": 1586175101220, \"uploadSequenceToken\": \"49599989655680038369205623273330095416487086853777112338\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"storedBytes\": 0 }, { \"logStreamName\": \"verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"creationTime\": 1586166745643, \"firstEventTimestamp\": 1586166746491, \"lastEventTimestamp\": 1586173037927, \"lastIngestionTime\": 1586175100660, \"uploadSequenceToken\": \"49605664273821671319096446647846424799651902350804230514\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"storedBytes\": 0 } ] } Copy From the previous list of log streams, identify the one with the task ID for which you want to retrieve the logs and use the logStreamName in this command: aws logs get-log-events --log-group-name /newrelic-infra/ecs --log-stream-name \"LOG_STREAM_NAME\" --output text > logs.txt Copy Continue with the enable verbose logs instructions. From running container To enable verbose logs by running a command from the running container: SSH into one of your container instances. Find the container ID of the New Relic integration container by running the command docker ps -a. The name of the container should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec INTEGRATION_CONTAINER_ID /usr/bin/newrelic-infra-ctl Copy For more details, see Troubleshoot the agent. Save the logs from the container with the command docker logs INTEGRATION_CONTAINER_ID > logs.txt Copy Leave the command running for about three minutes to generate sufficient logging data. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.76974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> <em>troubleshooting</em>: Generate verbose logs",
        "sections": "ECS <em>integration</em> <em>troubleshooting</em>: Generate verbose logs",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": " by running the command docker ps -a. The name of the <em>container</em> should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec <em>INTEGRATION_CONTAINER</em>_ID &#x2F;usr&#x2F;bin&#x2F;newrelic-infra-ctl Copy For more details, see <em>Troubleshoot</em> the agent. Save"
      },
      "id": "604507f9196a67c1ae960f5e"
    },
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-10-24T20:51:20Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 146.7054,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    },
    {
      "sections": [
        "Understand and use ECS data",
        "View data",
        "Query your data"
      ],
      "title": "Understand and use ECS data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "16689cc080d4a8482e802b404df9ae45c4283db2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data/",
      "published_at": "2021-10-25T00:45:09Z",
      "updated_at": "2021-03-29T20:30:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this integration. View data To view the ECS integration dashboard: Go to one.newrelic.com and select Explorer. On the left, search for ECS clusters, or type the name of your ECS cluster in the search bar. To view a dashboard, select the entity name corresponding to your ECS cluster. In addition to the pre-built dashboards, you can also create your own custom queries and charts using the query builder. To learn how to query this data, see Understand data. Query your data Data reported by this integration is displayed in its dashboards and is also available for querying and the creation of custom charts and dashboards. This integration reports an EcsClusterSample event, with attributes clusterName and arn. Other types of data that may be available for querying: Infrastructure agent-reported events, including Docker All the events reported from an ECS cluster contain the attributes ecsClusterName and ecsClusterArn. Here's an example NRQL query that returns the count of containers associated with each Docker image in an ECS cluster named MyClusterName created in us-east-1: SELECT uniqueCount(containerId) FROM ContainerSample WHERE awsRegion = 'us-east-1' AND ecsClusterName = 'MyClusterName' FACET imageName SINCE 1 HOUR AGO Copy To learn more about creating custom queries and charts: How to query New Relic data Introduction to NRQL",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.99885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this <em>integration</em>. View data To view the ECS <em>integration</em> dashboard: Go to one.newrelic.com and select Explorer"
      },
      "id": "603e9eb664441fbaad4e889f"
    }
  ],
  "/docs/infrastructure/elastic-container-service-integration/understand-use-data/ecs-integration-recommended-alert-conditions": [
    {
      "sections": [
        "Understand and use ECS data",
        "View data",
        "Query your data"
      ],
      "title": "Understand and use ECS data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "16689cc080d4a8482e802b404df9ae45c4283db2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data/",
      "published_at": "2021-10-25T00:45:09Z",
      "updated_at": "2021-03-29T20:30:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this integration. View data To view the ECS integration dashboard: Go to one.newrelic.com and select Explorer. On the left, search for ECS clusters, or type the name of your ECS cluster in the search bar. To view a dashboard, select the entity name corresponding to your ECS cluster. In addition to the pre-built dashboards, you can also create your own custom queries and charts using the query builder. To learn how to query this data, see Understand data. Query your data Data reported by this integration is displayed in its dashboards and is also available for querying and the creation of custom charts and dashboards. This integration reports an EcsClusterSample event, with attributes clusterName and arn. Other types of data that may be available for querying: Infrastructure agent-reported events, including Docker All the events reported from an ECS cluster contain the attributes ecsClusterName and ecsClusterArn. Here's an example NRQL query that returns the count of containers associated with each Docker image in an ECS cluster named MyClusterName created in us-east-1: SELECT uniqueCount(containerId) FROM ContainerSample WHERE awsRegion = 'us-east-1' AND ecsClusterName = 'MyClusterName' FACET imageName SINCE 1 HOUR AGO Copy To learn more about creating custom queries and charts: How to query New Relic data Introduction to NRQL",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.20956,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Understand</em> and <em>use</em> ECS <em>data</em>",
        "sections": "<em>Understand</em> and <em>use</em> ECS <em>data</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. Here we explain how to find, <em>understand</em>, and <em>use</em> the <em>data</em> reported by this <em>integration</em>. View <em>data</em> To view the ECS <em>integration</em> dashboard: Go to one.newrelic.com and select Explorer"
      },
      "id": "603e9eb664441fbaad4e889f"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-10-25T00:44:10Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.17523,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "sections": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To <em>use</em> the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, <em>use</em> the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-10-24T20:51:20Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.73492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation <em>Use</em> automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    }
  ],
  "/docs/infrastructure/elastic-container-service-integration/understand-use-data/understand-use-ecs-data": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-10-25T00:44:10Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.17523,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "sections": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To <em>use</em> the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, <em>use</em> the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-10-24T20:51:20Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.73492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation <em>Use</em> automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    },
    {
      "sections": [
        "ECS integration troubleshooting: Generate verbose logs",
        "Problem",
        "Solution",
        "Using task definition environment variable",
        "Retrieve logs via SSH (EC2 launch type only)",
        "Forward logs to CloudWatch and download them with awscli",
        "From running container"
      ],
      "title": "ECS integration troubleshooting: Generate verbose logs",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "06198f1b2e0faa69bd8a7dfb93f18c8955fea83b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs/",
      "published_at": "2021-10-24T22:06:42Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting the on-host ECS integration, you can generate verbose logs for a few minutes to find and investigate errors. This can be useful for conducting your own troubleshooting or when providing information to New Relic support. Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. You can automate this process by using the newrelic-infra-ctl command. For more information, see Troubleshooting a running agent. Solution Generating verbose log files requires editing your task definition file. For a sample config file that includes all applicable settings, see Infrastructure configuration settings. You have several options for implementing verbose logs: Change the task definition environment variable and do a task restart For EC2 launch type: Retrieve logs via SSH Forward to CloudWatch and download with awscli Run a command from the running container Using task definition environment variable To enable verbose logs by changing the environment variable and doing a task restart: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. Save your task definition. Update your service to use the newly registered task definition. If you chose NRIA_VERBOSE=3 and you're not sending the logs directly to New Relic, you have two options for viewing and downloading the logs: For EC2 launch type: you can retrieve the logs via SSH, or Forward logs to CloudWatch Return your settings to default: Disable verbose logging by editing your task definition and setting NRIA_VERBOSE to 0. Save your task definition. Update your service to the latest version of your task. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file. Retrieve logs via SSH (EC2 launch type only) To get logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your container instances. Find the container ID of the New Relic integration container, by running the command docker ps -a. The name of the container should be nri-ecs. Save the logs from the container with the command docker logs NRI_ECS_CONTAINER_ID > logs.txt. Leave the command running for about three minutes to generate sufficient logging data. Continue with the instructions in the enable verbose logs section. Forward logs to CloudWatch and download them with awscli To get logs via CloudWatch: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. We use a CloudWatch log group called /newrelic-infra/ecs to forward the logs to. To see if it already exists, run: aws logs describe-log-groups --log-group-name-prefix /newrelic-infra/ecs Copy If a log group exists with that prefix, you'll get this output: { \"logGroups\": [ { \"logGroupName\": \"/newrelic-infra/ecs\", \"creationTime\": 1585828615225, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:YOUR_REGION:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:*\", \"storedBytes\": 122539356 } ] } Copy Because this command matches log groups with prefixes, ensure the log group name returned is exactly /newrelic-infra/ecs. If the log group doesn't exist, the output will be: { \"logGroups\": [] } Copy If the log group doesn't exist, create it by running: aws logs create-log-group --log-group-name /newrelic-infra/ecs Copy Edit your task definition. In the container definition for the newrelic-infra container, add the following logConfiguration: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"/newrelic-infra/ecs\", \"awslogs-region\": \"AWS_REGION_OF_YOUR_CLUSTER\", \"awslogs-stream-prefix\": \"verbose\" } } Copy Register the new task version and update your service. Next you'll look for the relevant log stream. If you have multiple instances of the task running, they'll all send their logs to the same log group but each will have its own log stream. Log streams names follow the structure AWSLOGS_STREAM_PREFIX/TASK_FAMILY_NAME/TASK_ID. In this case, it will be verbose/newrelic-infra/TASK_ID. To get all the log streams for a given log group, run this command: aws logs describe-log-streams --log-group-name /newrelic-infra/ecs Copy The following is an example output of a log group with two streams: { \"logStreams\": [ { \"logStreamName\": \"verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"creationTime\": 1586166741197, \"firstEventTimestamp\": 1586166742030, \"lastEventTimestamp\": 1586173933472, \"lastIngestionTime\": 1586175101220, \"uploadSequenceToken\": \"49599989655680038369205623273330095416487086853777112338\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"storedBytes\": 0 }, { \"logStreamName\": \"verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"creationTime\": 1586166745643, \"firstEventTimestamp\": 1586166746491, \"lastEventTimestamp\": 1586173037927, \"lastIngestionTime\": 1586175100660, \"uploadSequenceToken\": \"49605664273821671319096446647846424799651902350804230514\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"storedBytes\": 0 } ] } Copy From the previous list of log streams, identify the one with the task ID for which you want to retrieve the logs and use the logStreamName in this command: aws logs get-log-events --log-group-name /newrelic-infra/ecs --log-stream-name \"LOG_STREAM_NAME\" --output text > logs.txt Copy Continue with the enable verbose logs instructions. From running container To enable verbose logs by running a command from the running container: SSH into one of your container instances. Find the container ID of the New Relic integration container by running the command docker ps -a. The name of the container should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec INTEGRATION_CONTAINER_ID /usr/bin/newrelic-infra-ctl Copy For more details, see Troubleshoot the agent. Save the logs from the container with the command docker logs INTEGRATION_CONTAINER_ID > logs.txt Copy Leave the command running for about three minutes to generate sufficient logging data. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.64684,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "sections": "ECS <em>integration</em> troubleshooting: Generate verbose logs",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": " the logs from the <em>container</em> with the command docker logs <em>INTEGRATION_CONTAINER</em>_ID &gt; logs.txt Copy Leave the command running for about three minutes to generate sufficient logging <em>data</em>. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log"
      },
      "id": "604507f9196a67c1ae960f5e"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-app-engine-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.20041,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-bigquery-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.2003,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.2003,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.2002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41884,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataflow-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.2002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41884,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataproc-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.2001,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41884,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-database-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.2001,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41884,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-hosting-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.2001,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41884,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-storage-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19998,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41841,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firestore-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19998,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41841,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19989,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41841,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-load-balancing-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19989,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41841,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-pubsub-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4184,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-router-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4184,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-run-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4184,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-spanner-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19968,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41881,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4184,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-sql-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19968,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41881,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4184,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-datastore-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19946,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-direct-interconnect-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19946,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-kubernetes-engine-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19946,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.4188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Cloud Bigtable monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Bigtable Cluster data",
        "Bigtable Table data"
      ],
      "title": "Google Cloud Bigtable monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a7cd3b2dca9763ad525a473fb8a9ed19d60d0875",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration/",
      "published_at": "2021-10-24T20:47:41Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Bigtable data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Bigtable integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Cluster GcpBigtableClusterSample GcpBigtableCluster Table GcpBigtableTableSample GcpBigtableTable For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Bigtable data for Cluster and Table. Bigtable Cluster data Metric Unit Description cluster.CpuLoad Count CPU load of a cluster. cluster.CpuLoadHottestNode Count CPU load of the busiest node in a cluster. cluster.DiskLoad Count Utilization of HDD disks in a cluster. cluster.Node Count Number of nodes in a cluster. cluster.StorageUtilization Count Storage used as a fraction of total storage capacity. disk.BytesUsed Bytes Amount of compressed data for tables stored in a cluster. disk.StorageCapacity Bytes Capacity of compressed data for tables that can be stored in a cluster. Bigtable Table data Metric Unit Description replication.Latency Milliseconds Distribution of replication request latencies for a table. Includes only requests that have been received by the destination cluster. replication.MaxDelay Seconds Upper bound for replication delay between clusters of a table. Indicates the time frame during which latency information may not be accurate. server.Error Count Number of server requests for a table that failed with an error. server.Latencies Milliseconds Distribution of server request latencies for a table, measured when calls reach Cloud Bigtable. server.ModifiedRows Count Number of rows modified by server requests for a table. server.MultiClusterFailovers Count Number of failovers during multi-cluster requests. server.ReceivedBytes Bytes Number of uncompressed bytes of request data received by servers for a table. server.Request Count Number of server requests for a table. server.ReturnedRows Count Number of rows returned by server requests for a table. server.SentBytes Bytes Number of uncompressed bytes of response data sent by servers for a table. table.BytesUsed Bytes Amount of compressed data stored in a table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41827,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Bigtable data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "6045082f196a67af34960f20"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41837,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Bigtable monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Bigtable Cluster data",
        "Bigtable Table data"
      ],
      "title": "Google Cloud Bigtable monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a7cd3b2dca9763ad525a473fb8a9ed19d60d0875",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration/",
      "published_at": "2021-10-24T20:47:41Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Bigtable data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Bigtable integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Cluster GcpBigtableClusterSample GcpBigtableCluster Table GcpBigtableTableSample GcpBigtableTable For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Bigtable data for Cluster and Table. Bigtable Cluster data Metric Unit Description cluster.CpuLoad Count CPU load of a cluster. cluster.CpuLoadHottestNode Count CPU load of the busiest node in a cluster. cluster.DiskLoad Count Utilization of HDD disks in a cluster. cluster.Node Count Number of nodes in a cluster. cluster.StorageUtilization Count Storage used as a fraction of total storage capacity. disk.BytesUsed Bytes Amount of compressed data for tables stored in a cluster. disk.StorageCapacity Bytes Capacity of compressed data for tables that can be stored in a cluster. Bigtable Table data Metric Unit Description replication.Latency Milliseconds Distribution of replication request latencies for a table. Includes only requests that have been received by the destination cluster. replication.MaxDelay Seconds Upper bound for replication delay between clusters of a table. Indicates the time frame during which latency information may not be accurate. server.Error Count Number of server requests for a table that failed with an error. server.Latencies Milliseconds Distribution of server request latencies for a table, measured when calls reach Cloud Bigtable. server.ModifiedRows Count Number of rows modified by server requests for a table. server.MultiClusterFailovers Count Number of failovers during multi-cluster requests. server.ReceivedBytes Bytes Number of uncompressed bytes of request data received by servers for a table. server.Request Count Number of server requests for a table. server.ReturnedRows Count Number of rows returned by server requests for a table. server.SentBytes Bytes Number of uncompressed bytes of response data sent by servers for a table. table.BytesUsed Bytes Amount of compressed data stored in a table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41827,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Bigtable data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "6045082f196a67af34960f20"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration": [
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.19925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> services <em>integrations</em>",
        "sections": "<em>Cloud</em> services <em>integrations</em>",
        "body": "With New Relic you can easily instrument your services in AWS, <em>Google</em> <em>Cloud</em> <em>Platform</em>, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> <em>List</em> of AWS <em>integrations</em> <em>GCP</em> <em>integrations</em> Introduction to <em>GCP</em> <em>integrations</em> <em>List</em> of <em>GCP</em> <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> <em>List</em> of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-10-24T21:53:50Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41878,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.41837,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic": [
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.35075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-10-24T20:48:37Z",
      "updated_at": "2021-07-27T15:50:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.96323,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Get started"
      ],
      "external_id": "9a44613b8a5ec0a9c9570b22c7d2f3ea726f2671",
      "image": "",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/introduction-new-relic-integrations/",
      "published_at": "2021-10-25T15:15:14Z",
      "updated_at": "2021-10-24T00:59:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.50453,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " (AWS), Microsoft Azure, and <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry <em>integrations</em>: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you"
      },
      "id": "603e817f28ccbc4857eba798"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/get-started/gcp-integration-metrics": [
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Tip",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-27T15:50:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. If you don't have one already, create a New Relic account. It's free, forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. If your organization uses a domain restriction constraint, you will have to update the policy to allow the New Relic domain, C02x1gp26. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.94104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. If you don&#x27;t have one already, create a New Relic account. It&#x27;s free, forever. Requirements These are the requirements for the authorization: GCP"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-10-24T20:48:37Z",
      "updated_at": "2021-07-27T15:50:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.96323,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Get started"
      ],
      "external_id": "9a44613b8a5ec0a9c9570b22c7d2f3ea726f2671",
      "image": "",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/introduction-new-relic-integrations/",
      "published_at": "2021-10-25T15:15:14Z",
      "updated_at": "2021-10-24T00:59:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.50447,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " (AWS), Microsoft Azure, and <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry <em>integrations</em>: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you"
      },
      "id": "603e817f28ccbc4857eba798"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/get-started/integrations-custom-roles": [
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Tip",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-27T15:50:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. If you don't have one already, create a New Relic account. It's free, forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. If your organization uses a domain restriction constraint, you will have to update the policy to allow the New Relic domain, C02x1gp26. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.94104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. If you don&#x27;t have one already, create a New Relic account. It&#x27;s free, forever. Requirements These are the requirements for the authorization: GCP"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.35074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-10-24T20:48:37Z",
      "updated_at": "2021-07-27T15:50:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.96323,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations": [
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Tip",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-27T15:50:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. If you don't have one already, create a New Relic account. It's free, forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. If your organization uses a domain restriction constraint, you will have to update the policy to allow the New Relic domain, C02x1gp26. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.94104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. If you don&#x27;t have one already, create a New Relic account. It&#x27;s free, forever. Requirements These are the requirements for the authorization: GCP"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.35074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Get started"
      ],
      "external_id": "9a44613b8a5ec0a9c9570b22c7d2f3ea726f2671",
      "image": "",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/introduction-new-relic-integrations/",
      "published_at": "2021-10-25T15:15:14Z",
      "updated_at": "2021-10-24T00:59:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.50447,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " (AWS), Microsoft Azure, and <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry <em>integrations</em>: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you"
      },
      "id": "603e817f28ccbc4857eba798"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/getting-started/polling-intervals-gcp-integrations": [
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Tip",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-27T15:50:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. If you don't have one already, create a New Relic account. It's free, forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. If your organization uses a domain restriction constraint, you will have to update the policy to allow the New Relic domain, C02x1gp26. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.94104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. If you don&#x27;t have one already, create a New Relic account. It&#x27;s free, forever. Requirements These are the requirements for the authorization: GCP"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.35074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-10-24T20:48:37Z",
      "updated_at": "2021-07-27T15:50:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.96321,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    }
  ],
  "/docs/infrastructure/google-cloud-platform-integrations/troubleshooting/gcp-integration-api-authentication-errors": [
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Tip",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-27T15:50:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. If you don't have one already, create a New Relic account. It's free, forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. If your organization uses a domain restriction constraint, you will have to update the policy to allow the New Relic domain, C02x1gp26. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.72612,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To start receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. If you don&#x27;t have one already, create a New Relic account. It&#x27;s free, forever. Requirements These are the requirements for the authorization: GCP"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-10-24T21:09:09Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 129.31503,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-10-24T20:48:37Z",
      "updated_at": "2021-07-27T15:50:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.49302,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    }
  ],
  "/docs/infrastructure/host-integrations/get-started/introduction-host-integrations": [
    {
      "sections": [
        "NGINX monitoring integration",
        "Tip",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate the integration",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Enabling your NGINX Server",
        "Configure the integration",
        "Important",
        "NGINX Instance Settings",
        "Labels/Custom Attributes",
        "Example configurations",
        "BASIC CONFIGURATION",
        "HTTP BASIC AUTHENTICATION",
        "METRICS ONLY WITH SELF-SIGNED CERTIFICATE",
        "ENVIRONMENT VARIABLES REPLACEMENT",
        "MULTI-INSTANCE MONITORING",
        "Find and use data",
        "Metrics",
        "NGINX Open Source",
        "NGINX Plus",
        "Inventory data",
        "System metadata",
        "Check the source code"
      ],
      "title": "NGINX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "2488458edecf24531f4d04ee0d1aec788e8e0258",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/nginx-monitoring-integration/",
      "published_at": "2021-10-24T22:01:21Z",
      "updated_at": "2021-10-24T00:55:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Tip Want to help make this doc better? Try our beta NGINX monitoring integration doc and give us feedback by creating a GitHub issue. Help us continue creating great experiences for you! Our NGINX integration collects and sends inventory and metrics from your NGINX server to our platform, where you can see data on connections and client requests so that you can find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with both NGINX Open Source and NGINX Plus. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. NGINX extension enabled, as described in the Configure the integration section. If NGINX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running NGINX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your NGINX server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the NGINX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your NGINX server. Install and activate the integration To install the NGINX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the general instructions for installing an integration, using the filename nri-nginx. Then continue on to the steps below. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp nginx-config.yml.sample nginx-config.yml Copy Edit the configuration file nginx-config.yml. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your NGINX Server To capture data from the NGINX integration, you must first enable and configure the applicable extension module: For NGINX Open Source: HTTP stub status module For NGINX Plus: HTTP status module and HTTP API module Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, nginx-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to NGINX are defined using the env section of the configuration file. These settings control the connection to your NGINX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. NGINX Instance Settings The NGINX integration collects both Metrics and Inventory information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To STATUS_URL The URL set up to provide the metrics using the status module. http://127.0.0.1/status Metrics/Inventory STATUS_MODULE Name of NGINX status module. Valid options are: discover ngx_http_stub_status_module ngx_http_status_module ngx_http_api_module\" discover Metrics CONNECTION_TIMEOUT Connection timeout to the NGINX instance in seconds. 5 Metrics VALIDATE_CERTS Set to false if the status URL is HTTPS with a self-signed certificate. true Metrics CONFIG_PATH The path to the NGINX configuration file. N/A Inventory REMOTE_MONITORING Enable multi-tenancy monitoring. true Metrics/Inventory METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here or see example below. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations BASIC CONFIGURATION This is the very basic configuration to collect Metrics and Inventory from your localhost: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: http://127.0.0.1/status STATUS_MODULE: discover REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer - name: nri-nginx env: INVENTORY: \"true\" STATUS_URL: http://127.0.0.1/status CONFIG_PATH: /etc/nginx/nginx.conf REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/nginx Copy HTTP BASIC AUTHENTICATION This configuration collects Metrics and Inventory from your localhost protected with basic authentication. Replace the username and password on the STATUS_URL with your credentials: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: http://username:password@127.0.0.1/status STATUS_MODULE: discover REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer - name: nri-nginx env: INVENTORY: \"true\" STATUS_URL: http://username:password@127.0.0.1/status CONFIG_PATH: /etc/nginx/nginx.conf REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/nginx Copy METRICS ONLY WITH SELF-SIGNED CERTIFICATE In this configuration we only have 1 integration block with METRICS: true to collect only metrics and added VALIDATE_CERTS: false to prevent validation of the server's SSL certificate when using a self-signed one: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: https://my_nginx_host/status STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer Copy ENVIRONMENT VARIABLES REPLACEMENT In this configuration we are using the environment variable NGINX_STATUS to populate the STATUS_URL setting of the integration: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: {{NGINX_STATUS}} STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer Copy MULTI-INSTANCE MONITORING In this configuration we are monitoring multiple NGINX servers from the same integration. For the first instance (STATUS_URL: https://1st_nginx_host/status) we are collecting metrics and inventory while for the second instance (STATUS_URL: https://2nd_nginx_host/status) we will only collect metrics. integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: https://1st_nginx_host/status STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer - name: nri-nginx env: INVENTORY: \"true\" STATUS_URL: https://1st_nginx_host/status CONFIG_PATH: /etc/nginx/nginx.conf REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/nginx - name: nri-nginx env: METRICS: \"true\" STATUS_URL: http://2nd_nginx_host/status STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the NginxSample event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metrics The NGINX integration collects the following metric data attributes. To find these attributes, query the NginxSample event type. NGINX Open Source Metric Description net.connectionsActive Number of connections that are currently active net.connectionsAcceptedPerSecond Number of accepted client connections per second net.connectionsDroppedPerSecond Number of connections per second that were accepted but could not he handled and hence dropped net.connectionsReading Current number of connections where NGINX is reading the request header net.connectionsWaiting Current number of idle client connections waiting for a request net.connectionsWriting Current number of connections where NGINX is writing the response back to the client net.requestsPerSecond Total number of client requests per second NGINX Plus Our integration retrieves all available metric data from the following HTTP API endpoints: /nginx, /processes, /connections, /http/requests, and /ssl. Metric Description net.connectionsAcceptedPerSecond Accepted client connections as requests per second net.connectionsDroppedPerSecond Dropped client connections as requests per second net.connectionsActive Current number of active client connections net.connectionsIdle Current number of idle client connections net.requests Current number of requests net.requestsPerSecond Current number of requests per second processes.respawned Current number of abnormally terminated and respawned child processes ssl.handshakes Current number for successful SSL handshakes ssl.failedHandshakes Current number of failed SSL handshakes ssl.sessionReuses Current number of session reuses during SSL handshake Inventory data The integration captures configuration options defined in the NGINX master config file (usually nginx.conf). Tip The master NGINX config file can contain \"include OTHER_FILE_NAME\" commands for splitting the configuration into multiple files. The Infrastructure agent ignores (does not parse) configuration set via include commands. System metadata The integration collects these additional attributes about the NGINX service: Name Description software.edition The NGINX edition: either \"open source\" or \"plus\". software.version The version of NGINX. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.53696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "NGINX monitoring <em>integration</em>",
        "sections": "NGINX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the infrastructure agent on a Linux OS <em>host</em> that&#x27;s running NGINX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick <em>start</em> Instrument your NGINX server quickly and send your telemetry data with guided install. Our guided install creates a customized"
      },
      "id": "6174aeeee7b9d20c7713bc3e"
    },
    {
      "sections": [
        "Apache monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Enabling your Apache server",
        "Configure the integration",
        "Important",
        "Apache Instance Settings",
        "Labels/Custom attributes",
        "Example configurations",
        "BASIC CONFIGURATION",
        "HTTP BASIC AUTHENTICATION",
        "METRICS ONLY WITH SELF-SIGNED CERTIFICATE",
        "METRICS ONLY WITH ALTERNATIVE CERTIFICATE",
        "ENVIRONMENT VARIABLES REPLACEMENT",
        "MULTI-INSTANCE MONITORING",
        "Find and use data",
        "Metric data",
        "Inventory data",
        "System metadata",
        "Troubleshooting",
        "Problem accessing HTTPS endpoint for Apache",
        "Check the source code"
      ],
      "title": "Apache monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cae1fcc5a402bf71ae7d304b00420a9aa9b1152d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/apache-monitoring-integration/",
      "published_at": "2021-10-24T21:58:30Z",
      "updated_at": "2021-10-24T00:52:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Apache integration sends performance metrics and inventory data from your Apache web server to the New Relic platform. You can view pre-built dashboards of your Apache metric data, create alert policies, and create your own custom queries and charts. The integration works by gathering data from Apache's status module, so that module must be enabled and configured for your Apache instance (more details in Requirements). Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Apache versions 2.2 or 2.4. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Apache status module enabled and configured for Apache instance. Apache status module endpoint (default server-status) available from the host containing the Apache integration. If Apache is not running on Kubernetes or Amazon ECS, you must have the infrastructure agent installed on a Linux OS host that's running Apache. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Apache web server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Apache integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your Apache web server. Install and activate To install the Apache integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-apache. Change directory to the integration's folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp apache-config.yml.sample apache-config.yml Copy Edit the apache-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your Apache server To capture data from the Apache integration, you must first enable and configure the status module: Ensure the Apache status module is enabled and configured for Apache instance. Ensure the Apache status module endpoint (default server-status) is available from the host containing the Apache integration. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, apache-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, inventory_source. To read all about these common settings, refer to our Configuration Format document. Important If you are still using our legacy configuration/definition files, please refer to this document for help. Specific settings related to Apache are defined using the env section of the configuration file. These settings control the connection to your Apache instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Apache Instance Settings The Apache integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to STATUS_URL The URL set up to provide the metrics using the status module. http://127.0.0.1/server-status?auto M/I CA_BUNDLE_FILE Alternative Certificate Authority bundle file. N/A M CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M VALIDATE_CERTS Set to false if the status URL is HTTPS with a self-signed certificate. true M REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here or see the example below. Using secrets management. Use this to protect sensitive information, such as passwords that would be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics on. Our default sample config file includes examples of labels; however, as they are not mandatory, you can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations BASIC CONFIGURATION This is the very basic configuration to collect metrics and inventory from your localhost: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy HTTP BASIC AUTHENTICATION This configuration collects metrics and inventory from your localhost protected with basic authentication. Replace the username and password on the STATUS_URL with your credentials: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy METRICS ONLY WITH SELF-SIGNED CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added VALIDATE_CERTS: false to prevent validation of the server's SSL certificate when using a self-signed one: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy METRICS ONLY WITH ALTERNATIVE CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added CA_BUNDLE_FILE pointing to an alternative certificate file: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto CA_BUNDLE_FILE='/etc/ssl/certs/custom-ca.crt' REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy ENVIRONMENT VARIABLES REPLACEMENT In this configuration we are using the environment variable APACHE_STATUS to populate the STATUS_URL setting of the integration: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: {{APACHE_STATUS}} REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy MULTI-INSTANCE MONITORING In this configuration we are monitoring multiple Apache servers from the same integration. For the first instance (STATUS_URL: https://1st_apache_host/server-status?auto) we are collecting metrics and inventory while for the second instance (STATUS_URL: https://2nd_apache_host/server-status?auto) we will only collect metrics. integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://2nd_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy Find and use data Data from this service is reported to an integration dashboard. Apache data is attached to the ApacheSample event type. You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Apache integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as net. or server.. Name Description net.bytesPerSecond Rate of the number of bytes served, in bytes per second. net.requestsPerSecond Rate of the number of client requests, in requests per second. server.busyWorkers Current number of busy workers. server.idleWorkers Current number of idle workers. server.scoreboard.closingWorkers Current number of workers closing TCP connection after serving the response. server.scoreboard.dnsLookupWorkers Current number of workers performing a DNS lookup. server.scoreboard.finishingWorkers Current number of workers gracefully finishing. server.scoreboard.idleCleanupWorkers Current number of idle workers ready for cleanup. server.scoreboard.keepAliveWorkers Current number of workers maintaining a keep-alive connection. server.scoreboard.loggingWorkers Current number of workers that are logging. server.scoreboard.readingWorkers Current number of workers reading requests (headers or body). server.scoreboard.startingWorkers Current number of workers that are starting up. server.scoreboard.totalWorkers Total number of workers available. Workers that are not needed to process requests may not be started. server.scoreboard.writingWorkers Current number of workers that are writing. Inventory data Inventory data captures the version numbers from running Apache and from all loaded Apache modules, and adds those version numbers under the config/apache namespace. For more about inventory data, see Understand data. System metadata Besides the standard attributes collected by the infrastructure agent, the integration collects inventory data associated with the ApacheSample event type: Name Description software.version The version of the Apache server. Example: Apache/2.4.7 (Ubuntu). Troubleshooting Problem accessing HTTPS endpoint for Apache If you are having issues accessing the HTTPS endpoint for Apache, here are two possible solutions: Although you cannot ignore the SSL certification, you can set the config file parameters ca_bundle_file and ca_bundle_dir to point to an unsigned certificate in the Apache config file. Example: instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto ca_bundle_file: /etc/newrelic-infra/integrations.d/ssl/b2b.ca-bundle Copy An example using ca_bundle_dir: ca_bundle_dir: /etc/newrelic-infra/integrations.d/ssl Copy Alternatively, you can use HTTP instead of HTTPS. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.5217,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Apache monitoring <em>integration</em>",
        "sections": "Apache monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " containing the Apache integration. If Apache is not running on Kubernetes or Amazon ECS, you must have the infrastructure agent installed on a Linux OS <em>host</em> that&#x27;s running Apache. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick <em>start</em>"
      },
      "id": "6174ae5a64441f5baf5fc976"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "76e490b6f08befa3a9ed50cc021b8fc1ddb40423",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-10-24T21:58:47Z",
      "updated_at": "2021-10-24T00:54:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.81161,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". Quick <em>start</em> Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to <em>get</em> <em>started</em>? Click one"
      },
      "id": "6174aec2e7b9d227f313d6bf"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/apache-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08832,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28577,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/cassandra-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08813,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28577,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28558,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/collectd-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08813,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28577,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28558,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/couchbase-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28558,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/elasticsearch-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28558,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/f5-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08783,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28528,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/flex-integration-tool-build-your-own-integration": [
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Get started"
      ],
      "external_id": "9a44613b8a5ec0a9c9570b22c7d2f3ea726f2671",
      "image": "",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/introduction-new-relic-integrations/",
      "published_at": "2021-10-25T15:15:14Z",
      "updated_at": "2021-10-24T00:59:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.3519,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Instrument</em> <em>everything</em>",
        "body": " <em>integrations</em>, such as our APM agents. Whatever data you need to bring in, chances are that we have options for <em>your</em> environment. If you prefer to make <em>your</em> <em>own</em> solutions, we also offer tools to get you started. Choose what&#x27;s right for you We offer a wide range of solutions so you can easily collect"
      },
      "id": "603e817f28ccbc4857eba798"
    },
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "509277aa4f9f8ad66cf5f82a94104531df64c296",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/cloud-services-integrations/",
      "published_at": "2021-10-25T16:29:51Z",
      "updated_at": "2021-10-24T00:48:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 113.41147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Cloud services <em>integrations</em>",
        "sections": "Cloud services <em>integrations</em>",
        "tags": "<em>Instrument</em> <em>everything</em>",
        "body": "With New Relic you can easily <em>instrument</em> <em>your</em> services in AWS, Google Cloud Platform, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> List of AWS <em>integrations</em> GCP <em>integrations</em> Introduction to GCP <em>integrations</em> List of GCP <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> List of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "image": "https://docs.newrelic.com/static/d2a9c929c7541b67b6fe4c87844fc01b/ae694/prometheus_grafana_dashboard.png",
      "url": "https://docs.newrelic.com/whats-new/2020/08/create-grafana-dashboards-prometheus-data-stored-new-relic/",
      "sections": [
        "Create Grafana dashboards with Prometheus data stored in New Relic",
        "Step 1: Get data flowing into New Relic with the Prometheus remote write integration",
        "Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic"
      ],
      "published_at": "2021-10-25T01:03:57Z",
      "title": "Create Grafana dashboards with Prometheus data stored in New Relic",
      "updated_at": "2021-10-19T05:58:32Z",
      "type": "docs",
      "external_id": "da09ab47a2ac806ad3ed1fa67e3a02dd54394383",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We’ve teamed up with Grafana Labs so you can use our platform as a data source for Prometheus metrics and see them in your existing dashboards, seamlessly tapping into the reliability, scale, and security provided by New Relic. Follow the steps below or use this more detailed walkthrough to send Prometheus data to New Relic, so that Grafana can populate your existing Prometheus-specific dashboards with that data. This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. You’ll also need to sign up for New Relic. Here's an example of how these Grafana dashboards with Prometheus data look in our new dark mode. Step 1: Get data flowing into New Relic with the Prometheus remote write integration Go to Instrument Everything – US or Instrument Everything – EU, then click the Prometheus tile. You can also go to the Prometheus remote write setup page to get your remote_write URL. For more information on how to set up the Prometheus remote write integration, check out our docs. Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic For more information on how to configure New Relic as a Prometheus data source for Grafana, check out our docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 85.83739,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 1: Get data flowing into New Relic with the Prometheus remote write <em>integration</em>",
        "body": " dashboards with Prometheus data look in our new dark mode. Step 1: Get data flowing into New Relic with the Prometheus remote write integration Go to <em>Instrument</em> <em>Everything</em> – US or <em>Instrument</em> <em>Everything</em> – EU, then click the Prometheus tile. You can also go to the Prometheus remote write setup page to get"
      },
      "id": "60445821e7b9d23b585799e4"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/go-insights-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08783,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28528,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/haproxy-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08765,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28528,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/hashicorp-consul-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08765,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28528,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/jmx-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.0875,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/kafka-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.0875,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/memcached-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/mongodb-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/mysql-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/nagios-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.087,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/nfs-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.087,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/nginx-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08685,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/oracle-database-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08685,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/perfmon-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/port-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/postgresql-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.284,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    },
    {
      "sections": [
        "PostgreSQL monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "PostgreSQL users and permissions",
        "Configure the integration",
        "Important",
        "PostgreSQL Instance Settings",
        "Labels/Custom Attributes",
        "Example configuration",
        "PostgreSQL configuration collection file",
        "PostgreSQL SSL configuration collection file",
        "PostgreSQL custom query",
        "PostgreSQL custom query config file",
        "Find and use data",
        "Metric data",
        "PostgresqlDatabaseSample metrics",
        "PostgresqlIndexSample metrics",
        "PostgresqlInstanceSample metrics",
        "PostgresqlTableSample metrics",
        "PgBouncerSample metrics",
        "Inventory data",
        "Troubleshooting",
        "Check the source code"
      ],
      "title": "PostgreSQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "3453e5b64a8b3635c9e088d820fc8eecfc96a0f5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/postgresql-monitoring-integration/",
      "published_at": "2021-10-24T17:56:12Z",
      "updated_at": "2021-10-24T00:55:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic PostgreSQL on-host integration receives and sends inventory metrics from your PostgreSQL instance to the New Relic platform, where you can aggregate and visualize key performance metrics. Data from instances, databases, and clusters helps you find the source of problems. Read on to install the integration, and to see what data we collect. If you don't have one already, create a New Relic account. It's free, forever. Compatibility and requirements Our integration is compatible with PostgreSQL 9.0 or higher. If PostgreSQL is not running on Kubernetes or Amazon ECS, you can install the infrastructure agent on a Linux or Windows OS host where PostgreSQL is installed or on a host capable of remotely accessing where PostgreSQL is installed. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your PostgreSQL instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the PostgreSQL integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your PostgreSQL instance. Install and activate To install the PostgreSQL integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the procedures to install the infrastructure integration package using the file name nri-postgresql. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp postgresql-config.yml.sample postgresql-config.yml Copy Edit the postgresql-config.yml file as described in the configuration settings. Before you restart the infrastructure agent, create a user with READ permissions on the required functions. Restart the infrastructure agent. Windows Download the nri-postgresql .MSI installer image from: https://download.newrelic.com/infrastructure_agent/windows/integrations/nri-postgresql/nri-postgresql-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-postgresql-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp postgresql-config.yml.sample postgresql-config.yml Copy Edit the postgresql-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. PostgreSQL users and permissions Create a user with SELECT permissions on: pg_stat_database pg_stat_database_conflicts pg_stat_bgwriter You can complete this step before or after you configure the postgresql-config.yml file. To create the user for the PostgreSQL integration: CREATE USER new_relic WITH PASSWORD 'PASSWORD'; GRANT SELECT ON pg_stat_database TO new_relic; GRANT SELECT ON pg_stat_database_conflicts TO new_relic; GRANT SELECT ON pg_stat_bgwriter TO new_relic; Copy This will allow the integration to gather global metrics related to the PostgreSQL instance. If you also want to obtain table and index-related metrics (for example, table size and index size), the PostgreSQL role used by the integration (new_relic) also needs SELECT permissions on the tables from which it will gather metrics from. For example, to allow the integration to collect metrics from all the tables and indexes present in the database (in the public schema), use the following: GRANT SELECT ON ALL TABLES IN SCHEMA public TO new_relic; Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, postgresql-config.yml. Config options are below. For an example configuration, see the example config file. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to PostgreSQL are defined using the env section of the configuration file. These settings control the connection to your PostgreSQL instance as well as other security settings and features. The list of valid settings is described in the next section of this document. PostgreSQL Instance Settings The PostgreSQL integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To HOSTNAME The hostname for the PostgreSQL connection. localhost M/I PORT The port where PostgreSQL is running. 5432 M/I USERNAME The user name for the PostgreSQL connection. Required. N/A M/I PASSWORD The password for the PostgreSQL connection. Required. N/A M/I COLLECTION_LIST JSON array, a JSON object, or the string literal ALL that specifies the entities to be collected. The PostgreSQL user can only collect table and index metrics from tables it has SELECT permissions for. Required except for PgBouncer. Examples. N/A M COLLECTION_IGNORE_DATABASE_LIST JSON array of database names that will be ignored for metrics collection. Typically useful for cases where COLLECTION_LIST is set to ALL and some databases need to be ignored. '[]' M PGBOUNCER Collect pgbouncer metrics. false M ENABLE_SSL Determines if SSL is enabled. If true, ssl_cert_location and ssl_key_location are required. false M/I TRUST_SERVER_CERTIFICATE If true, the server certificate is not verified for SSL. If false, the server certificate identified in ssl_root_cert_location is verified. false M/I SSL_ROOT_CERT_LOCATION Absolute path to PEM-encoded root certificate file. Required if trust_server_certificate is false. N/A M/I SSL_CERT_LOCATION Absolute path to PEM-encoded client certificate file. Required if enable_ssl is true. N/A M/I SSL_KEY_LOCATION Absolute path to PEM-encoded client key file. Required if enable_ssl is true. N/A M/I TIMEOUT maximum wait for connection, in seconds. Set to 0 for no timeout. 10 M/I DATABASE The PostgreSQL database to connect to. postgres M/I CUSTOM_METRICS_QUERY A SQL query that required columns metric_name, metric_type, and metric_value.metric_type can be gauge, rate, delta, or attribute. Additional columns collected with the query are added to the metric set as attributes. N/A M CUSTOM_METRICS_CONFIG A path to a YAML file with a list of custom queries, along with their metric type, database, and sample name overrides. See example for details. N/A M COLLECT_DB_LOCK_METRICS Enable collecting database lock metrics, which can be performance intensive. false M COLLECT_BLOAT_METRICS Enable tablespace bloat metrics, which can be performance intensive. true M METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: postgresql Copy Example configuration Example postgresql-config.yml file configuration: PostgreSQL configuration collection file JSON array: Interpreted as a list of database names from which to collect all relevant metrics, including any tables and indexes belonging to that database. For example: collection_list: '[\"postgres\"]' JSON object: only entities specified in the object will be collected, no automatic discovery will be performed. The levels of JSON are database name -> schema name -> table name -> index name. For example: collection_list: '{\"postgres\":{\"public\":{\"pg_table1\":[\"pg_index1\",\"pg_index2\"],\"pg_table2\":[]}}}' ALL: collect metrics for all databases, schemas, tables, and indexes discovered. For example: collection_list: 'ALL' integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: '{\"postgres\":{\"public\":{\"pg_table1\":[\"pg_index1\",\"pg_index2\"],\"pg_table2\":[]}}}' TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL SSL configuration collection file integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: '[\"postgres\"]' ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: false SSL_ROOT_CERT_LOCATION: /etc/newrelic-infra/root_cert.crt SSL_CERT_LOCATION: /etc/newrelic-infra/postgresql.crt SSL_KEY_LOCATION: /etc/newrelic-infra/postgresql.key TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL custom query integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: ALL CUSTOM_METRICS_QUERY: >- select 'rows_inserted' as \"metric_name\", 'delta' as \"metric_type\", sd.tup_inserted as \"metric_value\", sd.datid as \"database_id\" from pg_stat_database sd; TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL custom query config file An additional YAML configuration file with one or more custom SQL can be defined and the integration will need the path to the file in the CUSTOM_METRICS_CONFIG parameter. postgresql-config.yml integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: ALL CUSTOM_METRICS_CONFIG: \"path/to/postgresql-custom-query.yml\" TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy postgresql-custom-query.yml --- queries: # Metric names are set to the column names in the query results - query: >- SELECT BG.checkpoints_timed AS scheduled_checkpoints_performed, BG.checkpoints_req AS requested_checkpoints_performed, BG.buffers_checkpoint AS buffers_written_during_checkpoint, BG.buffers_clean AS buffers_written_by_background_writer, BG.maxwritten_clean AS background_writer_stops, BG.buffers_backend AS buffers_written_by_backend, BG.buffers_alloc AS buffers_allocated FROM pg_stat_bgwriter BG; # database defaults to the auth database in the main config database: new_frontier_config_dev # If not set explicitly here, metric type will default to # 'gauge' for numbers and 'attribute' for strings metric_types: buffers_allocated: rate # If unset, sample_name defaults to PostgresqlCustomSample sample_name: MyCustomSample Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: PostgresqlDatabaseSample PostgresqlIndexSample PostgresqlInstanceSample PostgresqlTableSample PgBouncerSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The PostgreSQL integration collects the following database metric attributes. Some attributes apply to all PostgreSQL event types. Some metric names are prefixed with a category indicator and a period, such as db. or index. metric names. PostgresqlDatabaseSample metrics These attributes are attached to the PostgresqlDatabaseSample event type: PostgreSQLDatabaseSample attributes Description db.connections Number of backends currently connected to this database. db.maxconnections The maximum number of concurrent connections to the database server. db.commitsPerSecond Committed transactions per second. db.rollbacksPerSecond Transactions rolled back per second. db.readsPerSecond Number of disk blocks read in this database per second. db.bufferHitsPerSecond Number of times disk blocks were found already in the buffer cache, so that a read was not necessary. This only includes hits in the PostgreSQL buffer cache, not the operating system's file system cache. db.rowsReturnedPerSecond Rows returned by queries per second. db.rowsFetchedPerSecond Rows fetched by queries per second. db.rowsInsertedPerSecond Rows inserted per second. db.rowsUpdatedPerSecond Rows updated per second. db.rowsDeletedPerSecond Rows deleted per second. db.conflicts.tablespacePerSecond Number of queries in this database that have been canceled due to dropped tablespaces. db.conflicts.locksPerSecond Number of queries in this database that have been canceled due to lock timeouts. db.conflicts.snapshotPerSecond Number of queries in this database that have been canceled due to old snapshots. db.conflicts.bufferpinPerSecond Number of queries in this database that have been canceled due to pinned buffers. db.conflicts.deadlockPerSecond Number of queries in this database that have been canceled due to deadlocks. db.tempFilesCreatedPerSecond Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (for example, sorting or hashing), and regardless of the log_temp_files setting. db.tempWrittenInBytesPerSecond Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. db.deadlocksPerSecond Number of deadlocks detected in this database. db.readTimeInMillisecondsPerSecond Time spent reading data file blocks by backends in this database, in milliseconds. db.writeTimeInMillisecondsPerSecond Time spent writing data file blocks by backends in this database, in milliseconds. PostgresqlIndexSample metrics These attributes are attached to the PostgresqlIndexSample event type: PostgreSQLIndexSample attributes Description index.sizeInBytes The size of an index. index.rowsReadPerSecond The number of index entries returned by scans on this index. index.rowsFetchedPerSecond The number of index entries fetched by scans on this index. PostgresqlInstanceSample metrics These attributes are attached to the PostgresqlInstanceSample event type: PostgreSQLInstanceSample attributes Description bgwriter.checkpointsScheduledPerSecond Number of scheduled checkpoints that have been performed. bgwriter.checkpointsRequestedPerSecond Number of requested checkpoints that have been performed. bgwriter.buffersWrittenForCheckpointsPerSecond Number of buffers written during checkpoints. bgwriter.buffersWrittenByBackgroundWriterPerSecond Number of buffers written by the background writer. bgwriter.backgroundWriterStopsPerSecond Number of times the background writer stopped a cleaning scan because it had written too many buffers. bgwriter.buffersWrittenByBackendPerSecond Number of buffers written directly by a backend. bgwriter.buffersAllocatedPerSecond Number of buffers allocated. bgwriter.backendFsyncCallsPerSecond Number of times a backend had to execute its own fsync call. Normally the background writer handles them even when the backend does its own write. bgwriter.checkpointWriteTimeInMillisecondsPerSecond Total amount of time that has been spent in the portion of checkpoint processing where files are written to disk, in milliseconds. bgwriter.checkpointSyncTimeInMillisecondsPerSecond Total amount of time that has been spent in the portion of checkpoint processing where files are synchronized to disk, in milliseconds. PostgresqlTableSample metrics These attributes are attached to the PostgresqlTableSample event type: PostgreSQLTableSample attributes Description table.totalSizeInBytes The total disk space used by the table, including indexes and TOAST data. table.indexSizeInBytes The total disk space used by indexes attached to the specified table. table.liveRows Number of live rows. table.deadRows Number of dead rows. table.indexBlocksReadPerSecond The number of disk blocks read from all indexes on this table. table.indexBlocksHitPerSecond The number of buffer hits in all indexes on this table. table.indexToastBlocksReadPerSecond The number of disk blocks read from this table's TOAST table index. table.indexToastBlocksHitPerSecond The number of buffer hits in this table's TOAST table index. table.lastVacuum Time of last vacuum on table. table.lastAutoVacuum Time of last automatic vacuum on table. table.lastAnalyze Time of last analyze on table. table.lastAutoAnalyze Time of last automatic analyze on table. table.sequentialScansPerSecond Number of sequential scans initiated on this table per second. table.sequentialScanRowsFetchedPerSecond Number of live rows fetched by sequential scans per second. table.indexScansPerSecond Number of index scans initiated on this table. table.indexScanRowsFetchedPerSecon Number of live rows fetched by index scans. table.rowsInsertedPerSecond Rows inserted per second. table.rowsUpdatedPerSecond Rows updated per second. table.rowsDeletedPerSecond Rows deleted per second. table.bloatSizeInBytes Size of bloat in bytes. table.dataSizeInBytes Size of disk spaced used by the main fork of the table. table.bloatRatio Fraction of table data size that is bloat. PgBouncerSample metrics These attributes are attached to the PgBouncerSample event type: PgBouncerSample attributes Description pgbouncer.stats.transactionsPerSecond The transaction rate. pgbouncer.stats.queriesPerSecond The query rate. pgbouncer.stats.bytesInPerSecond The total network traffic received. pgbouncer.stats.bytesOutPerSecond The total network traffic sent. pgbouncer.stats.totalTransactionDurationInMillisecondsPerSecond Time spent by pgbouncer in transaction. pgbouncer.stats.totalQueryDurationInMillisecondsPerSecond Time spent by pgbouncer actively querying PostgreSQL. pgbouncer.stats.avgTransactionCount The average number of transactions per second in last stat period. pgbouncer.stats.avgTransactionDurationInMilliseconds The average transaction duration. pgbouncer.stats.avgQueryCount The average number of queries per second in last stat period. pgbouncer.stats.avgBytesIn The client network traffic received. pgbouncer.stats.avgBytesOut The client network traffic sent. pgbouncer.stats.avgQueryDurationInMilliseconds The average query duration. pgbouncer.pools.clientConnectionsActive Client connections linked to server connection and able to process queries. pgbouncer.pools.clientConnectionsWaiting Client connections waiting on a server connection. pgbouncer.pools.serverConnectionsActive Server connections linked to a client connection. pgbouncer.pools.serverConnectionsIdle Server connections idle and ready for a client query. pgbouncer.pools.serverConnectionsUsed Server connections idle more than server_check_delay, needing server_check_query. pgbouncer.pools.serverConnectionsTested Server connections currently running either server_reset_query or server_check_query. pgbouncer.pools.serverConnectionsLogin Server connections currently in the process of logging in. pgbouncer.pools.maxwaitInMilliseconds Age of oldest unserved client connection. Inventory data The PostgreSQL integration collects each setting from pg_settings along with its boot_val and reset_val. The inventory data appears on the Inventory page, under the config/postgresql source. Troubleshooting Here are some troubleshooting tips for the PostgreSQL integration: If you have connection problems, make sure you can connect to the cluster from the same box with psql. If you have problems collecting PgBouncer metrics, make sure you are connected to the instance through PgBouncer. Default port is 6432. If you get the error message Error creating list of entities to collect: pq: unsupported startup parameter: extra_float_digits, set ignore_startup_parameters = extra_float_digits in the PgBouncer config file. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.27576,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PostgreSQL monitoring <em>integration</em>",
        "sections": "PostgreSQL monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. PostgreSQL users and permissions Create a user with SELECT permissions on: pg_stat_database"
      },
      "id": "6174aeeee7b9d2451513d580"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/rabbitmq-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08652,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.284,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.284,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "PostgreSQL monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "PostgreSQL users and permissions",
        "Configure the integration",
        "Important",
        "PostgreSQL Instance Settings",
        "Labels/Custom Attributes",
        "Example configuration",
        "PostgreSQL configuration collection file",
        "PostgreSQL SSL configuration collection file",
        "PostgreSQL custom query",
        "PostgreSQL custom query config file",
        "Find and use data",
        "Metric data",
        "PostgresqlDatabaseSample metrics",
        "PostgresqlIndexSample metrics",
        "PostgresqlInstanceSample metrics",
        "PostgresqlTableSample metrics",
        "PgBouncerSample metrics",
        "Inventory data",
        "Troubleshooting",
        "Check the source code"
      ],
      "title": "PostgreSQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "3453e5b64a8b3635c9e088d820fc8eecfc96a0f5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/postgresql-monitoring-integration/",
      "published_at": "2021-10-24T17:56:12Z",
      "updated_at": "2021-10-24T00:55:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic PostgreSQL on-host integration receives and sends inventory metrics from your PostgreSQL instance to the New Relic platform, where you can aggregate and visualize key performance metrics. Data from instances, databases, and clusters helps you find the source of problems. Read on to install the integration, and to see what data we collect. If you don't have one already, create a New Relic account. It's free, forever. Compatibility and requirements Our integration is compatible with PostgreSQL 9.0 or higher. If PostgreSQL is not running on Kubernetes or Amazon ECS, you can install the infrastructure agent on a Linux or Windows OS host where PostgreSQL is installed or on a host capable of remotely accessing where PostgreSQL is installed. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your PostgreSQL instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the PostgreSQL integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your PostgreSQL instance. Install and activate To install the PostgreSQL integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the procedures to install the infrastructure integration package using the file name nri-postgresql. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp postgresql-config.yml.sample postgresql-config.yml Copy Edit the postgresql-config.yml file as described in the configuration settings. Before you restart the infrastructure agent, create a user with READ permissions on the required functions. Restart the infrastructure agent. Windows Download the nri-postgresql .MSI installer image from: https://download.newrelic.com/infrastructure_agent/windows/integrations/nri-postgresql/nri-postgresql-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-postgresql-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp postgresql-config.yml.sample postgresql-config.yml Copy Edit the postgresql-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. PostgreSQL users and permissions Create a user with SELECT permissions on: pg_stat_database pg_stat_database_conflicts pg_stat_bgwriter You can complete this step before or after you configure the postgresql-config.yml file. To create the user for the PostgreSQL integration: CREATE USER new_relic WITH PASSWORD 'PASSWORD'; GRANT SELECT ON pg_stat_database TO new_relic; GRANT SELECT ON pg_stat_database_conflicts TO new_relic; GRANT SELECT ON pg_stat_bgwriter TO new_relic; Copy This will allow the integration to gather global metrics related to the PostgreSQL instance. If you also want to obtain table and index-related metrics (for example, table size and index size), the PostgreSQL role used by the integration (new_relic) also needs SELECT permissions on the tables from which it will gather metrics from. For example, to allow the integration to collect metrics from all the tables and indexes present in the database (in the public schema), use the following: GRANT SELECT ON ALL TABLES IN SCHEMA public TO new_relic; Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, postgresql-config.yml. Config options are below. For an example configuration, see the example config file. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to PostgreSQL are defined using the env section of the configuration file. These settings control the connection to your PostgreSQL instance as well as other security settings and features. The list of valid settings is described in the next section of this document. PostgreSQL Instance Settings The PostgreSQL integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To HOSTNAME The hostname for the PostgreSQL connection. localhost M/I PORT The port where PostgreSQL is running. 5432 M/I USERNAME The user name for the PostgreSQL connection. Required. N/A M/I PASSWORD The password for the PostgreSQL connection. Required. N/A M/I COLLECTION_LIST JSON array, a JSON object, or the string literal ALL that specifies the entities to be collected. The PostgreSQL user can only collect table and index metrics from tables it has SELECT permissions for. Required except for PgBouncer. Examples. N/A M COLLECTION_IGNORE_DATABASE_LIST JSON array of database names that will be ignored for metrics collection. Typically useful for cases where COLLECTION_LIST is set to ALL and some databases need to be ignored. '[]' M PGBOUNCER Collect pgbouncer metrics. false M ENABLE_SSL Determines if SSL is enabled. If true, ssl_cert_location and ssl_key_location are required. false M/I TRUST_SERVER_CERTIFICATE If true, the server certificate is not verified for SSL. If false, the server certificate identified in ssl_root_cert_location is verified. false M/I SSL_ROOT_CERT_LOCATION Absolute path to PEM-encoded root certificate file. Required if trust_server_certificate is false. N/A M/I SSL_CERT_LOCATION Absolute path to PEM-encoded client certificate file. Required if enable_ssl is true. N/A M/I SSL_KEY_LOCATION Absolute path to PEM-encoded client key file. Required if enable_ssl is true. N/A M/I TIMEOUT maximum wait for connection, in seconds. Set to 0 for no timeout. 10 M/I DATABASE The PostgreSQL database to connect to. postgres M/I CUSTOM_METRICS_QUERY A SQL query that required columns metric_name, metric_type, and metric_value.metric_type can be gauge, rate, delta, or attribute. Additional columns collected with the query are added to the metric set as attributes. N/A M CUSTOM_METRICS_CONFIG A path to a YAML file with a list of custom queries, along with their metric type, database, and sample name overrides. See example for details. N/A M COLLECT_DB_LOCK_METRICS Enable collecting database lock metrics, which can be performance intensive. false M COLLECT_BLOAT_METRICS Enable tablespace bloat metrics, which can be performance intensive. true M METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: postgresql Copy Example configuration Example postgresql-config.yml file configuration: PostgreSQL configuration collection file JSON array: Interpreted as a list of database names from which to collect all relevant metrics, including any tables and indexes belonging to that database. For example: collection_list: '[\"postgres\"]' JSON object: only entities specified in the object will be collected, no automatic discovery will be performed. The levels of JSON are database name -> schema name -> table name -> index name. For example: collection_list: '{\"postgres\":{\"public\":{\"pg_table1\":[\"pg_index1\",\"pg_index2\"],\"pg_table2\":[]}}}' ALL: collect metrics for all databases, schemas, tables, and indexes discovered. For example: collection_list: 'ALL' integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: '{\"postgres\":{\"public\":{\"pg_table1\":[\"pg_index1\",\"pg_index2\"],\"pg_table2\":[]}}}' TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL SSL configuration collection file integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: '[\"postgres\"]' ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: false SSL_ROOT_CERT_LOCATION: /etc/newrelic-infra/root_cert.crt SSL_CERT_LOCATION: /etc/newrelic-infra/postgresql.crt SSL_KEY_LOCATION: /etc/newrelic-infra/postgresql.key TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL custom query integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: ALL CUSTOM_METRICS_QUERY: >- select 'rows_inserted' as \"metric_name\", 'delta' as \"metric_type\", sd.tup_inserted as \"metric_value\", sd.datid as \"database_id\" from pg_stat_database sd; TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL custom query config file An additional YAML configuration file with one or more custom SQL can be defined and the integration will need the path to the file in the CUSTOM_METRICS_CONFIG parameter. postgresql-config.yml integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: ALL CUSTOM_METRICS_CONFIG: \"path/to/postgresql-custom-query.yml\" TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy postgresql-custom-query.yml --- queries: # Metric names are set to the column names in the query results - query: >- SELECT BG.checkpoints_timed AS scheduled_checkpoints_performed, BG.checkpoints_req AS requested_checkpoints_performed, BG.buffers_checkpoint AS buffers_written_during_checkpoint, BG.buffers_clean AS buffers_written_by_background_writer, BG.maxwritten_clean AS background_writer_stops, BG.buffers_backend AS buffers_written_by_backend, BG.buffers_alloc AS buffers_allocated FROM pg_stat_bgwriter BG; # database defaults to the auth database in the main config database: new_frontier_config_dev # If not set explicitly here, metric type will default to # 'gauge' for numbers and 'attribute' for strings metric_types: buffers_allocated: rate # If unset, sample_name defaults to PostgresqlCustomSample sample_name: MyCustomSample Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: PostgresqlDatabaseSample PostgresqlIndexSample PostgresqlInstanceSample PostgresqlTableSample PgBouncerSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The PostgreSQL integration collects the following database metric attributes. Some attributes apply to all PostgreSQL event types. Some metric names are prefixed with a category indicator and a period, such as db. or index. metric names. PostgresqlDatabaseSample metrics These attributes are attached to the PostgresqlDatabaseSample event type: PostgreSQLDatabaseSample attributes Description db.connections Number of backends currently connected to this database. db.maxconnections The maximum number of concurrent connections to the database server. db.commitsPerSecond Committed transactions per second. db.rollbacksPerSecond Transactions rolled back per second. db.readsPerSecond Number of disk blocks read in this database per second. db.bufferHitsPerSecond Number of times disk blocks were found already in the buffer cache, so that a read was not necessary. This only includes hits in the PostgreSQL buffer cache, not the operating system's file system cache. db.rowsReturnedPerSecond Rows returned by queries per second. db.rowsFetchedPerSecond Rows fetched by queries per second. db.rowsInsertedPerSecond Rows inserted per second. db.rowsUpdatedPerSecond Rows updated per second. db.rowsDeletedPerSecond Rows deleted per second. db.conflicts.tablespacePerSecond Number of queries in this database that have been canceled due to dropped tablespaces. db.conflicts.locksPerSecond Number of queries in this database that have been canceled due to lock timeouts. db.conflicts.snapshotPerSecond Number of queries in this database that have been canceled due to old snapshots. db.conflicts.bufferpinPerSecond Number of queries in this database that have been canceled due to pinned buffers. db.conflicts.deadlockPerSecond Number of queries in this database that have been canceled due to deadlocks. db.tempFilesCreatedPerSecond Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (for example, sorting or hashing), and regardless of the log_temp_files setting. db.tempWrittenInBytesPerSecond Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. db.deadlocksPerSecond Number of deadlocks detected in this database. db.readTimeInMillisecondsPerSecond Time spent reading data file blocks by backends in this database, in milliseconds. db.writeTimeInMillisecondsPerSecond Time spent writing data file blocks by backends in this database, in milliseconds. PostgresqlIndexSample metrics These attributes are attached to the PostgresqlIndexSample event type: PostgreSQLIndexSample attributes Description index.sizeInBytes The size of an index. index.rowsReadPerSecond The number of index entries returned by scans on this index. index.rowsFetchedPerSecond The number of index entries fetched by scans on this index. PostgresqlInstanceSample metrics These attributes are attached to the PostgresqlInstanceSample event type: PostgreSQLInstanceSample attributes Description bgwriter.checkpointsScheduledPerSecond Number of scheduled checkpoints that have been performed. bgwriter.checkpointsRequestedPerSecond Number of requested checkpoints that have been performed. bgwriter.buffersWrittenForCheckpointsPerSecond Number of buffers written during checkpoints. bgwriter.buffersWrittenByBackgroundWriterPerSecond Number of buffers written by the background writer. bgwriter.backgroundWriterStopsPerSecond Number of times the background writer stopped a cleaning scan because it had written too many buffers. bgwriter.buffersWrittenByBackendPerSecond Number of buffers written directly by a backend. bgwriter.buffersAllocatedPerSecond Number of buffers allocated. bgwriter.backendFsyncCallsPerSecond Number of times a backend had to execute its own fsync call. Normally the background writer handles them even when the backend does its own write. bgwriter.checkpointWriteTimeInMillisecondsPerSecond Total amount of time that has been spent in the portion of checkpoint processing where files are written to disk, in milliseconds. bgwriter.checkpointSyncTimeInMillisecondsPerSecond Total amount of time that has been spent in the portion of checkpoint processing where files are synchronized to disk, in milliseconds. PostgresqlTableSample metrics These attributes are attached to the PostgresqlTableSample event type: PostgreSQLTableSample attributes Description table.totalSizeInBytes The total disk space used by the table, including indexes and TOAST data. table.indexSizeInBytes The total disk space used by indexes attached to the specified table. table.liveRows Number of live rows. table.deadRows Number of dead rows. table.indexBlocksReadPerSecond The number of disk blocks read from all indexes on this table. table.indexBlocksHitPerSecond The number of buffer hits in all indexes on this table. table.indexToastBlocksReadPerSecond The number of disk blocks read from this table's TOAST table index. table.indexToastBlocksHitPerSecond The number of buffer hits in this table's TOAST table index. table.lastVacuum Time of last vacuum on table. table.lastAutoVacuum Time of last automatic vacuum on table. table.lastAnalyze Time of last analyze on table. table.lastAutoAnalyze Time of last automatic analyze on table. table.sequentialScansPerSecond Number of sequential scans initiated on this table per second. table.sequentialScanRowsFetchedPerSecond Number of live rows fetched by sequential scans per second. table.indexScansPerSecond Number of index scans initiated on this table. table.indexScanRowsFetchedPerSecon Number of live rows fetched by index scans. table.rowsInsertedPerSecond Rows inserted per second. table.rowsUpdatedPerSecond Rows updated per second. table.rowsDeletedPerSecond Rows deleted per second. table.bloatSizeInBytes Size of bloat in bytes. table.dataSizeInBytes Size of disk spaced used by the main fork of the table. table.bloatRatio Fraction of table data size that is bloat. PgBouncerSample metrics These attributes are attached to the PgBouncerSample event type: PgBouncerSample attributes Description pgbouncer.stats.transactionsPerSecond The transaction rate. pgbouncer.stats.queriesPerSecond The query rate. pgbouncer.stats.bytesInPerSecond The total network traffic received. pgbouncer.stats.bytesOutPerSecond The total network traffic sent. pgbouncer.stats.totalTransactionDurationInMillisecondsPerSecond Time spent by pgbouncer in transaction. pgbouncer.stats.totalQueryDurationInMillisecondsPerSecond Time spent by pgbouncer actively querying PostgreSQL. pgbouncer.stats.avgTransactionCount The average number of transactions per second in last stat period. pgbouncer.stats.avgTransactionDurationInMilliseconds The average transaction duration. pgbouncer.stats.avgQueryCount The average number of queries per second in last stat period. pgbouncer.stats.avgBytesIn The client network traffic received. pgbouncer.stats.avgBytesOut The client network traffic sent. pgbouncer.stats.avgQueryDurationInMilliseconds The average query duration. pgbouncer.pools.clientConnectionsActive Client connections linked to server connection and able to process queries. pgbouncer.pools.clientConnectionsWaiting Client connections waiting on a server connection. pgbouncer.pools.serverConnectionsActive Server connections linked to a client connection. pgbouncer.pools.serverConnectionsIdle Server connections idle and ready for a client query. pgbouncer.pools.serverConnectionsUsed Server connections idle more than server_check_delay, needing server_check_query. pgbouncer.pools.serverConnectionsTested Server connections currently running either server_reset_query or server_check_query. pgbouncer.pools.serverConnectionsLogin Server connections currently in the process of logging in. pgbouncer.pools.maxwaitInMilliseconds Age of oldest unserved client connection. Inventory data The PostgreSQL integration collects each setting from pg_settings along with its boot_val and reset_val. The inventory data appears on the Inventory page, under the config/postgresql source. Troubleshooting Here are some troubleshooting tips for the PostgreSQL integration: If you have connection problems, make sure you can connect to the cluster from the same box with psql. If you have problems collecting PgBouncer metrics, make sure you are connected to the instance through PgBouncer. Default port is 6432. If you get the error message Error creating list of entities to collect: pq: unsupported startup parameter: extra_float_digits, set ignore_startup_parameters = extra_float_digits in the PgBouncer config file. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2756,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PostgreSQL monitoring <em>integration</em>",
        "sections": "PostgreSQL monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. PostgreSQL users and permissions Create a user with SELECT permissions on: pg_stat_database"
      },
      "id": "6174aeeee7b9d2451513d580"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/snmp-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.284,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    },
    {
      "sections": [
        "PostgreSQL monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "PostgreSQL users and permissions",
        "Configure the integration",
        "Important",
        "PostgreSQL Instance Settings",
        "Labels/Custom Attributes",
        "Example configuration",
        "PostgreSQL configuration collection file",
        "PostgreSQL SSL configuration collection file",
        "PostgreSQL custom query",
        "PostgreSQL custom query config file",
        "Find and use data",
        "Metric data",
        "PostgresqlDatabaseSample metrics",
        "PostgresqlIndexSample metrics",
        "PostgresqlInstanceSample metrics",
        "PostgresqlTableSample metrics",
        "PgBouncerSample metrics",
        "Inventory data",
        "Troubleshooting",
        "Check the source code"
      ],
      "title": "PostgreSQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "3453e5b64a8b3635c9e088d820fc8eecfc96a0f5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/postgresql-monitoring-integration/",
      "published_at": "2021-10-24T17:56:12Z",
      "updated_at": "2021-10-24T00:55:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic PostgreSQL on-host integration receives and sends inventory metrics from your PostgreSQL instance to the New Relic platform, where you can aggregate and visualize key performance metrics. Data from instances, databases, and clusters helps you find the source of problems. Read on to install the integration, and to see what data we collect. If you don't have one already, create a New Relic account. It's free, forever. Compatibility and requirements Our integration is compatible with PostgreSQL 9.0 or higher. If PostgreSQL is not running on Kubernetes or Amazon ECS, you can install the infrastructure agent on a Linux or Windows OS host where PostgreSQL is installed or on a host capable of remotely accessing where PostgreSQL is installed. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your PostgreSQL instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the PostgreSQL integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your PostgreSQL instance. Install and activate To install the PostgreSQL integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the procedures to install the infrastructure integration package using the file name nri-postgresql. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp postgresql-config.yml.sample postgresql-config.yml Copy Edit the postgresql-config.yml file as described in the configuration settings. Before you restart the infrastructure agent, create a user with READ permissions on the required functions. Restart the infrastructure agent. Windows Download the nri-postgresql .MSI installer image from: https://download.newrelic.com/infrastructure_agent/windows/integrations/nri-postgresql/nri-postgresql-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-postgresql-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp postgresql-config.yml.sample postgresql-config.yml Copy Edit the postgresql-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. PostgreSQL users and permissions Create a user with SELECT permissions on: pg_stat_database pg_stat_database_conflicts pg_stat_bgwriter You can complete this step before or after you configure the postgresql-config.yml file. To create the user for the PostgreSQL integration: CREATE USER new_relic WITH PASSWORD 'PASSWORD'; GRANT SELECT ON pg_stat_database TO new_relic; GRANT SELECT ON pg_stat_database_conflicts TO new_relic; GRANT SELECT ON pg_stat_bgwriter TO new_relic; Copy This will allow the integration to gather global metrics related to the PostgreSQL instance. If you also want to obtain table and index-related metrics (for example, table size and index size), the PostgreSQL role used by the integration (new_relic) also needs SELECT permissions on the tables from which it will gather metrics from. For example, to allow the integration to collect metrics from all the tables and indexes present in the database (in the public schema), use the following: GRANT SELECT ON ALL TABLES IN SCHEMA public TO new_relic; Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, postgresql-config.yml. Config options are below. For an example configuration, see the example config file. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to PostgreSQL are defined using the env section of the configuration file. These settings control the connection to your PostgreSQL instance as well as other security settings and features. The list of valid settings is described in the next section of this document. PostgreSQL Instance Settings The PostgreSQL integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To HOSTNAME The hostname for the PostgreSQL connection. localhost M/I PORT The port where PostgreSQL is running. 5432 M/I USERNAME The user name for the PostgreSQL connection. Required. N/A M/I PASSWORD The password for the PostgreSQL connection. Required. N/A M/I COLLECTION_LIST JSON array, a JSON object, or the string literal ALL that specifies the entities to be collected. The PostgreSQL user can only collect table and index metrics from tables it has SELECT permissions for. Required except for PgBouncer. Examples. N/A M COLLECTION_IGNORE_DATABASE_LIST JSON array of database names that will be ignored for metrics collection. Typically useful for cases where COLLECTION_LIST is set to ALL and some databases need to be ignored. '[]' M PGBOUNCER Collect pgbouncer metrics. false M ENABLE_SSL Determines if SSL is enabled. If true, ssl_cert_location and ssl_key_location are required. false M/I TRUST_SERVER_CERTIFICATE If true, the server certificate is not verified for SSL. If false, the server certificate identified in ssl_root_cert_location is verified. false M/I SSL_ROOT_CERT_LOCATION Absolute path to PEM-encoded root certificate file. Required if trust_server_certificate is false. N/A M/I SSL_CERT_LOCATION Absolute path to PEM-encoded client certificate file. Required if enable_ssl is true. N/A M/I SSL_KEY_LOCATION Absolute path to PEM-encoded client key file. Required if enable_ssl is true. N/A M/I TIMEOUT maximum wait for connection, in seconds. Set to 0 for no timeout. 10 M/I DATABASE The PostgreSQL database to connect to. postgres M/I CUSTOM_METRICS_QUERY A SQL query that required columns metric_name, metric_type, and metric_value.metric_type can be gauge, rate, delta, or attribute. Additional columns collected with the query are added to the metric set as attributes. N/A M CUSTOM_METRICS_CONFIG A path to a YAML file with a list of custom queries, along with their metric type, database, and sample name overrides. See example for details. N/A M COLLECT_DB_LOCK_METRICS Enable collecting database lock metrics, which can be performance intensive. false M COLLECT_BLOAT_METRICS Enable tablespace bloat metrics, which can be performance intensive. true M METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: postgresql Copy Example configuration Example postgresql-config.yml file configuration: PostgreSQL configuration collection file JSON array: Interpreted as a list of database names from which to collect all relevant metrics, including any tables and indexes belonging to that database. For example: collection_list: '[\"postgres\"]' JSON object: only entities specified in the object will be collected, no automatic discovery will be performed. The levels of JSON are database name -> schema name -> table name -> index name. For example: collection_list: '{\"postgres\":{\"public\":{\"pg_table1\":[\"pg_index1\",\"pg_index2\"],\"pg_table2\":[]}}}' ALL: collect metrics for all databases, schemas, tables, and indexes discovered. For example: collection_list: 'ALL' integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: '{\"postgres\":{\"public\":{\"pg_table1\":[\"pg_index1\",\"pg_index2\"],\"pg_table2\":[]}}}' TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL SSL configuration collection file integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: '[\"postgres\"]' ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: false SSL_ROOT_CERT_LOCATION: /etc/newrelic-infra/root_cert.crt SSL_CERT_LOCATION: /etc/newrelic-infra/postgresql.crt SSL_KEY_LOCATION: /etc/newrelic-infra/postgresql.key TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL custom query integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: ALL CUSTOM_METRICS_QUERY: >- select 'rows_inserted' as \"metric_name\", 'delta' as \"metric_type\", sd.tup_inserted as \"metric_value\", sd.datid as \"database_id\" from pg_stat_database sd; TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy PostgreSQL custom query config file An additional YAML configuration file with one or more custom SQL can be defined and the integration will need the path to the file in the CUSTOM_METRICS_CONFIG parameter. postgresql-config.yml integrations: - name: nri-postgresql env: USERNAME: postgres PASSWORD: pass HOSTNAME: psql-sample.localnet PORT: 6432 DATABASE: postgres COLLECT_DB_LOCK_METRICS: false COLLECTION_LIST: ALL CUSTOM_METRICS_CONFIG: \"path/to/postgresql-custom-query.yml\" TIMEOUT: 10 interval: 15s labels: env: production role: postgresql inventory_source: config/postgresql Copy postgresql-custom-query.yml --- queries: # Metric names are set to the column names in the query results - query: >- SELECT BG.checkpoints_timed AS scheduled_checkpoints_performed, BG.checkpoints_req AS requested_checkpoints_performed, BG.buffers_checkpoint AS buffers_written_during_checkpoint, BG.buffers_clean AS buffers_written_by_background_writer, BG.maxwritten_clean AS background_writer_stops, BG.buffers_backend AS buffers_written_by_backend, BG.buffers_alloc AS buffers_allocated FROM pg_stat_bgwriter BG; # database defaults to the auth database in the main config database: new_frontier_config_dev # If not set explicitly here, metric type will default to # 'gauge' for numbers and 'attribute' for strings metric_types: buffers_allocated: rate # If unset, sample_name defaults to PostgresqlCustomSample sample_name: MyCustomSample Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: PostgresqlDatabaseSample PostgresqlIndexSample PostgresqlInstanceSample PostgresqlTableSample PgBouncerSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The PostgreSQL integration collects the following database metric attributes. Some attributes apply to all PostgreSQL event types. Some metric names are prefixed with a category indicator and a period, such as db. or index. metric names. PostgresqlDatabaseSample metrics These attributes are attached to the PostgresqlDatabaseSample event type: PostgreSQLDatabaseSample attributes Description db.connections Number of backends currently connected to this database. db.maxconnections The maximum number of concurrent connections to the database server. db.commitsPerSecond Committed transactions per second. db.rollbacksPerSecond Transactions rolled back per second. db.readsPerSecond Number of disk blocks read in this database per second. db.bufferHitsPerSecond Number of times disk blocks were found already in the buffer cache, so that a read was not necessary. This only includes hits in the PostgreSQL buffer cache, not the operating system's file system cache. db.rowsReturnedPerSecond Rows returned by queries per second. db.rowsFetchedPerSecond Rows fetched by queries per second. db.rowsInsertedPerSecond Rows inserted per second. db.rowsUpdatedPerSecond Rows updated per second. db.rowsDeletedPerSecond Rows deleted per second. db.conflicts.tablespacePerSecond Number of queries in this database that have been canceled due to dropped tablespaces. db.conflicts.locksPerSecond Number of queries in this database that have been canceled due to lock timeouts. db.conflicts.snapshotPerSecond Number of queries in this database that have been canceled due to old snapshots. db.conflicts.bufferpinPerSecond Number of queries in this database that have been canceled due to pinned buffers. db.conflicts.deadlockPerSecond Number of queries in this database that have been canceled due to deadlocks. db.tempFilesCreatedPerSecond Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (for example, sorting or hashing), and regardless of the log_temp_files setting. db.tempWrittenInBytesPerSecond Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. db.deadlocksPerSecond Number of deadlocks detected in this database. db.readTimeInMillisecondsPerSecond Time spent reading data file blocks by backends in this database, in milliseconds. db.writeTimeInMillisecondsPerSecond Time spent writing data file blocks by backends in this database, in milliseconds. PostgresqlIndexSample metrics These attributes are attached to the PostgresqlIndexSample event type: PostgreSQLIndexSample attributes Description index.sizeInBytes The size of an index. index.rowsReadPerSecond The number of index entries returned by scans on this index. index.rowsFetchedPerSecond The number of index entries fetched by scans on this index. PostgresqlInstanceSample metrics These attributes are attached to the PostgresqlInstanceSample event type: PostgreSQLInstanceSample attributes Description bgwriter.checkpointsScheduledPerSecond Number of scheduled checkpoints that have been performed. bgwriter.checkpointsRequestedPerSecond Number of requested checkpoints that have been performed. bgwriter.buffersWrittenForCheckpointsPerSecond Number of buffers written during checkpoints. bgwriter.buffersWrittenByBackgroundWriterPerSecond Number of buffers written by the background writer. bgwriter.backgroundWriterStopsPerSecond Number of times the background writer stopped a cleaning scan because it had written too many buffers. bgwriter.buffersWrittenByBackendPerSecond Number of buffers written directly by a backend. bgwriter.buffersAllocatedPerSecond Number of buffers allocated. bgwriter.backendFsyncCallsPerSecond Number of times a backend had to execute its own fsync call. Normally the background writer handles them even when the backend does its own write. bgwriter.checkpointWriteTimeInMillisecondsPerSecond Total amount of time that has been spent in the portion of checkpoint processing where files are written to disk, in milliseconds. bgwriter.checkpointSyncTimeInMillisecondsPerSecond Total amount of time that has been spent in the portion of checkpoint processing where files are synchronized to disk, in milliseconds. PostgresqlTableSample metrics These attributes are attached to the PostgresqlTableSample event type: PostgreSQLTableSample attributes Description table.totalSizeInBytes The total disk space used by the table, including indexes and TOAST data. table.indexSizeInBytes The total disk space used by indexes attached to the specified table. table.liveRows Number of live rows. table.deadRows Number of dead rows. table.indexBlocksReadPerSecond The number of disk blocks read from all indexes on this table. table.indexBlocksHitPerSecond The number of buffer hits in all indexes on this table. table.indexToastBlocksReadPerSecond The number of disk blocks read from this table's TOAST table index. table.indexToastBlocksHitPerSecond The number of buffer hits in this table's TOAST table index. table.lastVacuum Time of last vacuum on table. table.lastAutoVacuum Time of last automatic vacuum on table. table.lastAnalyze Time of last analyze on table. table.lastAutoAnalyze Time of last automatic analyze on table. table.sequentialScansPerSecond Number of sequential scans initiated on this table per second. table.sequentialScanRowsFetchedPerSecond Number of live rows fetched by sequential scans per second. table.indexScansPerSecond Number of index scans initiated on this table. table.indexScanRowsFetchedPerSecon Number of live rows fetched by index scans. table.rowsInsertedPerSecond Rows inserted per second. table.rowsUpdatedPerSecond Rows updated per second. table.rowsDeletedPerSecond Rows deleted per second. table.bloatSizeInBytes Size of bloat in bytes. table.dataSizeInBytes Size of disk spaced used by the main fork of the table. table.bloatRatio Fraction of table data size that is bloat. PgBouncerSample metrics These attributes are attached to the PgBouncerSample event type: PgBouncerSample attributes Description pgbouncer.stats.transactionsPerSecond The transaction rate. pgbouncer.stats.queriesPerSecond The query rate. pgbouncer.stats.bytesInPerSecond The total network traffic received. pgbouncer.stats.bytesOutPerSecond The total network traffic sent. pgbouncer.stats.totalTransactionDurationInMillisecondsPerSecond Time spent by pgbouncer in transaction. pgbouncer.stats.totalQueryDurationInMillisecondsPerSecond Time spent by pgbouncer actively querying PostgreSQL. pgbouncer.stats.avgTransactionCount The average number of transactions per second in last stat period. pgbouncer.stats.avgTransactionDurationInMilliseconds The average transaction duration. pgbouncer.stats.avgQueryCount The average number of queries per second in last stat period. pgbouncer.stats.avgBytesIn The client network traffic received. pgbouncer.stats.avgBytesOut The client network traffic sent. pgbouncer.stats.avgQueryDurationInMilliseconds The average query duration. pgbouncer.pools.clientConnectionsActive Client connections linked to server connection and able to process queries. pgbouncer.pools.clientConnectionsWaiting Client connections waiting on a server connection. pgbouncer.pools.serverConnectionsActive Server connections linked to a client connection. pgbouncer.pools.serverConnectionsIdle Server connections idle and ready for a client query. pgbouncer.pools.serverConnectionsUsed Server connections idle more than server_check_delay, needing server_check_query. pgbouncer.pools.serverConnectionsTested Server connections currently running either server_reset_query or server_check_query. pgbouncer.pools.serverConnectionsLogin Server connections currently in the process of logging in. pgbouncer.pools.maxwaitInMilliseconds Age of oldest unserved client connection. Inventory data The PostgreSQL integration collects each setting from pg_settings along with its boot_val and reset_val. The inventory data appears on the Inventory page, under the config/postgresql source. Troubleshooting Here are some troubleshooting tips for the PostgreSQL integration: If you have connection problems, make sure you can connect to the cluster from the same box with psql. If you have problems collecting PgBouncer metrics, make sure you are connected to the instance through PgBouncer. Default port is 6432. If you get the error message Error creating list of entities to collect: pq: unsupported startup parameter: extra_float_digits, set ignore_startup_parameters = extra_float_digits in the PgBouncer config file. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.27545,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PostgreSQL monitoring <em>integration</em>",
        "sections": "PostgreSQL monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. PostgreSQL users and permissions Create a user with SELECT permissions on: pg_stat_database"
      },
      "id": "6174aeeee7b9d2451513d580"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/unix-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/varnish-cache-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.086,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2835,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.086,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2835,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/vmware-vsphere-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08588,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2835,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/windows-services-integration": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/infrastructure-agent-release-notes/new-relic-infrastructure-agent-1121/",
      "sections": [
        "Infrastructure agent v1.12.1",
        "Notes",
        "Added",
        "Fixed"
      ],
      "published_at": "2021-10-25T08:23:56Z",
      "title": "Infrastructure agent v1.12.1",
      "updated_at": "2021-03-13T03:15:26Z",
      "type": "docs",
      "external_id": "93e606131035b520d2d5f6be4220698349929793",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "Notes A new version of the agent has been released. Follow standard procedures to update your Infrastructure agent. Added Beta version (v0.1.0-beta) of nri-winservices is now packaged with the agent. For more information, see the Windows services integration documentation. Fixed d35cbe7 Fixed the sending of heartbeat samples to New Relic. 6503df0 Inventory is now fully re-sent if the host has been offline for 24 hours or if the agent ID changes. 4ccd9ff Fixed issue where running Docker auto discovery was leaking file descriptors.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 460.77838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "Notes A new version of the agent has been released. Follow standard procedures to update your Infrastructure agent. Added Beta version (v0.1.0-beta) of nri-winservices is now packaged with the agent. For more information, see the <em>Windows</em> <em>services</em> <em>integration</em> documentation. Fixed d35cbe7 Fixed"
      },
      "id": "6044211fe7b9d21ae05799cb"
    },
    {
      "sections": [
        "New Relic Metrics Adapter",
        "BETA FEATURE",
        "Requirements",
        "Installation",
        "Tip",
        "Configuration",
        "How it works",
        "Caution",
        "Troubleshooting",
        "Get verbose logs",
        "Get raw metrics",
        "Metrics not working"
      ],
      "title": "New Relic Metrics Adapter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "e2a825763b10ccf4bd1bd8423e2209f66dfb61bb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/newrelic-hpa-metrics-adapter/newrelic-metrics-adapter/",
      "published_at": "2021-10-24T16:35:54Z",
      "updated_at": "2021-10-24T09:00:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and services in your Kubernetes cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic and makes them available for the Horizontal Pod Autoscalers. The newrelic-k8s-metrics-adapter implements the external.metrics.k8s.io API to support the use of external metrics based New Relic NRQL queries results. Once deployed, the value for each configured metric is fetched using the NerdGraph API based on the configured NRQL query. The metrics adapter exposes the metrics over a secured endpoint with TLS. New Relic metrics adapter in a cluster. Requirements Kubernetes 1.16 or higher. The New Relic Kubernetes integration. New Relic's user API key. No other External Metrics Adapter installed in the cluster. Installation To install the New Relic Metrics Adapter, we provide the newrelic-k8s-metrics-adapter Helm chart, which is also included in the nri-bundle chart used to deploy all New Relic Kubernetes components. Install the New Relic Metrics Adapter by running the following command: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set ksm.enabled=true \\ --set kubeEvents.enabled=true \\ --set metrics-adapter.enabled=true \\ --set newrelic-k8s-metrics-adapter.personalAPIKey=YOUR_NEW_RELIC_PERSONAL_API_KEY \\ --set newrelic-k8s-metrics-adapter.config.accountID=YOUR_NEW_RELIC_ACCOUNT_ID \\ --set newrelic-k8s-metrics-adapter.config.externalMetrics.external_metric_name.query=NRQL query Copy Please notice and adjust the following flags: metrics-adapter.enabled: Must be set to true so the metrics adapter chart is installed. newrelic-k8s-metrics-adapter.personalAPIKey: Must be set to valid New Relic Personal API key. newrelic-k8s-metrics-adapter.accountID: Must be set to valid New Relic account where metrics are going to be fetched from. newrelic-k8s-metrics-adapter.config.externalMetrics.<var>external_metric_name</var>.<var>query</var>: Adds a new external metric where: <var>external_metric_name</var>: The metric name. <var>query</var>: The base NRQL query that is used to get the value for the metric. Tip Alternatively, you can use a values.yaml file that can be passed to the helm command with the --values flag. Values files can contain all parameters needed to configure the metrics explained in the configuration section. Configuration You can configure multiple metrics in the metrics adapter and change some parameters to modify the behaviour of the metrics cache and filtering. To see the full list and descriptions of all parameters that can be modified, refer to the chart README.md and values.yaml files. How it works The following values enable the metrics adapter on the nri-bundle chart installation: metrics-adapter: enabled: true newrelic-k8s-metrics-adapter: personalAPIKey: <Personal API Key> config: accountID: <Account ID> externalMetrics: nginx_average_requests: query: \"FROM Metric SELECT average(nginx.server.net.requestsPerSecond) SINCE 2 MINUTES AGO\" Copy Caution The default time span for metrics is 1h. Therefore, you should define queries with the SINCE clause to adjust the time span according to your environment and needs. There is an HPA consuming the external metric as follows: kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 metadata: name: nginx-scaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 10 metrics: - type: External external: metric: name: nginx_average_requests selector: matchLabels: k8s.namespaceName: nginx target: type: Value value: 10000 Copy Based on the HPA definition, the controller manager fetches the metrics from the external metrics API which are served by the New Relic metrics adapter. The New Relic metrics adapter receives the query including the nginx_average_requests metric name and all the selectors, and searches for a matching metric name in the internal memory based on the configured metrics. Then, it adds the selectors to the query to form a final query that is executed using NerdGraph to fetch the value from New Relic. The above example will generate a query like the following: FROM Metric SELECT average(nginx.server.net.requestsPerSecond) WHERE clusterName=<clusterName> AND `k8s.namespaceName`='nginx' SINCE 2 MINUTES AGO Copy Notice that a clusterName filter has been automatically added to the query to exclude metrics from other clusters in the same account. You can remove it by using the removeClusterFilter configuration parameter. Also the value is cached for a period of time defined by the cacheTTLSeconds configuration parameter, whose deafult is 30 seconds. Troubleshooting Get verbose logs Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. To get verbose logging details for an integration using Helm: Enable verbose logging: bash Copy $ helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=true newrelic/nri-bundle Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: bash Copy $ helm upgrade --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=false newrelic/nri-bundle Caution Verbose mode increases significantly the amount of information sent to log files. Enable this mode temporarily, only for troubleshooting purposes, and reset the log level when finished. Get raw metrics Sometimes it's useful to get the list of available metrics and also to get the current value of an specific metric. To get the list of metrics available, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/\" To get the value for a specific metric with a selector, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/namespaces/*/<metric_name>?labelSelector=<selector_key>=<selector_value>\" Tip You must replace <metric_name>, <selector_key> and <selector_value> with your values. Metrics not working There are some usual errors that could cause a metric fail to retrieve the value. These errors are showed in the status of the metrics when you describe the HPA or are printed when you get the raw metrics directly. executing query: NRQL Syntax Error: Error at line...: The query that is being run has syntax errors. The same error message gives you the executed query and position of the error. You can try this query inside the New Relic query builder and correct the configuration from the adapter. extracting return value: expected first value to be of type \"float64\", got %!q(<nil>): The query doesn't return any value. The same error message gives you the executed query so you can try this query inside the New Relic query builder and correct the configuration from the adapter or the match selectors in the HPA.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.19162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Integrations</em>",
        "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and <em>services</em> in your Kubernetes cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic"
      },
      "id": "6175209d28ccbcf310c6bb2f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "76e490b6f08befa3a9ed50cc021b8fc1ddb40423",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-10-24T21:58:47Z",
      "updated_at": "2021-10-24T00:54:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 85.183014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": ". The guided install works with most setups. But if it doesn&#x27;t suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server <em>integration</em>: Download the latest .MSI installer image from: 32-bit <em>Windows</em>"
      },
      "id": "6174aec2e7b9d227f313d6bf"
    }
  ],
  "/docs/infrastructure/host-integrations/host-integrations-list/zookeeper-monitoring-integration": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08588,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.2835,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.28333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " are also available in tarball format to allow installation outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk": [
    {
      "sections": [
        "Go language integration tutorial and build tools",
        "Integrations tutorial",
        "Important",
        "Tip",
        "Go language integration building package"
      ],
      "title": "Go language integration tutorial and build tools",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "068b7f6bc27cf0a360699121c3cf8c46c9398d6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/go-language-integration-tutorial-build-tools/",
      "published_at": "2021-10-25T01:07:54Z",
      "updated_at": "2021-03-13T01:25:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. Integrations tutorial Important The following tutorial is based on integrations using the SDK integration protocol v3. Find more information about the integration protocol v4 in the Github repository. The Go language integration-building tutorial on GitHub gives step-by-step procedures for building a Go language integration that reports Redis data. The tutorial shows how to build an integration using the Linux command line, but you can use the same techniques for a Windows integration with a standard Go install and PowerShell. The make command will not work with PowerShell, but you can use the Go commands inside it as a guide for building your integration. Tip You can create an on-host integration in any language, but Go is the language New Relic uses for its own integrations and build tools. To create an integration in another language, adhere to the integration file structures and JSON output requirements. Go language integration building package The tutorial relies on a New Relic Go language integration-building library package, which provides a set of useful Go functions and data structures. The package gives you tools that: Generate a \"scaffold\" integration structure with all the required fields. Read values from command-line arguments or environment variables. Generate and print JSON data to stdout. For information about file formats and JSON output specifications, see File requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.85349,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go language <em>integration</em> tutorial and build tools",
        "sections": "<em>Integrations</em> tutorial",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. <em>Integrations</em> tutorial Important The following tutorial is based on <em>integrations</em>"
      },
      "id": "6044091d28ccbcc5b22c6097"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.33963,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.33862,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you troubleshoot issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/get-started/go-language-integration-tutorial-build-tools": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-10-25T06:33:54Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.85493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.33963,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.33862,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you troubleshoot issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/get-started/introduction-infrastructure-integrations-sdk": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-10-25T06:33:54Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.85493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "Go language integration tutorial and build tools",
        "Integrations tutorial",
        "Important",
        "Tip",
        "Go language integration building package"
      ],
      "title": "Go language integration tutorial and build tools",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "068b7f6bc27cf0a360699121c3cf8c46c9398d6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/go-language-integration-tutorial-build-tools/",
      "published_at": "2021-10-25T01:07:54Z",
      "updated_at": "2021-03-13T01:25:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. Integrations tutorial Important The following tutorial is based on integrations using the SDK integration protocol v3. Find more information about the integration protocol v4 in the Github repository. The Go language integration-building tutorial on GitHub gives step-by-step procedures for building a Go language integration that reports Redis data. The tutorial shows how to build an integration using the Linux command line, but you can use the same techniques for a Windows integration with a standard Go install and PowerShell. The make command will not work with PowerShell, but you can use the Go commands inside it as a guide for building your integration. Tip You can create an on-host integration in any language, but Go is the language New Relic uses for its own integrations and build tools. To create an integration in another language, adhere to the integration file structures and JSON output requirements. Go language integration building package The tutorial relies on a New Relic Go language integration-building library package, which provides a set of useful Go functions and data structures. The package gives you tools that: Generate a \"scaffold\" integration structure with all the required fields. Read values from command-line arguments or environment variables. Generate and print JSON data to stdout. For information about file formats and JSON output specifications, see File requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.85349,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go language <em>integration</em> tutorial and build tools",
        "sections": "<em>Integrations</em> tutorial",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. <em>Integrations</em> tutorial Important The following tutorial is based on <em>integrations</em>"
      },
      "id": "6044091d28ccbcc5b22c6097"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.33963,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview": [
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91168,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-10-25T00:57:30Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.32954,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    },
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-10-25T06:33:54Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.24493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/specifications/host-integration-executable-file-json-specifications": [
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91165,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you troubleshoot issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    },
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-10-25T00:57:30Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.32953,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/specifications/host-integration-files": [
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you troubleshoot issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    },
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-10-25T00:57:30Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.32953,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    },
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-10-25T06:33:54Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.24493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format": [
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91165,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91025,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you troubleshoot issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    },
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-10-25T06:33:54Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.24493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format": [
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91165,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91025,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you troubleshoot issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    },
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-10-25T00:57:30Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.32951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/specifications/integration-logging-recommendations": [
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.91025,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you troubleshoot issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    },
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-10-25T00:57:30Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.3295,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-attributes": [
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-10-25T00:57:30Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.05942,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.76674,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.76572,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you <em>troubleshoot</em> issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    }
  ],
  "/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-infrastructure-integration-data": [
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-10-25T00:57:30Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.0594,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-10-25T00:57:29Z",
      "updated_at": "2021-09-27T16:02:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent A configuration file For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.76672,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integration configuration overview",
        "Overview of how configuration works",
        "Configuration file location",
        "Configuration formats"
      ],
      "title": "On-host integration configuration overview",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "b580c10bb0a6142dcb204639762561b65bd6ceb9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview/",
      "published_at": "2021-10-25T01:17:59Z",
      "updated_at": "2021-09-27T16:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host integrations send data to the infrastructure agent, which in turn sends that data to New Relic. How integrations interact with the agent is controlled by each integration's config. Understanding more about configuration can help you troubleshoot issues with your on-host integration. Overview of how configuration works New Relic's on-host integrations are external programs executed by the infrastructure agent. Each integration monitors a specific service. An integration has, at minimum, these files: An executable that exports various types of data in a JSON format expected by the agent One or more YAML-format config files (for example, the Apache integration configuration). (We recommend linting YAML config files before use to avoid formatting issues.) Note that in addition to the specific on-host integration's configuration, you can also edit the infrastructure agent's configuration. Configuration file location With standard on-host integration installations, the configuration is located in the infrastructure agent's directory. The agent determines this config location by a setting in its own configuration file. For some implementations, the integration's configuration will be located elsewhere. For example: Services running on Kubernetes: The configuration is located in the Kubernetes integration config file. Services running on Amazon ECS: The configuration is placed in the AWS console. Configuration formats On-host integrations use two configuration formats: Standard: Starting December 2019, infrastructure agent version 1.8.0 began supporting a new format used by some integrations. This format uses a single configuration file and provides other improvements. For more details, see Standard configuration. Legacy: This is the format used by most on-host integrations. This configuration uses two files: a definition file and a configuration file. For more details, see Legacy configuration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.76572,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> configuration overview",
        "sections": "On-host <em>integration</em> configuration overview",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic&#x27;s on-host <em>integrations</em> send data to the <em>infrastructure</em> agent, which in turn sends that data to New Relic. How <em>integrations</em> interact with the agent is controlled by each integration&#x27;s config. Understanding more about configuration can help you <em>troubleshoot</em> issues with your on-host"
      },
      "id": "6044091d28ccbc95852c60cb"
    }
  ],
  "/docs/infrastructure/host-integrations/installation/container-auto-discovery-host-integrations": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.02182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Consumer offset",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Labels/Custom Attributes",
        "Configure KafkaBrokerSample and KafkaTopicSample collection",
        "Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper):",
        "Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap):",
        "JMX options (Applies to all JMX connections on the instance):",
        "Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL):",
        "Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL):",
        "Broker Collection filtering:",
        "Configure KafkaConsumerSample and KafkaProducerSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on the instance):",
        "Configure KafkaOffsetSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on an instance):",
        "Example configurations",
        "ZOOKEEPER DISCOVERY",
        "ZOOKEEPER SSL DISCOVERY",
        "BOOOTSTRAP DISCOVERY",
        "BOOOTSTRAP DISCOVERY TLS",
        "BOOOTSTRAP DISCOVERY KERBEROS AUTH",
        "ZOOKEEPER DISCOVERY TOPIC BUCKET",
        "JAVA CONSUMER & PRODUCER",
        "CONSUMER OFFSET",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "c7571c04ab861b450146b37355c2e5cafb34eb8e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-10-25T01:14:16Z",
      "updated_at": "2021-10-24T00:50:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. The topics must have DESCRIBE access. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. If autodiscovery is set to bootstrap, the JMX settings defined for the bootstrap broker will be applied for all other discovered brokers, so the Port and other settings should be the same on all the brokers. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Consumer offset The offset of the consumer and consumer groups of the topics as well as the lag, can be retrieved as a KafkaOffsetSample with the CONSUMER_OFFSET=true flag but should be in a separate instance because when this flag is activated the instance will not collect other Samples. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored to get more specific metadata through the same mechanism (JMX). This will generate KafkaConsumerSamples and KafkaProducerSamples. JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to Kafka are defined using the env section of the configuration file. These settings control the connection to your Brokers, Zookeeper and JMX as well as other security settings and features. The list of valid settings is described in the next section of this document. Important The integration has two modes of operation, which are mutually exclusive: “Core” collection and \"Consumer offset collection\" controlled by the CONSUMER_OFFSET parameter: CONSUMER_OFFSET = true is consumer offset collection mode, and will produce KafkaOffsetSample. CONSUMER_OFFSET = false is core collection mode, which can collect the rest of the samples (KafkaBrokerSample, KafkaTopicSample, KafkaProducerSample, KafkaConsumerSample). These modes are separated because consumer offset collection takes a long time to run and has high performance requirements. The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Configure KafkaBrokerSample and KafkaTopicSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Broker Collection filtering: Setting Description Default Applies To LOCAL_ONLY_COLLECTION Collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Environments that use discovery (e.g. Kubernetes) must be set to true because othwerwise brokers will be discovered twice: By the integration, and by the discovery mechanism, leading to duplicate data. Note that activating this flag will skip KafkaTopicSample collection false M/I TOPIC_MODE Determines how many topics we collect. Options are all, none, list, or regex. none M/I TOPIC_LIST JSON array of topic names to monitor. Only in effect if topic_mode is set to list. [] M/I TOPIC_REGEX Regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. N/A M/I TOPIC_BUCKET Used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. 1/1 M/I COLLECT_TOPIC_SIZE Collect the metric Topic size. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I COLLECT_TOPIC_OFFSET Collect the metric Topic offset. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I Configure KafkaConsumerSample and KafkaProducerSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I PRODUCERS Producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaProducerSample. Example: [{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I CONSUMERS Consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaConsumerSample. Example: [{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I DEFAULT_JMX_HOST The default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. localhost M/I DEFAULT_JMX_PORT The default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. 9999 M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted from a producer or consumer configuration, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted from a producer or consumer configuration, this value will be used. admin M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false JMX SSL and timeout options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Configure KafkaOffsetSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I CONSUMER_OFFSET Populate consumer offset data in KafkaOffsetSample if set to true. Note that this option will skip Broker/Consumer/Producer collection and only collect KafkaOffsetSample false M/I CONSUMER_GROUP_REGEX regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. This option must be set when CONSUMER_OFFSET is true. N/A M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX SSL and timeout options (Applies to all JMX connections on an instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Example configurations ZOOKEEPER DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from two different JMX hosts : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}, {\"host\": \"localhost2\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: all interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER SSL DISCOVERY This configuration collects Metrics and Inventory discovering the brokers from a JMX host with SSL : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password KEY_STORE: \"/path/to/your/keystore\" KEY_STORE_PASSWORD: keystore_password TRUST_STORE: \"/path/to/your/truststore\" TRUST_STORE_PASSWORD: truststore_password TIMEOUT: 10000 #The timeout for individual JMX queries in milliseconds. interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from one bootstrap broker : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT BOOTSTRAP_BROKER_JMX_PORT: 9999 # This same port will be used to connect to all discover broker JMX BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password LOCAL_ONLY_COLLECTION: false COLLECT_BROKER_TOPIC_DATA: true TOPIC_MODE: \"all\" COLLECT_TOPIC_SIZE: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY TLS This configuration collects only Metrics discovering the brokers from one bootstrap broker listening with TLS protocol : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments TLS_CA_FILE: \"/path/to/CA.pem\" TLS_CERT_FILE: \"/path/to/cert.pem\" TLS_KEY_FILE: \"/path/to/key.pem\" TLS_INSECURE_SKIP_VERIFY: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY KERBEROS AUTH This configuration collects only Metrics discovering the brokers from one bootstrap broker in a Kerberos Auth Cluster : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # Currently support PLAINTEXT and SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments SASL_MECHANISM: GSSAPI SASL_GSSAPI_REALM: SOMECORP.COM SASL_GSSAPI_SERVICE_NAME: Kafka SASL_GSSAPI_USERNAME: kafka SASL_GSSAPI_KEY_TAB_PATH: /etc/newrelic-infra/kafka.keytab SASL_GSSAPI_KERBEROS_CONFIG_PATH: /etc/krb5.conf SASL_GSSAPI_DISABLE_FAST_NEGOTIATION: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER DISCOVERY TOPIC BUCKET This configuration collects Metrics splitting topic collection between 3 different instances: integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '1/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '2/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '3/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy JAVA CONSUMER & PRODUCER This gives an example for collecting JMX metrics from Java consumers and producers: integrations: - name: nri-kafka env: METRICS: \"true\" CLUSTER_NAME: \"testcluster3\" PRODUCERS: '[{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' CONSUMERS: '[{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' DEFAULT_JMX_HOST: \"localhost\" DEFAULT_JMX_PORT: \"9999\" interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy CONSUMER OFFSET This configuration collects consumer offset Metrics and Inventory for the cluster: integrations: - name: nri-kafka env: CONSUMER_OFFSET: true CLUSTER_NAME: testcluster3 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # A regex pattern that matches the consumer groups to collect metrics from CONSUMER_GROUP_REGEX: '.*' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. topic.diskSize In disk Topic size. Only present if COLLECT_TOPIC_SIZE is enabled. topic.offset Topic offset. Only present if COLLECT_TOPIC_OFFSET is enabled. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.63895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed"
      },
      "id": "6174adee196a6791332f0a5f"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.40866,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " are also available in tarball format to allow <em>installation</em> outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/installation/install-infrastructure-host-integrations": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.02173,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Consumer offset",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Labels/Custom Attributes",
        "Configure KafkaBrokerSample and KafkaTopicSample collection",
        "Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper):",
        "Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap):",
        "JMX options (Applies to all JMX connections on the instance):",
        "Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL):",
        "Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL):",
        "Broker Collection filtering:",
        "Configure KafkaConsumerSample and KafkaProducerSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on the instance):",
        "Configure KafkaOffsetSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on an instance):",
        "Example configurations",
        "ZOOKEEPER DISCOVERY",
        "ZOOKEEPER SSL DISCOVERY",
        "BOOOTSTRAP DISCOVERY",
        "BOOOTSTRAP DISCOVERY TLS",
        "BOOOTSTRAP DISCOVERY KERBEROS AUTH",
        "ZOOKEEPER DISCOVERY TOPIC BUCKET",
        "JAVA CONSUMER & PRODUCER",
        "CONSUMER OFFSET",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "c7571c04ab861b450146b37355c2e5cafb34eb8e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-10-25T01:14:16Z",
      "updated_at": "2021-10-24T00:50:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. The topics must have DESCRIBE access. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. If autodiscovery is set to bootstrap, the JMX settings defined for the bootstrap broker will be applied for all other discovered brokers, so the Port and other settings should be the same on all the brokers. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Consumer offset The offset of the consumer and consumer groups of the topics as well as the lag, can be retrieved as a KafkaOffsetSample with the CONSUMER_OFFSET=true flag but should be in a separate instance because when this flag is activated the instance will not collect other Samples. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored to get more specific metadata through the same mechanism (JMX). This will generate KafkaConsumerSamples and KafkaProducerSamples. JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to Kafka are defined using the env section of the configuration file. These settings control the connection to your Brokers, Zookeeper and JMX as well as other security settings and features. The list of valid settings is described in the next section of this document. Important The integration has two modes of operation, which are mutually exclusive: “Core” collection and \"Consumer offset collection\" controlled by the CONSUMER_OFFSET parameter: CONSUMER_OFFSET = true is consumer offset collection mode, and will produce KafkaOffsetSample. CONSUMER_OFFSET = false is core collection mode, which can collect the rest of the samples (KafkaBrokerSample, KafkaTopicSample, KafkaProducerSample, KafkaConsumerSample). These modes are separated because consumer offset collection takes a long time to run and has high performance requirements. The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Configure KafkaBrokerSample and KafkaTopicSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Broker Collection filtering: Setting Description Default Applies To LOCAL_ONLY_COLLECTION Collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Environments that use discovery (e.g. Kubernetes) must be set to true because othwerwise brokers will be discovered twice: By the integration, and by the discovery mechanism, leading to duplicate data. Note that activating this flag will skip KafkaTopicSample collection false M/I TOPIC_MODE Determines how many topics we collect. Options are all, none, list, or regex. none M/I TOPIC_LIST JSON array of topic names to monitor. Only in effect if topic_mode is set to list. [] M/I TOPIC_REGEX Regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. N/A M/I TOPIC_BUCKET Used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. 1/1 M/I COLLECT_TOPIC_SIZE Collect the metric Topic size. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I COLLECT_TOPIC_OFFSET Collect the metric Topic offset. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I Configure KafkaConsumerSample and KafkaProducerSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I PRODUCERS Producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaProducerSample. Example: [{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I CONSUMERS Consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaConsumerSample. Example: [{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I DEFAULT_JMX_HOST The default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. localhost M/I DEFAULT_JMX_PORT The default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. 9999 M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted from a producer or consumer configuration, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted from a producer or consumer configuration, this value will be used. admin M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false JMX SSL and timeout options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Configure KafkaOffsetSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I CONSUMER_OFFSET Populate consumer offset data in KafkaOffsetSample if set to true. Note that this option will skip Broker/Consumer/Producer collection and only collect KafkaOffsetSample false M/I CONSUMER_GROUP_REGEX regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. This option must be set when CONSUMER_OFFSET is true. N/A M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX SSL and timeout options (Applies to all JMX connections on an instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Example configurations ZOOKEEPER DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from two different JMX hosts : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}, {\"host\": \"localhost2\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: all interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER SSL DISCOVERY This configuration collects Metrics and Inventory discovering the brokers from a JMX host with SSL : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password KEY_STORE: \"/path/to/your/keystore\" KEY_STORE_PASSWORD: keystore_password TRUST_STORE: \"/path/to/your/truststore\" TRUST_STORE_PASSWORD: truststore_password TIMEOUT: 10000 #The timeout for individual JMX queries in milliseconds. interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from one bootstrap broker : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT BOOTSTRAP_BROKER_JMX_PORT: 9999 # This same port will be used to connect to all discover broker JMX BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password LOCAL_ONLY_COLLECTION: false COLLECT_BROKER_TOPIC_DATA: true TOPIC_MODE: \"all\" COLLECT_TOPIC_SIZE: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY TLS This configuration collects only Metrics discovering the brokers from one bootstrap broker listening with TLS protocol : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments TLS_CA_FILE: \"/path/to/CA.pem\" TLS_CERT_FILE: \"/path/to/cert.pem\" TLS_KEY_FILE: \"/path/to/key.pem\" TLS_INSECURE_SKIP_VERIFY: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY KERBEROS AUTH This configuration collects only Metrics discovering the brokers from one bootstrap broker in a Kerberos Auth Cluster : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # Currently support PLAINTEXT and SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments SASL_MECHANISM: GSSAPI SASL_GSSAPI_REALM: SOMECORP.COM SASL_GSSAPI_SERVICE_NAME: Kafka SASL_GSSAPI_USERNAME: kafka SASL_GSSAPI_KEY_TAB_PATH: /etc/newrelic-infra/kafka.keytab SASL_GSSAPI_KERBEROS_CONFIG_PATH: /etc/krb5.conf SASL_GSSAPI_DISABLE_FAST_NEGOTIATION: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER DISCOVERY TOPIC BUCKET This configuration collects Metrics splitting topic collection between 3 different instances: integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '1/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '2/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '3/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy JAVA CONSUMER & PRODUCER This gives an example for collecting JMX metrics from Java consumers and producers: integrations: - name: nri-kafka env: METRICS: \"true\" CLUSTER_NAME: \"testcluster3\" PRODUCERS: '[{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' CONSUMERS: '[{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' DEFAULT_JMX_HOST: \"localhost\" DEFAULT_JMX_PORT: \"9999\" interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy CONSUMER OFFSET This configuration collects consumer offset Metrics and Inventory for the cluster: integrations: - name: nri-kafka env: CONSUMER_OFFSET: true CLUSTER_NAME: testcluster3 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # A regex pattern that matches the consumer groups to collect metrics from CONSUMER_GROUP_REGEX: '.*' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. topic.diskSize In disk Topic size. Only present if COLLECT_TOPIC_SIZE is enabled. topic.offset Topic offset. Only present if COLLECT_TOPIC_OFFSET is enabled. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.63885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed"
      },
      "id": "6174adee196a6791332f0a5f"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.40857,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " are also available in tarball format to allow <em>installation</em> outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/installation/secrets-management": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.02173,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Consumer offset",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Labels/Custom Attributes",
        "Configure KafkaBrokerSample and KafkaTopicSample collection",
        "Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper):",
        "Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap):",
        "JMX options (Applies to all JMX connections on the instance):",
        "Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL):",
        "Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL):",
        "Broker Collection filtering:",
        "Configure KafkaConsumerSample and KafkaProducerSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on the instance):",
        "Configure KafkaOffsetSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on an instance):",
        "Example configurations",
        "ZOOKEEPER DISCOVERY",
        "ZOOKEEPER SSL DISCOVERY",
        "BOOOTSTRAP DISCOVERY",
        "BOOOTSTRAP DISCOVERY TLS",
        "BOOOTSTRAP DISCOVERY KERBEROS AUTH",
        "ZOOKEEPER DISCOVERY TOPIC BUCKET",
        "JAVA CONSUMER & PRODUCER",
        "CONSUMER OFFSET",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "c7571c04ab861b450146b37355c2e5cafb34eb8e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-10-25T01:14:16Z",
      "updated_at": "2021-10-24T00:50:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. The topics must have DESCRIBE access. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. If autodiscovery is set to bootstrap, the JMX settings defined for the bootstrap broker will be applied for all other discovered brokers, so the Port and other settings should be the same on all the brokers. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Consumer offset The offset of the consumer and consumer groups of the topics as well as the lag, can be retrieved as a KafkaOffsetSample with the CONSUMER_OFFSET=true flag but should be in a separate instance because when this flag is activated the instance will not collect other Samples. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored to get more specific metadata through the same mechanism (JMX). This will generate KafkaConsumerSamples and KafkaProducerSamples. JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to Kafka are defined using the env section of the configuration file. These settings control the connection to your Brokers, Zookeeper and JMX as well as other security settings and features. The list of valid settings is described in the next section of this document. Important The integration has two modes of operation, which are mutually exclusive: “Core” collection and \"Consumer offset collection\" controlled by the CONSUMER_OFFSET parameter: CONSUMER_OFFSET = true is consumer offset collection mode, and will produce KafkaOffsetSample. CONSUMER_OFFSET = false is core collection mode, which can collect the rest of the samples (KafkaBrokerSample, KafkaTopicSample, KafkaProducerSample, KafkaConsumerSample). These modes are separated because consumer offset collection takes a long time to run and has high performance requirements. The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Configure KafkaBrokerSample and KafkaTopicSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Broker Collection filtering: Setting Description Default Applies To LOCAL_ONLY_COLLECTION Collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Environments that use discovery (e.g. Kubernetes) must be set to true because othwerwise brokers will be discovered twice: By the integration, and by the discovery mechanism, leading to duplicate data. Note that activating this flag will skip KafkaTopicSample collection false M/I TOPIC_MODE Determines how many topics we collect. Options are all, none, list, or regex. none M/I TOPIC_LIST JSON array of topic names to monitor. Only in effect if topic_mode is set to list. [] M/I TOPIC_REGEX Regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. N/A M/I TOPIC_BUCKET Used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. 1/1 M/I COLLECT_TOPIC_SIZE Collect the metric Topic size. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I COLLECT_TOPIC_OFFSET Collect the metric Topic offset. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I Configure KafkaConsumerSample and KafkaProducerSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I PRODUCERS Producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaProducerSample. Example: [{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I CONSUMERS Consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaConsumerSample. Example: [{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I DEFAULT_JMX_HOST The default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. localhost M/I DEFAULT_JMX_PORT The default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. 9999 M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted from a producer or consumer configuration, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted from a producer or consumer configuration, this value will be used. admin M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false JMX SSL and timeout options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Configure KafkaOffsetSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I CONSUMER_OFFSET Populate consumer offset data in KafkaOffsetSample if set to true. Note that this option will skip Broker/Consumer/Producer collection and only collect KafkaOffsetSample false M/I CONSUMER_GROUP_REGEX regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. This option must be set when CONSUMER_OFFSET is true. N/A M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX SSL and timeout options (Applies to all JMX connections on an instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Example configurations ZOOKEEPER DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from two different JMX hosts : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}, {\"host\": \"localhost2\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: all interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER SSL DISCOVERY This configuration collects Metrics and Inventory discovering the brokers from a JMX host with SSL : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password KEY_STORE: \"/path/to/your/keystore\" KEY_STORE_PASSWORD: keystore_password TRUST_STORE: \"/path/to/your/truststore\" TRUST_STORE_PASSWORD: truststore_password TIMEOUT: 10000 #The timeout for individual JMX queries in milliseconds. interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from one bootstrap broker : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT BOOTSTRAP_BROKER_JMX_PORT: 9999 # This same port will be used to connect to all discover broker JMX BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password LOCAL_ONLY_COLLECTION: false COLLECT_BROKER_TOPIC_DATA: true TOPIC_MODE: \"all\" COLLECT_TOPIC_SIZE: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY TLS This configuration collects only Metrics discovering the brokers from one bootstrap broker listening with TLS protocol : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments TLS_CA_FILE: \"/path/to/CA.pem\" TLS_CERT_FILE: \"/path/to/cert.pem\" TLS_KEY_FILE: \"/path/to/key.pem\" TLS_INSECURE_SKIP_VERIFY: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY KERBEROS AUTH This configuration collects only Metrics discovering the brokers from one bootstrap broker in a Kerberos Auth Cluster : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # Currently support PLAINTEXT and SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments SASL_MECHANISM: GSSAPI SASL_GSSAPI_REALM: SOMECORP.COM SASL_GSSAPI_SERVICE_NAME: Kafka SASL_GSSAPI_USERNAME: kafka SASL_GSSAPI_KEY_TAB_PATH: /etc/newrelic-infra/kafka.keytab SASL_GSSAPI_KERBEROS_CONFIG_PATH: /etc/krb5.conf SASL_GSSAPI_DISABLE_FAST_NEGOTIATION: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER DISCOVERY TOPIC BUCKET This configuration collects Metrics splitting topic collection between 3 different instances: integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '1/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '2/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '3/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy JAVA CONSUMER & PRODUCER This gives an example for collecting JMX metrics from Java consumers and producers: integrations: - name: nri-kafka env: METRICS: \"true\" CLUSTER_NAME: \"testcluster3\" PRODUCERS: '[{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' CONSUMERS: '[{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' DEFAULT_JMX_HOST: \"localhost\" DEFAULT_JMX_PORT: \"9999\" interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy CONSUMER OFFSET This configuration collects consumer offset Metrics and Inventory for the cluster: integrations: - name: nri-kafka env: CONSUMER_OFFSET: true CLUSTER_NAME: testcluster3 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # A regex pattern that matches the consumer groups to collect metrics from CONSUMER_GROUP_REGEX: '.*' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. topic.diskSize In disk Topic size. Only present if COLLECT_TOPIC_SIZE is enabled. topic.offset Topic offset. Only present if COLLECT_TOPIC_OFFSET is enabled. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.63885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed"
      },
      "id": "6174adee196a6791332f0a5f"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.40857,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " are also available in tarball format to allow <em>installation</em> outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/installation/update-infrastructure-host-integration-package": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.0216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Consumer offset",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Labels/Custom Attributes",
        "Configure KafkaBrokerSample and KafkaTopicSample collection",
        "Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper):",
        "Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap):",
        "JMX options (Applies to all JMX connections on the instance):",
        "Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL):",
        "Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL):",
        "Broker Collection filtering:",
        "Configure KafkaConsumerSample and KafkaProducerSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on the instance):",
        "Configure KafkaOffsetSample collection",
        "JMX SSL and timeout options (Applies to all JMX connections on an instance):",
        "Example configurations",
        "ZOOKEEPER DISCOVERY",
        "ZOOKEEPER SSL DISCOVERY",
        "BOOOTSTRAP DISCOVERY",
        "BOOOTSTRAP DISCOVERY TLS",
        "BOOOTSTRAP DISCOVERY KERBEROS AUTH",
        "ZOOKEEPER DISCOVERY TOPIC BUCKET",
        "JAVA CONSUMER & PRODUCER",
        "CONSUMER OFFSET",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "c7571c04ab861b450146b37355c2e5cafb34eb8e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-10-25T01:14:16Z",
      "updated_at": "2021-10-24T00:50:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. The topics must have DESCRIBE access. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. If autodiscovery is set to bootstrap, the JMX settings defined for the bootstrap broker will be applied for all other discovered brokers, so the Port and other settings should be the same on all the brokers. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Consumer offset The offset of the consumer and consumer groups of the topics as well as the lag, can be retrieved as a KafkaOffsetSample with the CONSUMER_OFFSET=true flag but should be in a separate instance because when this flag is activated the instance will not collect other Samples. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored to get more specific metadata through the same mechanism (JMX). This will generate KafkaConsumerSamples and KafkaProducerSamples. JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to Kafka are defined using the env section of the configuration file. These settings control the connection to your Brokers, Zookeeper and JMX as well as other security settings and features. The list of valid settings is described in the next section of this document. Important The integration has two modes of operation, which are mutually exclusive: “Core” collection and \"Consumer offset collection\" controlled by the CONSUMER_OFFSET parameter: CONSUMER_OFFSET = true is consumer offset collection mode, and will produce KafkaOffsetSample. CONSUMER_OFFSET = false is core collection mode, which can collect the rest of the samples (KafkaBrokerSample, KafkaTopicSample, KafkaProducerSample, KafkaConsumerSample). These modes are separated because consumer offset collection takes a long time to run and has high performance requirements. The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Configure KafkaBrokerSample and KafkaTopicSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Broker Collection filtering: Setting Description Default Applies To LOCAL_ONLY_COLLECTION Collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Environments that use discovery (e.g. Kubernetes) must be set to true because othwerwise brokers will be discovered twice: By the integration, and by the discovery mechanism, leading to duplicate data. Note that activating this flag will skip KafkaTopicSample collection false M/I TOPIC_MODE Determines how many topics we collect. Options are all, none, list, or regex. none M/I TOPIC_LIST JSON array of topic names to monitor. Only in effect if topic_mode is set to list. [] M/I TOPIC_REGEX Regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. N/A M/I TOPIC_BUCKET Used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. 1/1 M/I COLLECT_TOPIC_SIZE Collect the metric Topic size. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I COLLECT_TOPIC_OFFSET Collect the metric Topic offset. Options are true or false, defaults to false. This is a resource-intensive metric to collect, especially against many topics. false M/I Configure KafkaConsumerSample and KafkaProducerSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I PRODUCERS Producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaProducerSample. Example: [{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I CONSUMERS Consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are the optional JMX settings and use the default if unspecified. Required to produce KafkaConsumerSample. Example: [{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}] [] M/I DEFAULT_JMX_HOST The default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. localhost M/I DEFAULT_JMX_PORT The default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. 9999 M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted from a producer or consumer configuration, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted from a producer or consumer configuration, this value will be used. admin M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false JMX SSL and timeout options (Applies to all JMX connections on the instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Configure KafkaOffsetSample collection The Kafka integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To CLUSTER_NAME user-defined name to uniquely identify the cluster being monitored. Required. N/A M/I KAFKA_VERSION The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker. Versions older than 1.0.0 may be missing some features. Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version. 1.0.0 M/I AUTODISCOVER_STRATEGY the method of discovering brokers. Options are zookeeper or bootstrap. zookeeper M/I CONSUMER_OFFSET Populate consumer offset data in KafkaOffsetSample if set to true. Note that this option will skip Broker/Consumer/Producer collection and only collect KafkaOffsetSample false M/I CONSUMER_GROUP_REGEX regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. This option must be set when CONSUMER_OFFSET is true. N/A M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): Setting Description Default Applies To ZOOKEEPER_HOSTS The list of Apache ZooKeeper hosts (in JSON format) that need to be connected. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. [] M/I ZOOKEEPER_AUTH_SCHEME The ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. N/A M/I ZOOKEEPER_AUTH_SECRET The ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. N/A M/I ZOOKEEPER_PATH The Zookeeper node under which the Kafka configuration resides. Defaults to /. N/A M/I PREFERRED_LISTENER Use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. N/A M/I Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): Setting Description Default Applies To BOOTSTRAP_BROKER_HOST The host for the bootstrap broker. If CONSUMER_OFFSET is set to false KafkaBrokerSamples and KafkaTopicSamples will be collected. N/A M/I BOOTSTRAP_BROKER_KAFKA_PORT The Kafka port for the bootstrap broker. N/A M/I BOOTSTRAP_BROKER_KAFKA_PROTOCOL The protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note the SASL_* protocols only support Kerberos (GSSAPI) authentication. PLAINTEXT M/I BOOTSTRAP_BROKER_JMX_PORT The JMX port to use for collection on each broker in the cluster. Note that all discovered brokers should have JMX active on this port N/A M/I BOOTSTRAP_BROKER_JMX_USER The JMX user to use for collection on each broker in the cluster. N/A M/I BOOTSTRAP_BROKER_JMX_PASSWORD The JMX password to use for collection on each broker in the cluster. N/A M/I JMX SSL and timeout options (Applies to all JMX connections on an instance): Setting Description Default Applies To KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the JMX SSL key store. N/A M/I TRUST_STORE The filepath of the trust keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the JMX trust store. N/A M/I DEFAULT_JMX_USER The default user that is connecting to the JMX host to collect metrics. If the username field is omitted for a JMX host, this value will be used. admin M/I DEFAULT_JMX_PASSWORD The default password to connect to the JMX host. If the password field is omitted for a JMX host, this value will be used. admin M/I TIMEOUT The timeout for individual JMX queries in milliseconds. 10000 M/I Broker TLS connection options (Needed if the broker protocol is SSL or SASL_SSL): Setting Description Default Applies To TLS_CA_FILE The certificate authority file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_CERT_FILE The client certificate file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_KEY_FILE The client key file for SSL and SASL_SSL listeners, in PEM format. N/A M/I TLS_INSECURE_SKIP_VERIFY Skip verifying the server's certificate chain and host name. false M/I Broker SASL and Kerberos connection options (Needed if the broker protocol is SASL_PLAINTEXT or SASL_SSL): Setting Description Default Applies To SASL_MECHANISM The type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. N/A M/I SASL_USERNAME SASL username required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_PASSWORD SASL password required with the PLAIN and SCRAM mechanisms. N/A M/I SASL_GSSAPI_REALM Kerberos realm required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_SERVICE_NAME Kerberos service name required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_USERNAME Kerberos username required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KEY_TAB_PATH Kerberos key tab path required with the GSSAPI mechanism. N/A M/I SASL_GSSAPI_KERBEROS_CONFIG_PATH Kerberos config path required with the GSSAPI mechanism. /etc/krb5.conf M/I SASL_GSSAPI_DISABLE_FAST_NEGOTIATION Disable FAST negotiation. false M/I Example configurations ZOOKEEPER DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from two different JMX hosts : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}, {\"host\": \"localhost2\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: all interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER SSL DISCOVERY This configuration collects Metrics and Inventory discovering the brokers from a JMX host with SSL : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password KEY_STORE: \"/path/to/your/keystore\" KEY_STORE_PASSWORD: keystore_password TRUST_STORE: \"/path/to/your/truststore\" TRUST_STORE_PASSWORD: truststore_password TIMEOUT: 10000 #The timeout for individual JMX queries in milliseconds. interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY This configuration collects Metrics and Inventory including all topics discovering the brokers from one bootstrap broker : integrations: - name: nri-kafka env: CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT BOOTSTRAP_BROKER_JMX_PORT: 9999 # This same port will be used to connect to all discover broker JMX BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password LOCAL_ONLY_COLLECTION: false COLLECT_BROKER_TOPIC_DATA: true TOPIC_MODE: \"all\" COLLECT_TOPIC_SIZE: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY TLS This configuration collects only Metrics discovering the brokers from one bootstrap broker listening with TLS protocol : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments TLS_CA_FILE: \"/path/to/CA.pem\" TLS_CERT_FILE: \"/path/to/cert.pem\" TLS_KEY_FILE: \"/path/to/key.pem\" TLS_INSECURE_SKIP_VERIFY: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy BOOOTSTRAP DISCOVERY KERBEROS AUTH This configuration collects only Metrics discovering the brokers from one bootstrap broker in a Kerberos Auth Cluster : integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # Currently support PLAINTEXT and SSL BOOTSTRAP_BROKER_JMX_PORT: 9999 BOOTSTRAP_BROKER_JMX_USER: admin BOOTSTRAP_BROKER_JMX_PASSWORD: password # Kerberos authentication arguments SASL_MECHANISM: GSSAPI SASL_GSSAPI_REALM: SOMECORP.COM SASL_GSSAPI_SERVICE_NAME: Kafka SASL_GSSAPI_USERNAME: kafka SASL_GSSAPI_KEY_TAB_PATH: /etc/newrelic-infra/kafka.keytab SASL_GSSAPI_KERBEROS_CONFIG_PATH: /etc/krb5.conf SASL_GSSAPI_DISABLE_FAST_NEGOTIATION: false interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy ZOOKEEPER DISCOVERY TOPIC BUCKET This configuration collects Metrics splitting topic collection between 3 different instances: integrations: - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '1/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '2/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka - name: nri-kafka env: METRICS: true CLUSTER_NAME: testcluster1 KAFKA_VERSION: \"1.0.0\" AUTODISCOVER_STRATEGY: zookeeper ZOOKEEPER_HOSTS: '[{\"host\": \"localhost\", \"port\": 2181}]' ZOOKEEPER_AUTH_SECRET: \"username:password\" ZOOKEEPER_PATH: \"/kafka-root\" DEFAULT_JMX_USER: username DEFAULT_JMX_PASSWORD: password TOPIC_MODE: regex TOPIC_REGEX: 'topic\\d+' TOPIC_BUCKET: '3/3' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy JAVA CONSUMER & PRODUCER This gives an example for collecting JMX metrics from Java consumers and producers: integrations: - name: nri-kafka env: METRICS: \"true\" CLUSTER_NAME: \"testcluster3\" PRODUCERS: '[{\"name\": \"myProducer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' CONSUMERS: '[{\"name\": \"myConsumer\", \"host\": \"localhost\", \"port\": 24, \"username\": \"me\", \"password\": \"secret\"}]' DEFAULT_JMX_HOST: \"localhost\" DEFAULT_JMX_PORT: \"9999\" interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy CONSUMER OFFSET This configuration collects consumer offset Metrics and Inventory for the cluster: integrations: - name: nri-kafka env: CONSUMER_OFFSET: true CLUSTER_NAME: testcluster3 AUTODISCOVER_STRATEGY: bootstrap BOOTSTRAP_BROKER_HOST: localhost BOOTSTRAP_BROKER_KAFKA_PORT: 9092 BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # A regex pattern that matches the consumer groups to collect metrics from CONSUMER_GROUP_REGEX: '.*' interval: 15s labels: env: production role: kafka inventory_source: config/kafka Copy Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. topic.diskSize In disk Topic size. Only present if COLLECT_TOPIC_SIZE is enabled. topic.offset Topic offset. Only present if COLLECT_TOPIC_OFFSET is enabled. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.63876,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration There are several ways to configure the integration, depending on how it was installed"
      },
      "id": "6174adee196a6791332f0a5f"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.40848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Redis monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " are also available in tarball format to allow <em>installation</em> outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/open-source-host-integrations-list/f5-open-source-integration": [
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Enable your F5 instance",
        "Tip",
        "Configure the integration",
        "F5 instance settings",
        "Labels/custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Environment variables replacement",
        "Metrics-only with partition filtering",
        "Multi-instance monitoring",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-10-24T21:57:59Z",
      "updated_at": "2021-09-14T18:21:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Auditor-level access user privileges and iControl REST API access permissions. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Enable your F5 instance Create a new F5 BIG-IP user and assign user permissions: Create a user account with, at minimum, Auditor-level access permissions. For instructions on how to do this, see the official F5 documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see the official F5 documentation on user role access descriptions. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes, see monitor services running on Kubernetes. If enabled via Amazon ECS, see monitor services running on ECS. If installed via on-host, edit the config in the integration's YAML config file, f5-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. The options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, and inventory_source, among others. For more on these common settings, see our list of configuration properties document. If you're still using our legacy configuration/definition files, see on-host integrations standard configuration format. Specific settings related to F5 are defined using the env section of the configuration file. These settings control the connection to your F5 instance, as well as other security settings and features. F5 instance settings The F5 integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where F5 is running. localhost M/I PORT Port on which F5 API is listening. 443 M/I USERNAME Username for accessing F5 API. N/A M/I PASSWORD Password for the given user. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. Only required if USE_SSL is true. N/A M/I CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M/I TIMEOUT Timeout for requests, in seconds. 30 M/I PARTITION_FILTER A JSON array of BIG-IP partitions to collect from. See this metrics-only with partition filtering example. [\"Common\"] M MAX_CONCURRENT_REQUESTS Maximum number of requests running concurrently. 10 M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false You can define these setting values in different ways, depending on your preference and need: Add the value directly in the config file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more on using passthrough or see the environment variables replacement example. Use secrets management to protect sensitive information, such as passwords, that would be exposed in plain text in the configuration file. For more information, read more about using secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics. Our default sample config file includes examples of labels. You can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-f5 env: HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production inventory_source: config/f5 Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-f5 env: METRICS: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production - name: nri-f5 env: INVENTORY: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: environment: production inventory_source: config/f5 Copy Environment variables replacement In this configuration, the environment variable F5_HOST populates the HOSTNAME setting of the integration: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Metrics-only with partition filtering This configuration only collects metrics and adds \"MyOtherPartition\" to the list of partitions to be sampled. By default, the integration only samples the \"Common\" partition: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password PARTITION_FILTER: '[\"Common\",\"MyOtherPartition\"]' interval: 15s labels: env: production role: load_balancer Copy Multi-instance monitoring This configuration monitors multiple F5 servers from the same integration. The first instance (HOSTNAME: 1st_f5_host) collects metrics and inventory, while the second instance (HOSTNAME: 2nd_f5_host) only collects metrics. integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer - name: nri-f5 env: INVENTORY: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: env: production role: load_balancer inventory_source: config/f5 - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 2nd_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 241.36324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>F5</em> monitoring <em>integration</em>",
        "sections": "<em>F5</em> monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": ". Inventory data The <em>F5</em> BIG-IP <em>integration</em> also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config&#x2F;<em>f5</em> <em>source</em>. For more about inventory data, see Understand <em>integration</em> data"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    },
    {
      "sections": [
        "On-host integrations metrics",
        "BETA FEATURE",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-10-25T01:03:03Z",
      "updated_at": "2021-09-14T20:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentSha",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.08449,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em> metrics",
        "sections": "On-host <em>integrations</em> metrics",
        "body": " transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes <em>F5</em> <em>f5</em>.node.availabilityState"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "67818451ffb7594e3c27526f4082bd1bc007bc51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-24T16:37:27Z",
      "updated_at": "2021-10-24T00:59:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.79178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> telemetry <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into <em>Open</em>Telemetry or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "6174afe1e7b9d2748213b3a6"
    }
  ],
  "/docs/infrastructure/host-integrations/open-source-host-integrations-list/memcached-open-source-integration": [
    {
      "sections": [
        "New Relic guided install overview",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes",
        "Troubleshoot common problems",
        "MySQL: Incorrect user permissions",
        "NGINX: No status URL"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "d2fb3ba0450a0810bb52db550fb261275c7d17f0",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-10-25T16:30:49Z",
      "updated_at": "2021-10-23T16:45:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU data center, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to what’s being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration Troubleshoot common problems As we identify areas where the guided install fails, we'll document them here and provide some troubleshooting guidance. MySQL: Incorrect user permissions To monitor MySQL health data, you need a valid username and password with specific permissions. These commands will create a user and grant the required permissions: Create a user newrelic@localhost with a specific password. sudo mysql -e \"CREATE USER 'newrelic'@'localhost' IDENTIFIED BY 'YOUR_SELECTED_PASSWORD';\" Copy Give replication privileges to newrelic@localhost with a maximum of 5 connections. sudo mysql -e \"GRANT REPLICATION CLIENT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Give select privileges to newrelic@localhost with a maximum of 5 connections. sudo mysql -e \"GRANT SELECT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Once done, your next guided install attempt should work. NGINX: No status URL To monitor your NGINX server, you'll need to configure a valid status URL. status_url: The URL set up to provide the metrics using the status module. If the default value of 127.0.0.1 is incorrect, substitute the address/FQDN/URL for your system. Example: status_url: http://127.0.0.1/status You can read more about the status_url in these NGINX docs: For NGINX Open Source: HTTP stub status module For NGINX Plus: HTTP status module and HTTP API module There are different ways to set status_url, depending on how NGINX was installed: If enabled via Kubernetes: See Monitor services running on Kubernetes. If enabled via Amazon ECS: See Monitor services running on ECS. If installed on-host: Edit the config in the integration's YAML config file, nginx-config.yml.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 706.33466,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "On-host <em>integration</em> (OHI) recipes",
        "body": " install -n hashicorp-consul-<em>open</em>-<em>source</em>-<em>integration</em> <em>Memcached</em> newrelic install -n <em>memcached</em>-<em>open</em>-<em>source</em>-<em>integration</em> Microsoft SQL Server (Windows only) newrelic install -n mssql-server-<em>integration</em>-installer MongoDB newrelic install -n mongodb-<em>open</em>-<em>source</em>-<em>integration</em> MySQL newrelic install -n mysql"
      },
      "id": "61743c2ce7b9d294a513c7c7"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "67818451ffb7594e3c27526f4082bd1bc007bc51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-24T16:37:27Z",
      "updated_at": "2021-10-24T00:59:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.79169,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> telemetry <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into <em>Open</em>Telemetry or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "6174afe1e7b9d2748213b3a6"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-10-24T21:53:53Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.65369,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Google Memorystore for <em>Memcached</em>",
        "sections": "Google Memorystore for <em>Memcached</em>",
        "tags": "<em>Integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a cloud <em>integration</em> for reporting your GCP Memcache data to our platform. Here we explain how to activate the <em>integration</em> and what data it collects. Activate the <em>integration</em> To enable the <em>integration</em> follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/infrastructure/host-integrations/troubleshooting/not-seeing-host-integration-data": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "23c49ad4f1b8758c05bca63c3ce4ca0b0201ae78",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-10-24T21:57:59Z",
      "updated_at": "2021-10-24T00:51:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.72467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6174ae22196a670e172f239a"
    },
    {
      "sections": [
        "Apache monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Enabling your Apache server",
        "Configure the integration",
        "Important",
        "Apache Instance Settings",
        "Labels/Custom attributes",
        "Example configurations",
        "BASIC CONFIGURATION",
        "HTTP BASIC AUTHENTICATION",
        "METRICS ONLY WITH SELF-SIGNED CERTIFICATE",
        "METRICS ONLY WITH ALTERNATIVE CERTIFICATE",
        "ENVIRONMENT VARIABLES REPLACEMENT",
        "MULTI-INSTANCE MONITORING",
        "Find and use data",
        "Metric data",
        "Inventory data",
        "System metadata",
        "Troubleshooting",
        "Problem accessing HTTPS endpoint for Apache",
        "Check the source code"
      ],
      "title": "Apache monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cae1fcc5a402bf71ae7d304b00420a9aa9b1152d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/apache-monitoring-integration/",
      "published_at": "2021-10-24T21:58:30Z",
      "updated_at": "2021-10-24T00:52:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Apache integration sends performance metrics and inventory data from your Apache web server to the New Relic platform. You can view pre-built dashboards of your Apache metric data, create alert policies, and create your own custom queries and charts. The integration works by gathering data from Apache's status module, so that module must be enabled and configured for your Apache instance (more details in Requirements). Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Apache versions 2.2 or 2.4. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Apache status module enabled and configured for Apache instance. Apache status module endpoint (default server-status) available from the host containing the Apache integration. If Apache is not running on Kubernetes or Amazon ECS, you must have the infrastructure agent installed on a Linux OS host that's running Apache. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Apache web server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Apache integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your Apache web server. Install and activate To install the Apache integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-apache. Change directory to the integration's folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp apache-config.yml.sample apache-config.yml Copy Edit the apache-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your Apache server To capture data from the Apache integration, you must first enable and configure the status module: Ensure the Apache status module is enabled and configured for Apache instance. Ensure the Apache status module endpoint (default server-status) is available from the host containing the Apache integration. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, apache-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, inventory_source. To read all about these common settings, refer to our Configuration Format document. Important If you are still using our legacy configuration/definition files, please refer to this document for help. Specific settings related to Apache are defined using the env section of the configuration file. These settings control the connection to your Apache instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Apache Instance Settings The Apache integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to STATUS_URL The URL set up to provide the metrics using the status module. http://127.0.0.1/server-status?auto M/I CA_BUNDLE_FILE Alternative Certificate Authority bundle file. N/A M CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M VALIDATE_CERTS Set to false if the status URL is HTTPS with a self-signed certificate. true M REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here or see the example below. Using secrets management. Use this to protect sensitive information, such as passwords that would be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics on. Our default sample config file includes examples of labels; however, as they are not mandatory, you can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations BASIC CONFIGURATION This is the very basic configuration to collect metrics and inventory from your localhost: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy HTTP BASIC AUTHENTICATION This configuration collects metrics and inventory from your localhost protected with basic authentication. Replace the username and password on the STATUS_URL with your credentials: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy METRICS ONLY WITH SELF-SIGNED CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added VALIDATE_CERTS: false to prevent validation of the server's SSL certificate when using a self-signed one: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy METRICS ONLY WITH ALTERNATIVE CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added CA_BUNDLE_FILE pointing to an alternative certificate file: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto CA_BUNDLE_FILE='/etc/ssl/certs/custom-ca.crt' REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy ENVIRONMENT VARIABLES REPLACEMENT In this configuration we are using the environment variable APACHE_STATUS to populate the STATUS_URL setting of the integration: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: {{APACHE_STATUS}} REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy MULTI-INSTANCE MONITORING In this configuration we are monitoring multiple Apache servers from the same integration. For the first instance (STATUS_URL: https://1st_apache_host/server-status?auto) we are collecting metrics and inventory while for the second instance (STATUS_URL: https://2nd_apache_host/server-status?auto) we will only collect metrics. integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://2nd_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy Find and use data Data from this service is reported to an integration dashboard. Apache data is attached to the ApacheSample event type. You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Apache integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as net. or server.. Name Description net.bytesPerSecond Rate of the number of bytes served, in bytes per second. net.requestsPerSecond Rate of the number of client requests, in requests per second. server.busyWorkers Current number of busy workers. server.idleWorkers Current number of idle workers. server.scoreboard.closingWorkers Current number of workers closing TCP connection after serving the response. server.scoreboard.dnsLookupWorkers Current number of workers performing a DNS lookup. server.scoreboard.finishingWorkers Current number of workers gracefully finishing. server.scoreboard.idleCleanupWorkers Current number of idle workers ready for cleanup. server.scoreboard.keepAliveWorkers Current number of workers maintaining a keep-alive connection. server.scoreboard.loggingWorkers Current number of workers that are logging. server.scoreboard.readingWorkers Current number of workers reading requests (headers or body). server.scoreboard.startingWorkers Current number of workers that are starting up. server.scoreboard.totalWorkers Total number of workers available. Workers that are not needed to process requests may not be started. server.scoreboard.writingWorkers Current number of workers that are writing. Inventory data Inventory data captures the version numbers from running Apache and from all loaded Apache modules, and adds those version numbers under the config/apache namespace. For more about inventory data, see Understand data. System metadata Besides the standard attributes collected by the infrastructure agent, the integration collects inventory data associated with the ApacheSample event type: Name Description software.version The version of the Apache server. Example: Apache/2.4.7 (Ubuntu). Troubleshooting Problem accessing HTTPS endpoint for Apache If you are having issues accessing the HTTPS endpoint for Apache, here are two possible solutions: Although you cannot ignore the SSL certification, you can set the config file parameters ca_bundle_file and ca_bundle_dir to point to an unsigned certificate in the Apache config file. Example: instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto ca_bundle_file: /etc/newrelic-infra/integrations.d/ssl/b2b.ca-bundle Copy An example using ca_bundle_dir: ca_bundle_dir: /etc/newrelic-infra/integrations.d/ssl Copy Alternatively, you can use HTTP instead of HTTPS. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.68549,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Apache monitoring <em>integration</em>",
        "sections": "Apache monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the apache-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best"
      },
      "id": "6174ae5a64441f5baf5fc976"
    },
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.62003,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    }
  ],
  "/docs/infrastructure/host-integrations/troubleshooting/pass-infrastructure-agent-parameters-host-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "23c49ad4f1b8758c05bca63c3ce4ca0b0201ae78",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-10-24T21:57:59Z",
      "updated_at": "2021-10-24T00:51:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.72455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6174ae22196a670e172f239a"
    },
    {
      "sections": [
        "Apache monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Enabling your Apache server",
        "Configure the integration",
        "Important",
        "Apache Instance Settings",
        "Labels/Custom attributes",
        "Example configurations",
        "BASIC CONFIGURATION",
        "HTTP BASIC AUTHENTICATION",
        "METRICS ONLY WITH SELF-SIGNED CERTIFICATE",
        "METRICS ONLY WITH ALTERNATIVE CERTIFICATE",
        "ENVIRONMENT VARIABLES REPLACEMENT",
        "MULTI-INSTANCE MONITORING",
        "Find and use data",
        "Metric data",
        "Inventory data",
        "System metadata",
        "Troubleshooting",
        "Problem accessing HTTPS endpoint for Apache",
        "Check the source code"
      ],
      "title": "Apache monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cae1fcc5a402bf71ae7d304b00420a9aa9b1152d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/apache-monitoring-integration/",
      "published_at": "2021-10-24T21:58:30Z",
      "updated_at": "2021-10-24T00:52:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Apache integration sends performance metrics and inventory data from your Apache web server to the New Relic platform. You can view pre-built dashboards of your Apache metric data, create alert policies, and create your own custom queries and charts. The integration works by gathering data from Apache's status module, so that module must be enabled and configured for your Apache instance (more details in Requirements). Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Apache versions 2.2 or 2.4. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Apache status module enabled and configured for Apache instance. Apache status module endpoint (default server-status) available from the host containing the Apache integration. If Apache is not running on Kubernetes or Amazon ECS, you must have the infrastructure agent installed on a Linux OS host that's running Apache. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Apache web server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Apache integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your Apache web server. Install and activate To install the Apache integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-apache. Change directory to the integration's folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp apache-config.yml.sample apache-config.yml Copy Edit the apache-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your Apache server To capture data from the Apache integration, you must first enable and configure the status module: Ensure the Apache status module is enabled and configured for Apache instance. Ensure the Apache status module endpoint (default server-status) is available from the host containing the Apache integration. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, apache-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, inventory_source. To read all about these common settings, refer to our Configuration Format document. Important If you are still using our legacy configuration/definition files, please refer to this document for help. Specific settings related to Apache are defined using the env section of the configuration file. These settings control the connection to your Apache instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Apache Instance Settings The Apache integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to STATUS_URL The URL set up to provide the metrics using the status module. http://127.0.0.1/server-status?auto M/I CA_BUNDLE_FILE Alternative Certificate Authority bundle file. N/A M CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M VALIDATE_CERTS Set to false if the status URL is HTTPS with a self-signed certificate. true M REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here or see the example below. Using secrets management. Use this to protect sensitive information, such as passwords that would be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics on. Our default sample config file includes examples of labels; however, as they are not mandatory, you can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations BASIC CONFIGURATION This is the very basic configuration to collect metrics and inventory from your localhost: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy HTTP BASIC AUTHENTICATION This configuration collects metrics and inventory from your localhost protected with basic authentication. Replace the username and password on the STATUS_URL with your credentials: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy METRICS ONLY WITH SELF-SIGNED CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added VALIDATE_CERTS: false to prevent validation of the server's SSL certificate when using a self-signed one: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy METRICS ONLY WITH ALTERNATIVE CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added CA_BUNDLE_FILE pointing to an alternative certificate file: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto CA_BUNDLE_FILE='/etc/ssl/certs/custom-ca.crt' REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy ENVIRONMENT VARIABLES REPLACEMENT In this configuration we are using the environment variable APACHE_STATUS to populate the STATUS_URL setting of the integration: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: {{APACHE_STATUS}} REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy MULTI-INSTANCE MONITORING In this configuration we are monitoring multiple Apache servers from the same integration. For the first instance (STATUS_URL: https://1st_apache_host/server-status?auto) we are collecting metrics and inventory while for the second instance (STATUS_URL: https://2nd_apache_host/server-status?auto) we will only collect metrics. integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://2nd_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy Find and use data Data from this service is reported to an integration dashboard. Apache data is attached to the ApacheSample event type. You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Apache integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as net. or server.. Name Description net.bytesPerSecond Rate of the number of bytes served, in bytes per second. net.requestsPerSecond Rate of the number of client requests, in requests per second. server.busyWorkers Current number of busy workers. server.idleWorkers Current number of idle workers. server.scoreboard.closingWorkers Current number of workers closing TCP connection after serving the response. server.scoreboard.dnsLookupWorkers Current number of workers performing a DNS lookup. server.scoreboard.finishingWorkers Current number of workers gracefully finishing. server.scoreboard.idleCleanupWorkers Current number of idle workers ready for cleanup. server.scoreboard.keepAliveWorkers Current number of workers maintaining a keep-alive connection. server.scoreboard.loggingWorkers Current number of workers that are logging. server.scoreboard.readingWorkers Current number of workers reading requests (headers or body). server.scoreboard.startingWorkers Current number of workers that are starting up. server.scoreboard.totalWorkers Total number of workers available. Workers that are not needed to process requests may not be started. server.scoreboard.writingWorkers Current number of workers that are writing. Inventory data Inventory data captures the version numbers from running Apache and from all loaded Apache modules, and adds those version numbers under the config/apache namespace. For more about inventory data, see Understand data. System metadata Besides the standard attributes collected by the infrastructure agent, the integration collects inventory data associated with the ApacheSample event type: Name Description software.version The version of the Apache server. Example: Apache/2.4.7 (Ubuntu). Troubleshooting Problem accessing HTTPS endpoint for Apache If you are having issues accessing the HTTPS endpoint for Apache, here are two possible solutions: Although you cannot ignore the SSL certification, you can set the config file parameters ca_bundle_file and ca_bundle_dir to point to an unsigned certificate in the Apache config file. Example: instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto ca_bundle_file: /etc/newrelic-infra/integrations.d/ssl/b2b.ca-bundle Copy An example using ca_bundle_dir: ca_bundle_dir: /etc/newrelic-infra/integrations.d/ssl Copy Alternatively, you can use HTTP instead of HTTPS. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.68538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Apache monitoring <em>integration</em>",
        "sections": "Apache monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the apache-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best"
      },
      "id": "6174ae5a64441f5baf5fc976"
    },
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.61992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    }
  ],
  "/docs/infrastructure/host-integrations/troubleshooting/run-integrations-manually": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "23c49ad4f1b8758c05bca63c3ce4ca0b0201ae78",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-10-24T21:57:59Z",
      "updated_at": "2021-10-24T00:51:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.72455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6174ae22196a670e172f239a"
    },
    {
      "sections": [
        "Apache monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Enabling your Apache server",
        "Configure the integration",
        "Important",
        "Apache Instance Settings",
        "Labels/Custom attributes",
        "Example configurations",
        "BASIC CONFIGURATION",
        "HTTP BASIC AUTHENTICATION",
        "METRICS ONLY WITH SELF-SIGNED CERTIFICATE",
        "METRICS ONLY WITH ALTERNATIVE CERTIFICATE",
        "ENVIRONMENT VARIABLES REPLACEMENT",
        "MULTI-INSTANCE MONITORING",
        "Find and use data",
        "Metric data",
        "Inventory data",
        "System metadata",
        "Troubleshooting",
        "Problem accessing HTTPS endpoint for Apache",
        "Check the source code"
      ],
      "title": "Apache monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cae1fcc5a402bf71ae7d304b00420a9aa9b1152d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/apache-monitoring-integration/",
      "published_at": "2021-10-24T21:58:30Z",
      "updated_at": "2021-10-24T00:52:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Apache integration sends performance metrics and inventory data from your Apache web server to the New Relic platform. You can view pre-built dashboards of your Apache metric data, create alert policies, and create your own custom queries and charts. The integration works by gathering data from Apache's status module, so that module must be enabled and configured for your Apache instance (more details in Requirements). Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Apache versions 2.2 or 2.4. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Apache status module enabled and configured for Apache instance. Apache status module endpoint (default server-status) available from the host containing the Apache integration. If Apache is not running on Kubernetes or Amazon ECS, you must have the infrastructure agent installed on a Linux OS host that's running Apache. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Apache web server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Apache integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your Apache web server. Install and activate To install the Apache integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-apache. Change directory to the integration's folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp apache-config.yml.sample apache-config.yml Copy Edit the apache-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your Apache server To capture data from the Apache integration, you must first enable and configure the status module: Ensure the Apache status module is enabled and configured for Apache instance. Ensure the Apache status module endpoint (default server-status) is available from the host containing the Apache integration. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, apache-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, inventory_source. To read all about these common settings, refer to our Configuration Format document. Important If you are still using our legacy configuration/definition files, please refer to this document for help. Specific settings related to Apache are defined using the env section of the configuration file. These settings control the connection to your Apache instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Apache Instance Settings The Apache integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to STATUS_URL The URL set up to provide the metrics using the status module. http://127.0.0.1/server-status?auto M/I CA_BUNDLE_FILE Alternative Certificate Authority bundle file. N/A M CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M VALIDATE_CERTS Set to false if the status URL is HTTPS with a self-signed certificate. true M REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here or see the example below. Using secrets management. Use this to protect sensitive information, such as passwords that would be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics on. Our default sample config file includes examples of labels; however, as they are not mandatory, you can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations BASIC CONFIGURATION This is the very basic configuration to collect metrics and inventory from your localhost: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy HTTP BASIC AUTHENTICATION This configuration collects metrics and inventory from your localhost protected with basic authentication. Replace the username and password on the STATUS_URL with your credentials: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: http://username:password@127.0.0.1/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache Copy METRICS ONLY WITH SELF-SIGNED CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added VALIDATE_CERTS: false to prevent validation of the server's SSL certificate when using a self-signed one: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy METRICS ONLY WITH ALTERNATIVE CERTIFICATE In this configuration we only have one integration block with METRICS: true to collect only metrics and added CA_BUNDLE_FILE pointing to an alternative certificate file: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://my_apache_host/server-status?auto CA_BUNDLE_FILE='/etc/ssl/certs/custom-ca.crt' REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy ENVIRONMENT VARIABLES REPLACEMENT In this configuration we are using the environment variable APACHE_STATUS to populate the STATUS_URL setting of the integration: integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: {{APACHE_STATUS}} REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy MULTI-INSTANCE MONITORING In this configuration we are monitoring multiple Apache servers from the same integration. For the first instance (STATUS_URL: https://1st_apache_host/server-status?auto) we are collecting metrics and inventory while for the second instance (STATUS_URL: https://2nd_apache_host/server-status?auto) we will only collect metrics. integrations: - name: nri-apache env: METRICS: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer - name: nri-apache env: INVENTORY: \"true\" STATUS_URL: https://1st_apache_host/server-status?auto REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/apache - name: nri-apache env: METRICS: \"true\" STATUS_URL: http://2nd_apache_host/server-status?auto REMOTE_MONITORING: true interval: 15s labels: env: production role: load_balancer Copy Find and use data Data from this service is reported to an integration dashboard. Apache data is attached to the ApacheSample event type. You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Apache integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as net. or server.. Name Description net.bytesPerSecond Rate of the number of bytes served, in bytes per second. net.requestsPerSecond Rate of the number of client requests, in requests per second. server.busyWorkers Current number of busy workers. server.idleWorkers Current number of idle workers. server.scoreboard.closingWorkers Current number of workers closing TCP connection after serving the response. server.scoreboard.dnsLookupWorkers Current number of workers performing a DNS lookup. server.scoreboard.finishingWorkers Current number of workers gracefully finishing. server.scoreboard.idleCleanupWorkers Current number of idle workers ready for cleanup. server.scoreboard.keepAliveWorkers Current number of workers maintaining a keep-alive connection. server.scoreboard.loggingWorkers Current number of workers that are logging. server.scoreboard.readingWorkers Current number of workers reading requests (headers or body). server.scoreboard.startingWorkers Current number of workers that are starting up. server.scoreboard.totalWorkers Total number of workers available. Workers that are not needed to process requests may not be started. server.scoreboard.writingWorkers Current number of workers that are writing. Inventory data Inventory data captures the version numbers from running Apache and from all loaded Apache modules, and adds those version numbers under the config/apache namespace. For more about inventory data, see Understand data. System metadata Besides the standard attributes collected by the infrastructure agent, the integration collects inventory data associated with the ApacheSample event type: Name Description software.version The version of the Apache server. Example: Apache/2.4.7 (Ubuntu). Troubleshooting Problem accessing HTTPS endpoint for Apache If you are having issues accessing the HTTPS endpoint for Apache, here are two possible solutions: Although you cannot ignore the SSL certification, you can set the config file parameters ca_bundle_file and ca_bundle_dir to point to an unsigned certificate in the Apache config file. Example: instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto ca_bundle_file: /etc/newrelic-infra/integrations.d/ssl/b2b.ca-bundle Copy An example using ca_bundle_dir: ca_bundle_dir: /etc/newrelic-infra/integrations.d/ssl Copy Alternatively, you can use HTTP instead of HTTPS. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.68538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Apache monitoring <em>integration</em>",
        "sections": "Apache monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the apache-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best"
      },
      "id": "6174ae5a64441f5baf5fc976"
    },
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.61992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "PowerDNS monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the <em>integrations</em> folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    }
  ],
  "/docs/infrastructure/host-integrations/understand-use-data/host-integration-data-collection-reporting": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.90709,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " powerdns_url: http:&#x2F;&#x2F;localhost:8082&#x2F;api&#x2F;v1&#x2F; Copy Find and <em>use</em> <em>data</em> For more on how to find and <em>use</em> your <em>data</em>, see <em>Understand</em> integration <em>data</em>. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this <em>data</em>"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4436,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest <em>data</em>. To <em>use</em> these APIs, you&#x27;ll need a license key. The integration adheres to the Metric API requirements and <em>data</em> limits. The default rate limit is 100,000 <em>data</em> points"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4435,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-<em>host</em>: edit the config in the integration&#x27;s YAML configuration file redis-config.yml. <em>Use</em> the YAML configuration to place required login credentials and configure how your <em>data</em> is collected"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/host-integrations/understand-use-data/remote-monitoring-host-integrations": [
    {
      "sections": [
        "PowerDNS monitoring integration",
        "BETA FEATURE",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "PowerDNS instance settings",
        "Example configurations",
        "Basic configuration",
        "Find and use data",
        "Metric data",
        "Check the source code"
      ],
      "title": "PowerDNS monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e84cc75e844cd8780c859ff6ae4730a8c561b29d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/powerdns-monitoring-integration/",
      "published_at": "2021-10-24T22:02:11Z",
      "updated_at": "2021-10-24T02:18:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in open beta and only applies to the versions starting on 0.0.5 released in October 2021. Our PowerDNS integration collects and sends dimensional metrics from PowerDNS. You can view this metric data in pre-built dashboards, create alert policies, and create custom queries and charts. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with PowerDNS Recursor and Authoritative. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent, minimum version 1.19.2. Linux distribution compatible with the infrastructure agent. The integration obtains data by scrapping the PowerDNS API through a Prometheus exporter. To enable the API, the webserver and the HTTP API need to be enabled. Add these lines to the pdns.conf: api=yes api-key=changeme Copy And restart, the following examples should start working: curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1/servers/localhost | jq . curl -v -H 'X-API-Key: changeme' http://127.0.0.1:8081/api/v1 Copy For more information, Enabling Webserver and Api Install and activate To install the PowerDNS integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-powerdns. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp powerdns-config.yml.sample powerdns-config.yml Copy Edit the powerdns-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration To configure the integration, edit the config in the integration's YAML configuration file powerdns-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. Since this integration is based on a prometheus exporter, settings applicable to other integrations, like interval, timeout or inventory_source are not supported. PowerDNS instance settings The PowerDNS integration collects dimensional Metrics: Setting Description Default powerdns_url API URL of the powerdns service N/A exporter_port Port to expose scrape endpoint on, If this is not provided a random port will be used to launch the exporter random-port api_key API key used to connect to the PowerDNS server N/A Example configurations Basic configuration This is the basic configuration used to collect metrics from an authoritative and a recursor instance: integrations: - name: nri-powerdns config: api_key: authoritative-secret exporter_port: 9121 powerdns_url: http://localhost:8081/api/v1/ - name: nri-powerdns config: api_key: recursor-secret exporter_port: 9122 powerdns_url: http://localhost:8082/api/v1/ Copy Find and use data For more on how to find and use your data, see Understand integration data. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this data for troubleshooting purposes, or to create custom charts and dashboards. Metric data The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_AUTHORITATIVE: Name Description Dimensions powerdns_authoritative_uptime (count) Uptime in seconds of the daemon. type powerdns_authoritative_packet_cache_size (gauge) Number of entries in the packet cache. powerdns_authoritative_recursive_queries_total (count) Total number of recursive queries by status. status powerdns_authoritative_remote_queries (count) Remote server IP addresses. remote powerdns_authoritative_security_status (gauge) PDNS Server Security status based on security-status.secpoll.powerdns.com. powerdns_authoritative_exceptions_total (count) Total number of exceptions by error. error powerdns_authoritative_latency_average_seconds (gauge) Average number of microseconds a packet spends within PowerDNS. powerdns_authoritative_dnsupdate_queries_total (count) Total number of DNS update queries by status. status powerdns_authoritative_qsize (gauge) Number of packets waiting for database attention. powerdns_authoritative_response_rcodes (count) Distribution of rcodes. rcode powerdns_authoritative_signature_cache_size (gauge) Number of entries in the signature cache. powerdns_authoritative_queries_unauth (count) Queries for domains that we are not authoritative for. record powerdns_authoritative_answers_bytes_total (count) Total number of answer bytes sent over by protocol. proto powerdns_authoritative_queries (count) UDP Queries Received. record powerdns_authoritative_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_authoritative_deferred_cache_actions (count) Deferred cache actions because of maintenance by type. type powerdns_authoritative_up (gauge) The last scrape of PowerDNS was successful. powerdns_authoritative_query_cache_lookup (count) Query cache lookups by result. result powerdns_authoritative_key_cache_size (gauge) Number of entries in the key cache. powerdns_authoritative_answers_total (count) Total number of answers by protocol. proto powerdns_authoritative_packet_cache_lookup (count) Packet cache lookups by result. result powerdns_authoritative_metadata_cache_size (gauge) Number of entries in the metadata cache. powerdns_authoritative_queries_total (count) Total number of queries by protocol. proto powerdns_authoritative_cpu_utilisation (count) Number of CPU milliseconds spent in user, and kernel space. type powerdns_authoritative_dnssec (count) DNSSEC counters. type powerdns_authoritative_response_sizes (count) Size distribution of responses. size powerdns_authoritative_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_authoritative_remote_queries_unauth (count) Remote hosts querying domains for which we are not authoritative. remote The following dimensional metrics are captured scraping the exporter and linked to the entity POWERDNS_RECURSOR: Name Description Dimensions powerdns_recursor_incoming_queries_total (count) Total number of incoming queries by network. net powerdns_recursor_outgoing_queries_total (count) Total number of outgoing queries by network. net powerdns_recursor_cache_size (gauge) Number of entries in the cache. powerdns_recursor_cache_lookups_total (count) Total number of cache lookups by result. result powerdns_recursor_exporter_json_parse_failures (count) Number of errors while parsing PowerDNS JSON stats. powerdns_recursor_answers_rcodes_total (count) Total number of answers by response code. rcode powerdns_recursor_exporter_total_scrapes (count) Current total PowerDNS scrapes. powerdns_recursor_concurrent_queries (gauge) Number of concurrent queries. powerdns_recursor_answers_rtime_total (count) Total number of answers grouped by response time slots. timeslot powerdns_recursor_latency_average_seconds (gauge) Exponential moving average of question-to-answer latency. powerdns_recursor_exceptions_total (count) Total number of exceptions by error. error powerdns_recursor_response_time_seconds_sum (count) Histogram of PowerDNS recursor response times in seconds. (sum metric) powerdns_recursor_response_time_seconds_bucket (count) Histogram of PowerDNS recursor response times in seconds. (bucket metric) le powerdns_recursor_up (gauge) The last scrape of PowerDNS was successful. Check the source code This integration is open source software. This means you can browse its source code and send improvements, or create your own fork and build it. Moreover this integration leverages an opensource exporter created by the community.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.90709,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "PowerDNS monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " powerdns_url: http:&#x2F;&#x2F;localhost:8082&#x2F;api&#x2F;v1&#x2F; Copy Find and <em>use</em> <em>data</em> For more on how to find and <em>use</em> your <em>data</em>, see <em>Understand</em> integration <em>data</em>. Metrics are attached to the Metric sample and event types of the entities POWERDNS_AUTHORITATIVE and POWERDNS_RECURSOR. You can query this <em>data</em>"
      },
      "id": "6174c27628ccbc1575c6b8dd"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-10-24T21:10:05Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4436,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest <em>data</em>. To <em>use</em> these APIs, you&#x27;ll need a license key. The integration adheres to the Metric API requirements and <em>data</em> limits. The default rate limit is 100,000 <em>data</em> points"
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4435,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-<em>host</em>: edit the config in the integration&#x27;s YAML configuration file redis-config.yml. <em>Use</em> the YAML configuration to place required login credentials and configure how your <em>data</em> is collected"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/index": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Linux",
        "Quick start",
        "Tip",
        "Step-by-step instructions",
        "Debian",
        "Ubuntu",
        "Amazon Linux, CentOS, RHEL",
        "SLES",
        "Important",
        "Amazon Linux",
        "CentOS / RHEL & Oracle Linux",
        "Root (default)",
        "Privileged user",
        "Unprivileged user",
        "Install using configuration management tools",
        "Install for Docker containers on instrumented hosts",
        "Install using Azure extensions",
        "Install using tarball files",
        "Update the agent",
        "What's next?"
      ],
      "title": "Install the infrastructure monitoring agent for Linux",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "060512e99d6143e7a7e8e6d16ba96cdcc7534e57",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux/",
      "published_at": "2021-10-24T20:52:41Z",
      "updated_at": "2021-10-24T20:52:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring agent for Linux can be installed using several package managers. You can use our launcher, or follow the instructions in this document to complete a basic installation. No matter which installation option you choose, make sure you've created a free New Relic account (No credit card required). Quick start The quickest way to get started with our infrastructure monitoring agent is through our guided install. Tip Try our guided install for yourself. (If you're hosted in the EU, use our EU guided install.) Step-by-step instructions If guided install doesn't work, you can install the agent manually. Before installing infrastructure, be sure to: Review the requirements. Have a valid New Relic license key. To install infrastructure in Linux, follow these instructions: Create the configuration file and add your license key: echo \"license_key: YOUR_LICENSE_KEY\" | sudo tee -a /etc/newrelic-infra.yml Copy Determine the distribution version number: Debian cat /etc/os-release Copy Ubuntu cat /etc/lsb-release Copy Amazon Linux, CentOS, RHEL cat /etc/os-release Copy SLES cat /etc/os-release | grep VERSION_ID Copy Enable New Relic's GPG key. Debian curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Ubuntu curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Amazon Linux, CentOS, RHEL yum automatically installs the GPG key using the value in gpgkey. SLES curl https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg -s | sudo gpg --import Copy Important There's a known issue with the zypper package manager where GPG keys may not be validated as expected. If you get errors such as Signature verification failed, see New Relic's Explorers Hub for more information. Add the infrastructure monitoring agent repository: Debian Debian 8 (\"Jessie\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt jessie main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 9 (\"Stretch\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt stretch main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 10 (\"Buster\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt buster main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 11 (\"Bullseye\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bullseye main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu Ubuntu 16 (\"Xenial\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 18 (\"Bionic\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bionic main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20 (\"Focal\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt focal main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20.10 (\"Groovy\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt groovy main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 21.04 (\"Hirsute Hippo\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt hirsute main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Amazon Linux Amazon Linux 2 (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64/newrelic-infra.repo Copy Amazon Linux 2 (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/aarch64/newrelic-infra.repo Copy CentOS / RHEL & Oracle Linux CentOS, RHEL, Oracle Linux 7.x (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo Copy CentOS RHEL, Oracle Linux 7.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/aarch64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/x86_64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux 8.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/aarch64/newrelic-infra.repo Copy SLES SLES 11.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/11.4/x86_64/newrelic-infra.repo Copy SLES 12.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/x86_64/newrelic-infra.repo Copy SLES 12.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/aarch64/newrelic-infra.repo Copy SLES 12.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/x86_64/newrelic-infra.repo Copy SLES 12.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/aarch64/newrelic-infra.repo Copy SLES 12.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/x86_64/newrelic-infra.repo Copy SLES 12.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/aarch64/newrelic-infra.repo Copy SLES 12.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/x86_64/newrelic-infra.repo Copy SLES 12.4 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/aarch64/newrelic-infra.repo Copy SLES 12.5 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/x86_64/newrelic-infra.repo Copy SLES 12.5 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/aarch64/newrelic-infra.repo Copy SLES 15.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/x86_64/newrelic-infra.repo Copy SLES 15.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/aarch64/newrelic-infra.repo Copy SLES 15.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/x86_64/newrelic-infra.repo Copy SLES 15.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/aarch64/newrelic-infra.repo Copy SLES 15.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/x86_64/newrelic-infra.repo Copy SLES 15.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/aarch64/newrelic-infra.repo Copy Refresh the repositories: Debian sudo apt-get update Copy Ubuntu sudo apt-get update Copy Amazon Linux, CentOS, RHEL sudo yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' Copy SLES sudo zypper -n ref -r newrelic-infra Copy Install the newrelic-infra package in root (default), privileged user, or unprivileged user mode. For more information on each running mode, see Linux agent running modes. Root (default) Debian and Ubuntu: sudo apt-get install newrelic-infra -y Copy Amazon Linux, CentOS, RHEL: sudo yum install newrelic-infra -y Copy SLES: sudo zypper -n install newrelic-infra Copy Privileged user Install the libcap library and set the NRIA_MODE environment variable to PRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"PRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"PRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"PRIVILEGED\" zypper install newrelic-infra Copy Unprivileged user Install the libcap library and set the NRIA_MODE environment variable to UNPRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"UNPRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"UNPRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"UNPRIVILEGED\" zypper install newrelic-infra Copy Once the infrastructure monitoring agent is installed or updated, you can start, stop, or check the agent status. Important As of version 1.4.0, the infrastructure monitoring agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. Install using configuration management tools To install the infrastructure monitoring agent with a configuration management tool, see the documentation for: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Install for Docker containers on instrumented hosts See Docker instrumentation for infrastructure monitoring. Install using Azure extensions See Azure extensions for infrastructure monitoring. Install using tarball files For custom setup scenarios, you can install the infrastructure monitoring agent using our tarball files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment, or to install the infrastructure monitoring agent on distributions that lack the newrelic-infra package in their repositories. Important Installing the agent using tarball files is officially supported only for the AWS Graviton 2 processor. Update the agent Follow standard procedures to update the infrastructure monitoring agent. If you are using sudo to install or update the agent, use the -E argument to allow bypassing the environment variables, or specify the NRIA_MODE environment variable just after sudo. export NRIA_MODE=\"SET_MODE_HERE\" Copy OR sudo -E YOUR_PACKAGE_MANAGER_UPDATE_COMMAND Copy What's next? Generate some traffic and wait a few minutes, then view your hosts in the New Relic One UI. If necessary, follow our troubleshooting procedures if no data appears. Important The hostname for your server cannot be localhost. Data will not be reported for servers with that name. Make sure the host name uses a unique name. The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 58.909134,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>infrastructure</em> monitoring agent for Linux",
        "sections": "Install the <em>infrastructure</em> monitoring agent for Linux",
        "tags": "<em>Infrastructure</em>",
        "body": "Our <em>infrastructure</em> monitoring agent for Linux can be installed using several package managers. You can use our launcher, or follow the instructions in this document to complete a basic installation. No matter which installation option you choose, make sure you&#x27;ve created a free New Relic account"
      },
      "id": "6043edce64441f5335378f15"
    },
    {
      "sections": [
        "Infrastructure Events page: Live feed of config changes",
        "Event types",
        "Events page features",
        "Chart data attributes"
      ],
      "title": "Infrastructure Events page: Live feed of config changes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "e4a87670c8671072ae7cc6531721f46edc7f925d",
      "image": "https://docs.newrelic.com/static/75373d03d819516d3cbe23f1ea65957b/c1b63/infra-events-ui.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-events-page-live-feed-every-config-change/",
      "published_at": "2021-10-24T16:08:00Z",
      "updated_at": "2021-10-18T19:01:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Events page is a live feed of important system and host activity, including inventory change events, configuration changes, and log analytics events. The event feed helps you understand correlations between these events and system performance. Search and filter through your events to decrease the mean time to detect and repair infrastructure issues. You can access the Events page by going to one.newrelic.com > Infrastructure > Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When a violation is opened or closed, New Relic generates an event indicating the host and associated alert condition. Agent connection When an infrastructure agent connects to New Relic, our platform generates an Agent connected event. If New Relic doesn't receive data from an agent for three minutes, the platform generates an Agent disconnected event. Inventory changes These events are generated when inventory data is added, removed, or modified. Select the source icon to to understand which category corresponds to the altered inventory path. For additional details, select an inventory event to see a side-by-side comparison of the old and new state. Inventory events can include: Kernel (includes modules and configuration): Can be added, modified, or deleted. Metadata (includes various additional information about hosts): Can be added, modified, or deleted. Packages: Can be installed, removed, or modified. Services: Can be started, stopped, or restarted. Sessions (includes users): Can be connected or disconnected. Events page features To view the live event feed: Go to one.newrelic.com > Infrastructure > Events. The Events page includes a heatmap, which provides a snapshot of the events occurring within the selected time range. one.newrelic.com > Infrastructure > Events: Use the Events to view important, real-time activity in your infrastructure. With the Events page, you can easily search through your event log to quickly find vulnerable packages. If you want to... Do this... Focus on specific events Use the Search events field to look for specific events, config changes or agent installations. To focus on a specific set of events, select or change the filter set. Search within a particular time range Enter a time range to the right of the search bar to investigate events within a specific time range. For example, if you encountered a CPU spike around 11am on the previous day, search Yesterday at 11 am to investigate the possible cause. Compare events with host load, memory, CPU, and more View the events feed on the Hosts page. To compare infrastructure events and performance for a specific time, select a range via the time picker or drag and select a range on a chart. View events specifically related to agents, config, metadata, services, or sessions Group or sort events by selecting the filter icon be the search bar. Drill down into additional details Select an event to view additional details, such as attributes and values. To drill down further, select View in Inventory to see additional details in the Inventory page. View host's alert threshold violation Select the host's Critical icon or Warning icon. Chart data attributes For a technical explanation of the attributes used to populate the Events page, see InfrastructureEvent attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 55.738243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "sections": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "tags": "<em>Infrastructure</em>",
        "body": " the mean time to detect and repair <em>infrastructure</em> issues. You can access the Events page by going to one.newrelic.com &gt; <em>Infrastructure</em> &gt; Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When"
      },
      "id": "6043fa6c28ccbc13742c60a5"
    },
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2021-10-24T22:09:02Z",
      "updated_at": "2021-10-19T04:55:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). Built-in log forwarding is not yet available. macOS (Beta): 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, and 2019, and their service packs. Windows 10 and their service packs. macOS macOS 10.14 (Mohave), 10.15 (Catalina), 11 (Big Sur). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 48.309437,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> agent",
        "sections": "Requirements for the <em>infrastructure</em> agent",
        "tags": "<em>Infrastructure</em>",
        "body": "Before installing our <em>infrastructure</em> agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The <em>infrastructure</em> agent supports these processor architectures"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    }
  ],
  "/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition": [
    {
      "sections": [
        "Alerts for infrastructure: Add, edit, or view host alert information",
        "Create alert conditions for infrastructure",
        "Important",
        "Other infrastructure alert condition methods",
        "Use the Alerts UI",
        "Use the Infrastructure UI",
        "Use infrastructure settings for integrations",
        "Tip",
        "View host alert events",
        "Update or delete host alert information",
        "Use New Relic Alerts to monitor your entire infrastructure",
        "Add a description",
        "Add or edit a runbook URL",
        "Violation time limit for violations",
        "Alert conditions that generate too-long NRQL queries"
      ],
      "title": "Alerts for infrastructure: Add, edit, or view host alert information",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "00207a1020aa29ea6d5d5bbb8e806a50a5966f80",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerts-add-edit-or-view-host-alert-information/",
      "published_at": "2021-10-24T17:38:23Z",
      "updated_at": "2021-08-02T12:47:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring, you can create alert conditions directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic Alerts. Instead, you can immediately select your filter set and tailor the alert condition directly from the chart you are viewing. This helps you proactively manage and monitor the alerting system for your environment. Any alert violations will be created per entity within the filter set. Create alert conditions for infrastructure Alert conditions apply to alert policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to use other types of notification channels, create a new policy from within the Alerts UI. Important The Infrastructure REST API has a limit of 3,700 alert conditions, including both active and disabled conditions. The API, whether used directly or via the UI, will reject all requests to add any additional alert conditions beyond the 3,700 alert condition limit. To add an infrastructure alert condition to an alerts policy: Go to one.newrelic.com > Infrastructure, then select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Type a meaningful condition name. Select the Alert type, or refer to the examples to decide which type to select. Create individual filters, or copy all the filters from a filter set to identify the hosts that you want the alert condition to use. Important For more information about the rules behind filters, see Filter set logic. Define the Critical (required) and Warning (optional, if available) thresholds for triggering the alert notification. Optional: To create the condition criteria proactively but not receive alert notifications at this time, turn off the Enabled checkbox option. Select an existing policy for the new condition. OR Select the option to create a new policy and identify the email for alert notifications. Optional: Add a runbook url. Optional: Set Violation time limit for violations (this defaults to 24 hours). Select Create. Important If New Relic hasn't received a cloud integration service's attribute in the past 60 minutes, we refer to this as a \"silent attribute,\" and it won't be available to use as an alert condition in the UI. In this situation, you can use the API to create alert conditions for silent attributes. Other infrastructure alert condition methods You can also use these other methods to create an infrastructure alert condition: Use the Alerts UI Go to one.newrelic.com > Alerts & AI > Alerts > Alert policies > New alert policy > Create new condition, then select Infrastructure as the product. Use the Infrastructure UI Go to one.newrelic.com > Infrastructure. Select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Use infrastructure settings for integrations Tip Use this method to create an alert condition for infrastructure integrations. Go to one.newrelic.com > Infrastructure > Settings > Alerts, and then click Create alert condition. Name and describe the alert condition. Click the Integrations alert type, and then select the integration data source you'd like to use. Use the Filter entities dropdown to limit your condition to specific attributes. Use the Define thresholds dropdowns to define your condition's thresholds, and then click Create. The configuration settings are optional. You can always update them later. View host alert events Anyone included in the policy's notification channels receive alert notifications directly. In addition, anyone with permissions for your New Relic account can view Infrastructure alert incidents and individual violations through the user interface. Go to one.newrelic.com > Infrastructure > Events. To change the hosts or time frame, use the search window, Filter set, or Time functions. From the Events list, select the alert violation. To view detailed information in Alerts about the selected violation, select the link. Update or delete host alert information To edit, disable (or re-enable), or delete host alert information: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Use the search window or Select all checkbox to locate one or more alert conditions. Select any of the available functions to edit, disable, enable, or delete the selected conditions. Use New Relic Alerts to monitor your entire infrastructure New Relic Alerts provides a single, coordinated alerting tool across all of your New Relic products. This allows you to manage alert policies and conditions that focus on the metrics for entities that you care about the most, such as Docker containers, JVMs, and more. Alert features Features in Infrastructure Alert conditions Create: Use the Infrastructure UI. View, change, disable (or re-enable), or delete: Use the Infrastructure Settings > Alerts UI. Information on alerts View summary information about events: Use the Infrastructure Events UI. View detailed information about alert incidents or individual violations: Use the Alerts UI or the notification channel integrated with the associated policy. Alert policies View, add, change, disable, or delete: For policies with a variety of notification channels: Use the Alerts UI. For policies only needing email notifications: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Create a new policy, and add one or more email addresses as needed. Add host conditions to an existing policy: Use the Infrastructure UI. Notification channels To view, add, change, or delete available notification options: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Search for the condition or policy name. From the list of conditions, select the policy link to view notification channel information in the Alerts UI. Add a description The use of the Description field is available for these alert condition types: NRQL conditions: add a description using the NerdGraph API. Infrastructure conditions: add a description using the UI or the REST API. The text you place in an alert condition's Description field is passed downstream to associated violations and notifications. A description can be used for several purposes, including: Capturing the reason for the alert condition. Defining the signal being monitored. Defining next steps. Add metadata to downstream systems. You can use template substitution to insert values from the attributes in the associated violation event. The template format is {{attributeName}}. For the attributes you can use when creating a description, see Violation event attributes. One available attribute is the special {{tag.*}} attribute. This attribute prefix is used to access any of the tag values that are included with the target signal, or any of the entity tags that are associated with the target signal. If there are entity tags associated with your violation, then they can be accessed using the entity tag name. An example of this would be {{tag.aws.awsRegion}}. When entity tags are available to use, you see them included with the violation, and displayed when you view the violations in an incident. This field has a maximum character size of 4,000. Add or edit a runbook URL The alert condition creation process includes an option for setting a URL for runbook instructions. This lets you link to information or standard procedures for handling a violation. Before adding or updating the link, make sure you use a valid URL. To add, update, or delete an alert condition's runbook URL: Select an alert condition, and make changes to the Runbook URL link. Save the condition. In order to be saved, the URL must be a valid URL. Violation time limit for violations The violation time limit allows you to define a time period after which violations will be force-closed. By default, violation time limit is 24 hours. To add or update an alert condition's violation time limit: Select an alert condition, and make changes to the violation time limit. Save the condition. Alert conditions that generate too-long NRQL queries Alert conditions created for infrastructure rely on behind-the-scenes NRQL queries, and NRQL queries have a 4096-character limit. This means that if your condition generates a very complex NRQL query that filters on many elements (for example, including many hosts or many tags), it will exceed this limit and display an error message saying that the condition failed. To solve this problem, reduce the number of elements you are using in your alert condition. For example: Problem Solution Hosts If you entered a large number of hosts that caused the condition to fail, reduce the number of hosts. Use substrings to target hosts. For example, instead of targeting prod-host-01, prod-host-02, and prod-host-03, just target all hosts with prod-host-0 in the name. Entities Edit your alert condition to target specific attributes that apply to the entities you're trying to target. Create custom attributes for the entities you want to target, and use those attributes in your alert condition. For more information, see Best practices for filtering in infrastructure alerts in New Relic's Explorers Hub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.10236,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> for <em>infrastructure</em>: Add, edit, or view host <em>alert</em> information",
        "sections": "Create <em>alert</em> <em>conditions</em> for <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "With New Relic&#x27;s <em>infrastructure</em> monitoring, you can create <em>alert</em> <em>conditions</em> directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic <em>Alerts</em>"
      },
      "id": "6043fa3428ccbc401d2c60b9"
    },
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2021-10-25T00:47:23Z",
      "updated_at": "2021-07-27T14:15:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. The no_trigger_on field is optional. When set to [\"shutdown\"] this enables the Don't trigger alerts for hosts that perform a clean shutdown infrastructure condition option. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12, \"no_trigger_on\": [\"shutdown\"] } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. For new conditions, if a value is not provided, the following default values are used: All conditions: 24 hours When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.90297,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    },
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2021-10-24T20:57:09Z",
      "updated_at": "2021-07-27T13:58:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process is running too many instances on one host This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.90076,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process is running too many instances"
      },
      "id": "603eb49128ccbca939eba74a"
    }
  ],
  "/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/verify-your-alerts-after-activating-remote-monitoring": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/drop-data-using-nerdgraph/",
      "sections": [
        "Drop data using NerdGraph",
        "Tip",
        "Requirements",
        "Create drop data rule",
        "Caution",
        "NRQL restrictions",
        "Example drop rules",
        "Drop two event types",
        "Drop events meeting certain criteria",
        "Drop sensitive attributes while maintaining the rest of the data",
        "Verify your drop rule works",
        "View rules",
        "Delete drop rules",
        "Audit drop rule history",
        "Cautions when dropping data",
        "Learn more"
      ],
      "published_at": "2021-10-24T21:24:28Z",
      "title": "Drop data using NerdGraph",
      "updated_at": "2021-10-23T17:31:36Z",
      "type": "docs",
      "external_id": "4a400ac08bcc55060e9d037de79c012af6b618e9",
      "document_type": "page",
      "popularity": 1,
      "body": "You might find that you ingest data you don't need or want. You can drop some types of data, which enables you: To filter out unimportant low-value data To filter out potentially sensitive data If you choose to drop data, only new data will be affected. Existing data cannot be edited or deleted. Tip If you are sending data to New Relic using Prometheus remote write, you can also drop data by changing the remote_write section of your YAML config file. For more information, see Drop data using Prometheus remote write. Requirements You must have a user role with permissions for dropping data. Currently the following data types can be targeted for data dropping: APM-reported events Browser-reported events Mobile-reported events Synthetics-reported events Custom events (like those generated by the APM agent APIs or the Event API) Log data Trace spans Default infrastructure monitoring events and infrastructure integrations events, with this exception: Downsampled SystemSample, ProcessSample, NetworkSample and StorageSample with time windows longer than 59 minutes can't be dropped. This data doesn't count towards your data ingest. Dimensional metrics, with these caveats: Billing impacts: for New Relic One pricing, dropped data is not billable. For original pricing, dropped data is billable. For metrics generated by the events-to-metrics service: drop rules won't work but these metrics can be stopped or attributes pruned by disabling or re-configuring the events-to-metric rule. Support for additional types are planned for the future. Create drop data rule Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, please review caution information below. To drop data, create a NerdGraph-format drop rule that includes: A NRQL string that specifies what data types to drop An action type specifying how to apply the NRQL string You can form and make the call in the NerdGraph explorer. There are two ways to drop data: Drop entire data types or a data subset (with optional filter). This uses the DROP_DATA action type and uses NRQL of the form: SELECT * FROM DATA_TYPE_1, DATA_TYPE_2 (WHERE OPTIONAL_FILTER) Copy For this type of drop rule, you cannot use anything other than * in the SELECT clause. Drop attributes from data types (with optional filter). This uses the DROP_ATTRIBUTES action type and uses NRQL of the form: SELECT dropAttr1, dropAttr2 FROM DATA_TYPE (WHERE OPTIONAL_FILTER) Copy For this type of drop rule, you must pass in a non-empty list of raw attributes names. NRQL restrictions Not all NRQL clauses make sense for generating drop rules. You can provide a WHERE clause to select data with specific attributes. Other features such as TIMESERIES, COMPARE WITH, FACET, and other clauses cannot be used. The two action types have these restrictions: DROP_DATA can use only SELECT *. DROP_ATTRIBUTES requires use of SELECT with \"raw\" attributes (attributes with no aggregator function applied). This also means you cannot use SELECT *. Additionally, there are some attributes that are integral to their data type and cannot be dropped (such as timestamp on event data). If you include them, registration will fail. Example drop rules Here are some example drop rules: Drop two event types Let's say you notice you have some event types being sent to New Relic that are not important to you. Also, stopping the source from sending those event types quickly is unrealistic, requiring changes to agents and/or API instrumentation. Using a drop rule is an easier way to accomplish the same goal. Here is an example NerdGraph call that drops two event types: Event1 and Event2. mutation { nrqlDropRulesCreate(accountId: YOUR_ACCOUNT_ID, rules: [ { action: DROP_DATA nrql: \"SELECT * FROM Event1, Event2\" description: \"Drops all data for Event1 and Event2.\" } ]) { successes { id } failures { submitted { nrql } error { reason description } } } } Copy Drop events meeting certain criteria Let’s say you have a high volume custom event type that arrives from multiple sources. If you don't find all of that data important, you can use a drop rule. Here is an example of a drop rule that filters out events based on specific criteria. mutation { nrqlDropRulesCreate(accountId: YOUR_ACCOUNT_ID, rules: [ { action: DROP_DATA nrql: \"SELECT * FROM MyCustomEvent WHERE appName='LoadGeneratingApp' AND environment='development'\" description: \"Drops all data for MyCustomEvent that comes from the LoadGeneratingApp in the dev environment, because there is too much and we don’t look at it.\" } ]) { successes { id } failures { submitted { nrql } error { reason description } } } } Copy Drop sensitive attributes while maintaining the rest of the data Let's say you noticed an event has attributes that contain Personally Identifiable Information (PII). You are working to update your services to stop sending the data, but until then, you need to cease storing further PII in New Relic. Although you could drop all of the data as it comes in the door with a DROP_DATA rule, the rest of the data still provides value. Therefore, you can register a drop rule to remove only the offending PII from your data: mutation { nrqlDropRulesCreate(accountId: YOUR_ACCOUNT_ID, rules: [ { action: DROP_ATTRIBUTES nrql: \"SELECT userEmail, userName FROM MyCustomEvent description: \"Removes the user name and email fields from MyCustomEvent\" } ]) { successes { id } failures { submitted { nrql } error { reason description } } } } Copy Verify your drop rule works After you create a drop rule, verify that it is working as expected. The rule should take effect quickly after a successful registration, so try running a TIMESERIES version of the query you registered to see that the data drops off. Drop rule type NRQL DROP_DATA Drop rule NRQL: SELECT * FROM MyEvent WHERE foo = bar Copy Validation NRQL: SELECT count(*) FROM MyEvent WHERE foo = bar TIMESERIES Copy This should drop to 0. To verify that it did not affect any thing else, invert the WHERE clause. DROP_ATTRIBUTES Drop rule NRQL: SELECT dropAttr1, dropAttr2 FROM MyEvent WHERE foo = bar Copy Validation NRQL: SELECT count(dropAttr1), count(dropAttr2) FROM MyEvent WHERE foo = bar TIMESERIES Copy Both lines should drop to 0. To verify that it did not affect events that contained these attributes and still should, invert the WHERE clause. View rules Here is an example NerdGraph call that returns the drop rules set on an account: { actor { account(id: YOUR_ACCOUNT_ID) { nrqlDropRules { list { rules { id nrql accountId action createdBy createdAt description } error { reason description } } } } } } Copy Delete drop rules Here is an example NerdGraph call deleting two specific drop rules: mutation { nrqlDropRulesDelete(accountId: YOUR_ACCOUNT_ID, ruleIds: [\"48\", \"98\"]) { successes { id nrql accountId action description } failures { error { reason description } submitted { ruleId accountId } } } } Copy Audit drop rule history To see who created and deleted drop rules, query your account audit logs. The list endpoint also includes the user ID of the person who created the rule. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic does not review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Rules you create, including all information in those rules, can be viewed and edited by any user with the relevant role-based access control permissions. Only new data will be dropped. Existing data cannot be edited or deleted. Learn more Recommendations for learning more: NerdGraph basics and terminology NRQL basics Browse the Explorers Hub for community discussions about NRQL drop rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.79391,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Verify</em> <em>your</em> drop rule works",
        "body": "&quot; } ]) { successes { id } failures { submitted { nrql } error { reason description } } } } Copy <em>Verify</em> <em>your</em> drop rule works <em>After</em> you create a drop rule, <em>verify</em> that it is working as expected. The rule should take effect quickly <em>after</em> a successful registration, so try running a TIMESERIES version"
      },
      "id": "617446f828ccbcdc29c6ae13"
    },
    {
      "sections": [
        "Troubleshoot Metric API with NRIntegrationError events",
        "Problem",
        "Solution",
        "View error details",
        "Match errors to ingested payloads",
        "Programmatically retrieve NrIntegrationError events",
        "Tip"
      ],
      "title": "Troubleshoot Metric API with NRIntegrationError events",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "50ad21a895fc1f2644bdbfbadf85ecfd298b08d6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/troubleshoot-nrintegrationerror-events/",
      "published_at": "2021-10-24T20:16:09Z",
      "updated_at": "2021-10-23T17:26:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You sent metric data points to the Metric API, and are not seeing what you expect when querying the data. Use the following checklist to determine the root cause: Make sure you are querying the data correctly. Check the HTTP status codes returned by the API. Issues like authorization failures can be diagnosed with HTTP status codes. If you are sending data from a Prometheus server via New Relic's remote_write endpoint, check your Prometheus server logs for errors or non-2xx HTTP responses from the New Relic endpoint. Query your account for NrIntegrationError events. New Relic's ingestion endpoints are asynchronous, meaning the endpoint verifies the payload after it returns the HTTP response. If any issues occur while verifying your payload, then an NrIntegrationError event will be created in your account. New Relic also uses NrIntegrationError events to notify customers when various rate limits have been reached. Solution View error details For an introduction to using the NrIntegrationError event, see NrIntegrationError. Here's an example NRQL for examining issues with Metric API ingest: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' facet category, message limit 100 since 24 hours ago Copy The category indicates the type of error and the message provides more detailed information about the error. If the category is rateLimit, then you should also examine the rateLimitType field for more information on the type of rate limiting. Category rateLimitType Description and solution BadRequest (not set) There is an issue with the JSON payload. These include JSON syntax errors, attribute names, or values that are too long. Check the message field to determine the exact issue. Then review the JSON payload, and update it to ensure it meets the proper semantic guidelines. RateLimit DatapointsPerMinute You are sending too many datapoints per minute. If you get this error, you can either send data less frequently, or request changes to your metric rate limits by contacting your New Relic account representative, or visiting our Support portal. RateLimit UniqueTimeseriesPerDay You have an attribute with a high number of unique values, like containerId or URI. To resolve this error, review any attributes that may be causing the issue and remove them. If desired, you can use a data dropping rule to remove attributes at ingest time. RateLimit UniquePrometheusTimeseries You have Prometheus servers reporting too many unique timeseries via New Relic's remote_write endpoint. Reduce the number of unique timeseries reported by modifying your Prometheus server configuration to reduce the number of targets being scraped, or by using relabel rules in the remote_write section of your server configuration to drop timeseries or highly unique labels. RateLimit RequestsPerMinute Too many requests per minute are being sent. To resolve this, put more datapoints in each request, and send them less frequently. RateLimit ErrorGroupsPerDay You have exceeded your daily error group limit. Incoming error groups will be dropped for the remainder of the day and will continue as normal after UTC midnight. To resolve this, reduce the amount of unique error messages collected by New Relic. Match errors to ingested payloads When an NrIntegrationError event is created as a result of a syntax issue with the HTTP request payload, then the event contains the attributes apiKeyPrefix and requestId. The apiKeyPrefix matches the first 6 characters of the API key used to send the data. The requestId matches the requestId sent in the HTTP response. To view these fields, run this NRQL query: SELECT message, apiKeyPrefix, requestId FROM NrIntegrationError limit 100 Copy To verify a specific requestId, run this NRQL query: SELECT * FROM NrIntegrationError where requestId ='REQUEST_ID' Copy Programmatically retrieve NrIntegrationError events To programmatically retrieve these errors: Ensure you have an Insights query API key (go to insights.newrelic.com > Manage data > API keys). Create an HTTP request as shown below: Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. curl -H \"Accept: application/json\" -H \"X-Query-Key:YOUR_API_KEY_HERE\" \"https://insights-api.newrelic.com/v1/accounts/YOUR_ACCOUNT_HERE/query?nrql=SELECT%20*%20FROM%20NrIntegrationError%20where%20newRelicFeature='Metrics'\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.7388,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " failures can be diagnosed with HTTP status codes. If you are sending data from a Prometheus server via New Relic&#x27;s <em>remote</em>_write endpoint, check <em>your</em> Prometheus server logs for errors or non-2xx HTTP responses from the New Relic endpoint. Query <em>your</em> account for NrIntegrationError events. New Relic"
      },
      "id": "610f2900196a678a5d38ad82"
    },
    {
      "sections": [
        "Redis monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Connect with Unix socket",
        "Connect with TCP",
        "Windows",
        "Create a New Relic user for your Redis server",
        "Configure the integration",
        "Redis instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic authentication",
        "REDIS 6+ ACL authentication",
        "Metrics-only with TLS connection",
        "Metrics-only connecting over socket",
        "Using renamed commands",
        "Multi-instance monitoring with keys filtering",
        "Find and use data",
        "Metric data",
        "Redis sample metrics",
        "Keyspace metrics",
        "Inventory data",
        "Other system data",
        "Check the source code"
      ],
      "title": "Redis monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "65445300ce4ff2076d972626cf6f43fbfe709b2d",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/redis-monitoring-integration/",
      "published_at": "2021-10-24T22:03:00Z",
      "updated_at": "2021-10-24T00:56:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Redis integration reports critical performance data from your Redis server to New Relic products. You can view this metric data and inventory data in pre-built dashboards, create alert policies, and create custom queries and charts. You can also specify keys that are important to your application and get information about their length. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Redis versions 3.0 or higher. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If Redis is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running Redis. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. The integration obtains data by executing Redis commands: INFO command: Data from the INFO command populates metric data and some inventory data. CONFIG GET command: Most inventory data comes from this command. In managed Redis installations without permissions to execute the CONFIG command (for example, AWS ElastiCache), the execution of this command can be disabled with the config_inventory: false configuration option. Key length acquisition: Depending on the type of key, these commands are used: LLEN for list SCARD for set ZCOUNT for zset HLEN for hash type. For key length data collection, the agent uses pipelining to minimize the impact on your Redis performance. However, if you are collecting the length of many keys, your Redis performance may be affected. For this reason, the agent includes a default key limit (but this limit can be overwritten). Important If you edited the names of the Redis commands mentioned above, the integration will not be able to retrieve the Redis data. Quick start Instrument your Redis server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Redis integration, follow the instructions for your environment: Additional notes: Advanced: Integrations are also available in tarball format to allow installation outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-redis. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp redis-config.yml.sample redis-config.yml Copy Edit the redis-config.yml file based on your Redis server connection methods: Connect with Unix socket If you connect using Unix socket, specify the unix_socket_path in the configuration file. If there are more than one Redis instance using Unix sockets, make sure to set use_unix_socket to true in the configuration file. Be sure that the user executing the Redis integration has correct permissions for accessing that Unix socket. The permissions of the Unix socket are set in the Redis configuration (value of unixsocketperm). Connect with TCP If you connect via TCP, the config file is by default set to localhost and port 6379. You can change this by specifying hostname and/or the port argument. If you use this method, the unix_socket_path parameter cannot be set. If required, set other configuration file settings based on your Redis setup, as described in Configuration. Restart the infrastructure agent. Windows Download the nri-redis .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-redis/nri-redis-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-redis-amd64.msi Copy or double-click the file in Explorer. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy redis-win-config.yml.sample redis-win-config.yml Copy Edit the redis-win-config.yml file as described in the configuration settings. Restart the infrastructure agent. Create a New Relic user for your Redis server If you're using Redis 6+ and using access control lists (ACLs) to control access to your instance, you may want to create a newrelic user for the integration. Use this command to create and grant the required permissions to your user: ACL SETUSER newrelic on >'YOUR_SELECTED_PASSWORD' ~* +INFO +CONFIG|GET +SELECT +TYPE +LLEN +SCARD +ZCOUNT +HLEN Copy Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML configuration file redis-config.yml. Use the YAML configuration to place required login credentials and configure how your data is collected, depending on your setup and preference. The configuration file has common settings applicable to all integrations, like interval, timeout, inventory_source. To learn more about these common settings, see our YAML configuration format. Important If you're still using our legacy configuration/definition files, use our older, standard configuration foramt. Specific settings related to Redis are defined using the env section of the configuration file. These settings control the connection to your Redis instance as well as other security settings and features. Redis instance settings The Redis integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies To HOSTNAME Redis server hostname. localhost M/I PORT Port where Redis server is listening. 6379 M/I USERNAME Username to use when connecting to the Redis server. Use only with Redis 6+ if ACL is enabled. N/A M/I PASSWORD Password to use when connecting to the Redis server. Use only with Redis servers if REQUIREPASS or ACL is enabled. N/A M/I UNIX_SOCKET_PATH Path to Unix socket file on which Redis server is listening. Use this instead of Hostname/Port. N/A M/I USE_UNIX_SOCKET Set to true to uniquely identify the monitored entities when using Unix sockets. false M/I USE_TLS Use TLS when communicating with the Redis server. false M/I TLS_INSECURE_SKIP_VERIFY Disable server name verification when connecting over TLS. false M/I KEYS List of the keys for retrieving their lengths. N/A M KEYS_LIMIT Max number of keys to retrieve their lengths. 30 M CONFIG_INVENTORY Set to 'false' in environments where the Redis CONFIG command is not allowed (for example, AWS or ElastiCache). true I RENAMED_COMMANDS Map default Redis commands to their renamed form. N/A M/I REMOTE_MONITORING Enable multi-tenancy monitoring. true M/I METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Basic authentication This configuration connects to Redis using basic authentication. Replace my_password with your REQUIREPASS password: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy REDIS 6+ ACL authentication In Redis 6+, you can protect your instance with Access Control Lists (ACLs). For ACLs, replace the USERNAME and PASSWORD values with your credentials: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 USERNAME: my_user PASSWORD: my_password REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Metrics-only with TLS connection Use to connect to Redis with TLS. You can add TLS_INSECURE_SKIP_VERIFY : true to disable the server name verification: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 USE_TLS: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Metrics-only connecting over socket Use to connect to Redis using a socket file. This is an alternative to a TCP Hostname/Port connection: integrations: - name: nri-redis env: METRICS: true UNIX_SOCKET_PATH: /var/run/redis/redis.sock USE_UNIX_SOCKET: true REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Using renamed commands Use this to collect your metrics/inventory if you've used the rename-command to protect your instance: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: {\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\",\"LLEN\":\"c940fc2d15f59e41cb7be6c21\"} REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT: 6379 RENAMED_COMMANDS: '{\"CONFIG\":\"b840fc9f15f59e41cb7be6c52\"}' REMOTE_MONITORING: true interval: 60s labels: environment: production inventory_source: config/redis Copy Multi-instance monitoring with keys filtering Use this if you need to collect metrics from two different Redis servers using the the same integration. The first instance example is only collecting KEY_1 amd KEY_2 from Redis database 0. The second instance example shows how to collect from different databases with {\"DB\":\"KEYS\"}. integrations: - name: nri-redis env: METRICS: true HOSTNAME: redis_host_1 PORT: 6379 KEYS: '[\"KEY_1\",\"KEY_2\"]' REMOTE_MONITORING: true interval: 15s labels: environment: production - name: nri-redis env: METRICS: true HOSTNAME: redis_host_2 PORT: 6379 KEYS: '{\"0\":[\"KEY_1\",\"KEY_2\"],\"1\":[\"KEY_3\"]}' REMOTE_MONITORING: true interval: 15s labels: environment: production Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the RedisSample and RedisKeyspaceSample event types. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Redis integration collects the following metric data attributes: Redis sample metrics These attributes are attached to the RedisSample event type: Name Description cluster.connectedSlaves Number of connected slaves. db.aofLastRewriteTimeInMilliseconds Duration of the last AOF rewrite operation in milliseconds. db.aofLastBgrewriteStatus Boolean representing status of the last AOF background rewrite operation. db.aofLastWriteStatus Boolean representing status of the last AOF write operation. db.evictedKeysPerSecond Number of evicted keys due to maxmemory limit per second. db.expiredKeysPerSecond Number of key expiration events per second. db.keyspaceHitsPerSecond Number of successful lookups of keys in the main dictionary per second. db.keyspaceMissesPerSecond Number of failed lookup of keys in the main dictionary per second. db.latestForkUsecMilliseconds Duration of the latest fork operation in milliseconds. db.rdbBgsaveInProgress Boolean. A flag indicating a RDB save is ongoing. db.rdbChangesSinceLastSave Number of changes since the last dump. db.rdbLastBgsaveStatus Boolean representing the status of the last RDB save operation. db.rdbLastBgsaveTimeMilliseconds Duration of the last RDB save operation in milliseconds. db.rdbLastSaveTime Epoch-based timestamp of last successful RDB save in seconds. db.syncFull Count of the number times slaves have fully synchronized with this master. db.syncPartialErr Count of the number of times partial syncs have failed to complete. db.syncPartialOk Count of the number of times partial syncs have completed. net.blockedClients Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH). net.clientBiggestInputBufBytes The biggest input buffer among current client connections. net.clientLongestOutputList The longest output list among current client connections. net.commandsProcessedPerSecond Number of commands processed by the server per second. net.connectedClients Number of client connections (excluding connections from slaves). net.connectionsReceivedPerSecond Number of connections accepted by the server per second. net.inputBytesPerSecond Total number of bytes input per second. net.outputBytesPerSecond Total number of bytes output per second. net.pubsubChannels Global number of pub/sub channels with client subscriptions. net.pubsubPatterns Global number of pub/sub pattern with client subscriptions. net.rejectedConnectionsPerSecond Number of connections per second rejected because of maxclients limit. software.uptimeMilliseconds Number of milliseconds since Redis server start. system.memFragmentationRatio Ratio between used_memory_rss and used_memory. system.totalSystemMemoryBytes The amount of memory in bytes available in the instance where Redis is running. system.usedCpuSysMilliseconds System CPU consumed by the Redis server in milliseconds. system.usedCpuSysChildrenMilliseconds System CPU consumed by the background processes in milliseconds. system.usedCpuUserMilliseconds User CPU consumed by the Redis server in milliseconds. system.usedCpuUserChildrenMilliseconds User CPU consumed by the background processes in milliseconds. system.usedMemoryBytes The total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc). system.usedMemoryLuaBytes Number of bytes used by the Lua engine. system.usedMemoryPeakBytes The peak memory consumed by Redis in bytes. system.usedMemoryRssBytes Number of bytes that Redis allocated as seen by the operating system (also known as resident set size). This is the number reported by tools such as top(1) and ps(1). Keyspace metrics The Redis integration collects the following keyspace metadata and metrics. These attributes are attached to the RedisKeyspaceSample event type: Name Description db.avgTtl The average time to live (TTL) in milliseconds of keys that have an expiration set in the database being reported on. db.keys Number of keys in the database being reported on. db.keyspace Redis database index, which is the integer number (usually a number between 0 and 15). Format: db followed by the database index. For example: db0, db1, db2, etc. db.expires Number of keys with an expiration in the database being reported on. Inventory data Inventory data includes everything reported by the Redis CONFIG GET command, with the exception of requirepass, which stores the password to the Redis server. For more on inventory data, see Understand inventory data. Other system data The Redis integration collects these additional attributes about your Redis service: Name Description software.version The version of the Redis server. Example: 3.2.3. cluster.role Either master or slave, depending on the role of the Redis node being monitored. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.66183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Redis <em>monitoring</em> integration",
        "sections": "Redis <em>monitoring</em> integration",
        "body": " This is the basic configuration used to collect metrics and inventory from <em>your</em> localhost: integrations: - name: nri-redis env: METRICS: true HOSTNAME: localhost PORT: 6379 <em>REMOTE_MONITORING</em>: true interval: 15s labels: environment: production - name: nri-redis env: INVENTORY: true HOSTNAME: localhost PORT"
      },
      "id": "6174af2128ccbc21b3c6b259"
    }
  ],
  "/docs/infrastructure/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard": [
    {
      "sections": [
        "Configure polling frequency and data collection for cloud integrations",
        "Overview of settings",
        "Caution",
        "Change polling frequency",
        "Specify data to be fetched",
        "Data collection",
        "Filters",
        "Potential impact on alerts and charts"
      ],
      "title": "Configure polling frequency and data collection for cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "b900b7545f9032201c212449be114e10176bf789",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations/",
      "published_at": "2021-10-24T22:07:31Z",
      "updated_at": "2021-07-27T15:37:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our cloud integrations get data from cloud provider APIs. In New Relic, you can change some of the data collection-related settings for your cloud integrations. Read on to see what changes you can make and the reasons for making them. Overview of settings New Relic cloud integrations get data from cloud providers' APIs. Data is generally collected from monitoring APIs such as AWS CloudWatch, Azure Monitor, and GCP Stackdriver, and inventory metadata is collected from the specific services' APIs. You can use the account status dashboard to see how your cloud integrations are handling data from a cloud service provider. If you want to report more or less data from your cloud integrations, or if you need to control the use of the cloud providers' APIs to prevent reaching rate and throttling limits in your cloud account, you can change the configuration settings to modify the amount of data they report. The two main controls are: Change polling frequency Change what data is reported Examples of business reasons for wanting to change your polling frequency include: Billing: If you need to manage your AWS CloudWatch bill, you may want to decrease the polling frequency. Before you do this, make sure that any alert conditions set for your cloud integrations are not affected by this reduction. New services: If you are deploying a new service or configuration and you want to collect data more often, you may want to increase the polling frequency temporarily. Caution Changing the configuration settings for your integrations may impact alert conditions and chart trends. Change polling frequency The polling frequency configuration determines how often New Relic reports data from your cloud provider for each service. By default, the polling frequency is set to the maximum frequency that is available for each service. To change the polling frequency for a cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Use the dropdowns next to Data polling interval every to select how frequently you want New Relic to capture your cloud integration data. Specify data to be fetched You can specify which information you want captured for your cloud integration by enabling the collection of additional data and by applying multiple filters to each integration. To change this settings for your cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Under Data collections and filters, turn the toggles you want On. For filters, select or enter the values that you want included in your reported data. Data collection For some cloud integrations, an additional number of calls to the cloud provider APIs are needed in order to collect data. For example, to fetch tags for AWS Elastic Map Reduce clusters, an additional call to the service API is required. To better control the amount of API calls that are sent to your cloud account for these integrations, you can specify when you need these types of data to be collected. Different data collection toggles are available, depending on the integration. Toggle Description Collect tags Some integrations require additional API calls to the cloud provider to report tags. Tag collection is enabled by default. Switch this to Off if you don't want the integration to collect your cloud resource tags and thus reduce the volume of API calls. Collect extended inventory Some integrations can collect extended inventory metadata about your cloud resources by making additional API calls to the cloud provider. The metadata included within the extended inventory for each cloud integration is described in the integration documentation. Extended inventory collection is disabled by default. Switch this to On if you want to monitor extended inventory. This will increase the volume of API calls. Collect shards data Available for AWS Kinesis Streams integration. By default, we don't report shard metrics. Switch this to On if you want to monitor shard metrics in addition to data stream metrics. Collect Lambda@Edge data Available for AWS CloudFront integration. By default, we don't report Lambda@Edge data. Switch this to On if you're using Lambda@Edge in AWS CloudFront and want to get Lambda execution location metadata. Collect node data Available for AWS Elasticsearch integration. By default, we don't report Elasticsearch node metrics. Switch this to On if you want to monitor node metrics in addition to cluster metrics. Collect NAT Gateway data and Collect VPN data Available for AWS VPC integration. By default, we don't report NAT Gateway nor VPN metrics. Switch these to On if you want to monitor NAT Gateway and VPN metrics and inventory, in addition to other VPC related entities inventory. Collect IP addresses Available for AWS EC2 integration. By default, we collect EC2 instance metadata that includes public and private IP addresses, and network interface details. Switch this to Off if you don't want New Relic to store and display these IP data. Filters When a filter is On, you specify the data that you want to be collected; for example, if the Limit to AWS region is On, the regions that you select will be the ones that data will be collected for. There are different filters available, depending on the integration: Filter Description Region Select the regions that include the resources that you want to monitor. Queue prefixes Available for AWS SQS integration. Enter each name or prefix for the queues that you want to monitor. Filter values are case-sensitive. Load balancer prefixes Available for AWS ALB integration. Enter each name or prefix for the application load balancers that you want to monitor. Filter values are case-sensitive. Stage name prefixes Available for AWS API Gateway integration. Enter each name or prefix for the stages that you want to monitor. Filter values are case-sensitive. Tag key Enter one tag key that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag value filter. Tag value Enter one tag value that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag key. Resource group Select the resource groups that are associated with the resources that you want to monitor. Potential impact on alerts and charts If you change an integration's configuration, it can impact alert conditions and charts. Here are some things to consider: If you change this setting... It may have this impact... Any configuration setting When you change the configuration settings, the data that New Relic displays in infrastructure charts, on the inventory page, and in the events feed changes as well. Any filters When you create alert conditions after you set filters, make sure that your alerts are not triggered by resources that you filtered out. Filter for regions If you filter for specific regions, it may lower the amount of data reported to New Relic, which could trigger an alert. If you create an alert condition for a specific region and then filter that region out, the region would no longer report data and would never trigger the alert. Polling frequency When you create an alert, make sure that you define the threshold for a time period that is longer than the polling frequency. Tags and extended inventory If you turn on tags and/or extended inventory, New Relic makes more API calls to the cloud provider, which could increase your cloud provider API usage bill.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.23883,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "sections": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Our <em>cloud</em> <em>integrations</em> get data from <em>cloud</em> provider APIs. In New Relic, you can change some of the data collection-related settings for your <em>cloud</em> <em>integrations</em>. Read on to see what changes you can make and the reasons for making them. Overview of settings New Relic <em>cloud</em> <em>integrations</em> get data from"
      },
      "id": "603e8eef64441fcc7e4e8853"
    },
    {
      "sections": [
        "Metric data gaps with cloud integrations",
        "Problem",
        "Solution",
        "Amazon (AWS)",
        "Microsoft Azure",
        "Google Cloud Platform (GCP)",
        "Tip",
        "Cause"
      ],
      "title": "Metric data gaps with cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "ee3473d0cbc9059b6b36503949d08d1e76c7fc06",
      "image": "https://docs.newrelic.com/static/dfa79b9e3086b81f216d306ba0afe557/c1b63/screen-metric-gap.png",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations/",
      "published_at": "2021-10-24T22:07:30Z",
      "updated_at": "2021-03-29T21:18:28Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You've set up your AWS, Azure, or GCP integration and are monitoring your metrics. However, you notice gaps in your metric data charts. This screenshot shows a metric data chart with gaps. Solution Here’s a list of metrics which might show gaps in your metric data. If possible, avoid setting up alerts for these metrics because we know they can generate false positives. Amazon (AWS) Integration Provider Event Type Metric SNS SnsTopic QueueSample provider.subscriptionsConfirmed SnsTopic QueueSample provider.subscriptionsPending SnsTopic QueueSample provider.subscriptionsDeleted EFS EfsFileSystem BlockDeviceSample provider.lastKnownSizeInBytes ECS EcsCluster ComputeSample provider.registeredContainerInstancesCount EcsCluster ComputeSample provider.activeServicesCount EcsCluster ComputeSample provider.pendingTasksCount EcsCluster ComputeSample provider.runningTasksCount EcsService ComputeSample provider.pendingCount EcsService ComputeSample provider.runningCount EcsService ComputeSample provider.desiredCount DynamoDB DynamoDbTable DatastoreSample provider.itemCount DynamoDbTable DatastoreSample provider.tableSizeBytes AutoScaling AutoScalingInstance AutoScalingInstanceSample healthStatus Billing BillingBudget FinanceSample provider.actualAmount Billingbudget FinanceSample provider.forecastedAmount BillingBudget FinanceSample provider.limitAmount Microsoft Azure Integration Provider Event Type Metric SQL AzureSqlDatabase AzureSqlDatabaseSample databaseSizeCurrentBytes AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google Cloud Platform (GCP) Tip We're currently reviewing the GCP metrics that can cause data gaps. Tip This list isn't complete. We're currently reviewing the full list of metrics that can cause data gaps. Cause Some metrics aren’t present in the usual cloud provider APIs (CloudWatch, Stackdriver, Azure Monitor) and are fetched from the service APIs instead. Each cloud service provider has a unique service API that processes data and interacts with the service. For example, if a metric isn’t present in AWS CloudWatch, New Relic will fetch the metric from the AWS ECS service API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.073265,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "sections": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google <em>Cloud</em> Platform (GCP) Tip We&#x27;re currently reviewing the GCP metrics that can cause data gaps. Tip This list isn&#x27;t complete. We&#x27;re currently"
      },
      "id": "603e821e196a67a042a83df3"
    },
    {
      "sections": [
        "Get started with New Relic observability",
        "You’re in control because you understand your system",
        "All the answers in one place",
        "Start anywhere"
      ],
      "title": "Get started with New Relic observability",
      "type": "docs",
      "tags": [
        "Observe everything",
        "Get started"
      ],
      "external_id": "30f87d5f702f926efec49b59591679fa93627ad5",
      "image": "https://docs.newrelic.com/static/44970161aec793f3141cfcdc0fc96a57/c1b63/observability-2.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/get-started-full-stack-observability/",
      "published_at": "2021-10-25T15:06:57Z",
      "updated_at": "2021-10-23T16:46:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "True observability is the power of knowing what's happening across your digital system and why, at any time, whatever solution you’re using. It’s getting the whole picture of everything that enables your applications and devices to deliver value to your customers, from the container running a microservice in the cloud to a mobile website's shopping cart button. You’re in control because you understand your system New Relic helps you cut through the layers of complexity surrounding your systems by bringing together and connecting data from any instrumented source and environment, without having to jump between tools. You can interrogate your data for patterns, discover them using our data platform, or get proactive results from our machine learning tools. New Relic provides answers to essential questions in one place. All the answers in one place As a full user you get access to our entire set of observability tools. All our tools are interconnected and accessible in New Relic One. All the data you bring to New Relic through agents and integrations are metrics, events, logs, and traces that feed our platform's analytics and monitoring capabilities. New Relic links your data in a meaningful way so that you can explore it, build dashboards, and set up alerts. Our more curated observability UI experiences allow to visualize, analyze, and troubleshoot your entire software stack in one unified platform. The New Relic Explorer consolidates all the entities in your system, and how they're connected, in a single place, so you can easily detect performance trends and issues. By automatically connecting infrastructure health with application performance and end-user behavior, you can cut through the noise to find the signal. Start anywhere Being fully-connected, New Relic allows you to start your observability journey from any element of your stack. For example, you can get to crucial infrastructure logs from traces of an application running on a problematic Kubernetes pod. Use the Explorer in New Relic One to access and observe the full stack of your software, see performance data and alerting status at a glance, and check relationships. We provide you with a simple, yet powerful visual tool to monitor all your entities, that is, anything we can identify that reports data. In the New Relic ecosystem, entities include basic components like applications, hosts, containers, or database services, but it can also refer to custom groupings of such elements. You can also create your own entities. The more entities you instrument, the more data you'll bring in. The more data you've brought to New Relic, the more you'll understand your metrics, events, logs, and traces. You want to instrument Start with Keep exploring Front-end applications Mobile applications User behavior and flows New Relic Explorer Browser monitoring Mobile monitoring Synthetic monitoring Single page monitoring Scripted browsers Containerized minions Workloads Backend applications Serverless applications New Relic Explorer Application monitoring Serverless monitoring Learning about Apdex Distributed tracing Logs in context APM data to infrastructure Workloads Infrastructure hosts and services (on-premise, cloud, orchestrated) Container environments and orchestration tools (Kubernetes, ECS, etc.) Infrastructure monitoring Infrastructure integrations Kubernetes integration Docker integration ECS integration Log forwarding APM data to infrastructure Custom integrations Kubernetes cluster explorer Infrastructure alerts Workloads",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 88.00514,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " applications New Relic Explorer Application monitoring Serverless monitoring Learning about Apdex Distributed tracing Logs in context APM data to <em>infrastructure</em> Workloads <em>Infrastructure</em> hosts and services (on-premise, <em>cloud</em>, orchestrated) Container environments and orchestration tools (Kubernetes"
      },
      "id": "61743c6764441f60375fd317"
    }
  ]
}