{
  "/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-missing-nodes": [
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "322c24d8737fb1e9625f35fe495bf22b23f046f6",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2021-09-20T19:43:49Z",
      "updated_at": "2021-09-20T19:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.11.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.27075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ", K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = &#x27;MY_CLUSTER_NAME&#x27; Copy Tip If you still can&#x27;t see Control Plane data, try the solution described in <em>Kubernetes</em> <em>integration</em> <em>troubleshooting</em>: Not seeing data."
      },
      "id": "603e7f98196a67beaea83dbf"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-26T22:11:31Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.24417,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-26T22:10:36Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.132744,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-not-seeing-data": [
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "322c24d8737fb1e9625f35fe495bf22b23f046f6",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2021-09-20T19:43:49Z",
      "updated_at": "2021-09-20T19:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.11.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.27075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ", K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = &#x27;MY_CLUSTER_NAME&#x27; Copy Tip If you still can&#x27;t see Control Plane data, try the solution described in <em>Kubernetes</em> <em>integration</em> <em>troubleshooting</em>: Not seeing data."
      },
      "id": "603e7f98196a67beaea83dbf"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-26T22:11:31Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.24417,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-26T22:10:36Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.132744,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/not-seeing-control-plane-data": [
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "322c24d8737fb1e9625f35fe495bf22b23f046f6",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2021-09-20T19:43:49Z",
      "updated_at": "2021-09-20T19:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.11.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.27095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ", K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = &#x27;MY_CLUSTER_NAME&#x27; Copy Tip If you still can&#x27;t see Control Plane data, try the solution described in <em>Kubernetes</em> <em>integration</em> <em>troubleshooting</em>: Not seeing data."
      },
      "id": "603e7f98196a67beaea83dbf"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-26T22:11:31Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.24414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-26T22:10:36Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.13272,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    }
  ],
  "/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data": [
    {
      "sections": [
        "Navigate the Kubernetes cluster explorer",
        "Meet the cluster explorer",
        "Cluster dashboard",
        "Cluster explorer node table",
        "Search and filter your cluster data",
        "Browse your Kubernetes events"
      ],
      "title": "Navigate the Kubernetes cluster explorer",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "a3ef4aa459ed4503201c686000dff3a75331c2f7",
      "image": "https://docs.newrelic.com/static/34f90215b59ab8d7b4ec986bbb110805/9b7bd/nr1-cluster-explorer-node-tooltip.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/",
      "published_at": "2021-09-26T22:15:51Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes cluster explorer uses the data collected by the Kubernetes integration to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events integration, everything that happens in your cluster becomes visible, and logs brought in using the logs plugin are also available. Meet the cluster explorer The cluster explorer represents your most relevant cluster data on a chart with the shape of a ship's wheel — which is also Kubernetes' logo. Outer ring: Contains up to 24 nodes of your cluster, the most relevant based on the amount of alerts. Hover over each node to check resource consumption and the percentage of allocable pods used. Inner rings: Contain the pods ( ) of each node. Pods with active alerts are shown in the third innermost ring, and pods that are pending or unable to run are in the center. Hover the mouse over each node or pod to get a quick overview of its resource usage. You can click each node and pod to view its resource usage over time or to get more information about its health and active alerts. Colors are based on predefined alert conditions: Yellow pods have active warning alerts, while red pods have active critical alerts. one.newrelic.com > Kubernetes cluster explorer: Click any pod to get more information about its status and health, and to dig deeper into application data and traces, logs, and events. Click a node to see the following data: Pod statistics CPU, memory, and storage consumption against allocatable amounts Amount of pods used by the node against the allocatable amount of pods For each pod, depending on the integrations and features you've enabled, you can see: Pod status and metadata, including namespace and deployment Container status and statistics Active alerts (both warning and critical) Kubernetes events that happened in that pod APM data and traces (if you've linked your APM data) A link to the pods' and containers' logs, collected using the Kubernetes plugin for New Relic Logs Cluster and control plane statistics are always visible on the left side. Cluster dashboard The cluster dashboard can be accessed at any time from the cluster explorer by clicking Kubernetes dashboard. It provides a curated dashboard experience for your Kubernetes cluster. one.newrelic.com > Kubernetes cluster explorer > Kubernetes dashboard: The Kubernetes dashboard can be accessed from the Kubernetes cluster explorer. It shows useful Kubernetes metric data. Cluster explorer node table Below the cluster explorer is the node table, which shows all the nodes of the cluster, namespace, or deployment. Like all other usage indicators, the table shows consumption against allocatable resources. Search and filter your cluster data The main way to modify the data view in the cluster explorer is by using the top bar to search for specific attributes or values. All the attributes and values collected by the Kubernetes integration can be combined to narrow down the cluster view. one.newrelic.com > Kubernetes cluster explorer: All your Kubernetes cluster's attributes and data points can be used to filter the cluster explorer view. You can also change the time frame using the time picker in the upper right corner. The Auto-refresh box turns the cluster explorer into a real-time dashboard that refreshes every 60 seconds. one.newrelic.com > Kubernetes cluster explorer: The time picker lets you select several predefined time spans. To reload the data every minute, check the auto-refresh box. Browse your Kubernetes events If you’ve enabled the Kubernetes events integration, you can click the Events tab to browse everything that happened in your cluster, from warnings to normal events. To set it up, select the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into logs and infrastructure data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.18677,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Navigate the <em>Kubernetes</em> cluster explorer",
        "sections": "Navigate the <em>Kubernetes</em> cluster explorer",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> cluster explorer uses the <em>data</em> collected by the <em>Kubernetes</em> <em>integration</em> to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events"
      },
      "id": "603eb9a364441f82484e8879"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-09-26T18:21:57Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.02,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "322c24d8737fb1e9625f35fe495bf22b23f046f6",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2021-09-20T19:43:49Z",
      "updated_at": "2021-09-20T19:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.11.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.94334,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "See your <em>data</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That <em>data</em> can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "603e7f98196a67beaea83dbf"
    }
  ],
  "/docs/integrations/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer": [
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-09-26T22:16:39Z",
      "updated_at": "2021-08-08T13:47:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration condition.{conditionName}={conditionValue} Status of the current observed node condition. The reported conditions can vary depending on your Kubernetes flavor and installed operators. Examples of common conditions are: Ready, DiskPressure, MemoryPressure, PIDPressure and NetworkUnavailable. Condition values can be 1 (true), 0 (false), or -1 (unknown). cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem unschedulable Status of node schedulability of new pods. Its value can be 0 (false) or 1 (true) label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podIP IP address of the pod. If it doesn't have an IP, it'll be empty podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.90262,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " Relic <em>integration</em> you <em>use</em>, including the <em>Kubernetes</em> <em>integration</em>: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a <em>Kubernetes</em> (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message to the notification"
      },
      "id": "603eb9a4196a678bfca83dbb"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-09-26T18:21:57Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.02,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "322c24d8737fb1e9625f35fe495bf22b23f046f6",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2021-09-20T19:43:49Z",
      "updated_at": "2021-09-20T19:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.11.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.9433,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "See your <em>data</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That <em>data</em> can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "603e7f98196a67beaea83dbf"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.86041,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-app-service-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.4518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.86041,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-application-gateway-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.4518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.86041,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-containers-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.4518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.4518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.4518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-data-factory-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.4518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.4518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-postgresql-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-event-hub-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-express-route-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-firewalls-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-front-door-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-functions-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-key-vault-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-load-balancer-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-logic-apps-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-machine-learning-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-power-bi-dedicated-capacities-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-redis-cache-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-bus-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration": [
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-09-20T13:29:38Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.37798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-09-20T13:29:38Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.37798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-managed-instances-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-storage-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machines-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure virtual machine scale sets monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Virtual machine scale sets ScaleSet data"
      ],
      "title": "Azure virtual machine scale sets monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2b25b6720032817e09a6e844210f020b3a4fc98b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration/",
      "published_at": "2021-09-20T16:35:34Z",
      "updated_at": "2021-03-16T04:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Azure virtual machine scale sets data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure virtual machine scale sets integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. You can query and explore your data using the following event type: Entity Event type Provider ScaleSet AzureVirtualMachineScaleSetSample AzureVirtualMachineScaleSet For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure virtual machine scale sets data for ScaleSet. Virtual machine scale sets ScaleSet data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the Virtual Machine(s) networkInBytes Bytes The number of billable bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutBytes Bytes The number of billable bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic) diskReadBytes Bytes Bytes read from disk during monitoring period diskWriteBytes Bytes Bytes written to disk during monitoring period diskReadOperationsCountPerSecond CountPerSecond Disk Read IOPS diskWriteOperationsCountPerSecond CountPerSecond Disk Write IOPS cpuCreditsRemaining Count Total number of credits available to burst cpuCreditsConsumed Count Total number of credits consumed by the Virtual Machine dataDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period dataDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period dataDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period dataDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period dataDiskQueueDepth Count Data Disk Queue Depth(or Queue Length) osDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period for OS disk osDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period for OS disk osDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period for OS disk osDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period for OS disk osDiskQueueDepth Count OS Disk Queue Depth(or Queue Length) inboundFlows Count Inbound Flows are number of current flows in the inbound direction (traffic going into the VM) outboundFlows Count Outbound Flows are number of current flows in the outbound direction (traffic going out of the VM) inboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of inbound flows (traffic going into the VM) outboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of outbound flows (traffic going out of the VM) premiumDataDiskCacheReadHitPercent Percent Premium Data Disk Cache Read Hit premiumDataDiskCacheReadMissPercent Percent Premium Data Disk Cache Read Miss premiumOSDiskCacheReadHitPercent Percent Premium OS Disk Cache Read Hit premiumOSDiskCacheReadMissPercent Percent Premium OS Disk Cache Read Miss networkInTotalBytes Bytes The number of bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutTotalBytes Bytes The number of bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1159.7717,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": " data. Metric data This <em>integration</em> collects <em>Azure</em> <em>virtual</em> machine <em>scale</em> <em>sets</em> data for <em>ScaleSet</em>. <em>Virtual</em> machine <em>scale</em> <em>sets</em> <em>ScaleSet</em> data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the <em>Virtual</em> Machine(s) networkInBytes Bytes"
      },
      "id": "603ea1cfe7b9d2a8342a0819"
    },
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "c641d1367f1f8fd2b589a2707112759becae609b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2021-09-26T13:48:02Z",
      "updated_at": "2021-09-05T01:49:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Our automated installer will generate either a helm command or a set of plain manifests for you to install. Our automated installer: Allows users to select the cluster name and namespace for the installation. Allows users to selectively enable or disable bundling of Kube-state-metrics, a dependency of the Kubernetes integration. Allows users to seamlessly install our other products related to Kubernetes such as: Kubernetes events monitoring In-cluster prometheus services monitoring Service instrumentation without code changes using Pixie Automatically fills the required properties with the license keys the integration needs to work. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard, or follow the instructions. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 156.7238,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Kubernetes <em>integration</em>",
        "sections": "Introduction to the Kubernetes <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": " from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes <em>integration</em> We have an automated installer to help you with many types of installations: servers, <em>virtual</em> <em>machines</em>, and unprivileged environments. It can also help you"
      },
      "id": "6043a212196a678d86960f46"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-09-20T19:19:10Z",
      "updated_at": "2021-09-14T18:17:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.54977,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": ".network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond <em>Azure</em> <em>Virtual</em> Network <em>azure</em>.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond <em>Azure</em> <em>Virtual</em> Network <em>azure</em>.network.virtualnetworks.availableAddresses availableAddresses <em>Azure</em> VMs <em>Scale</em> <em>Sets</em> <em>azure</em>.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed <em>Azure</em> VMs"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45172,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59795,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-09-20T13:29:38Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.37796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-09-20T18:20:19Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.45172,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-09-20T16:55:19Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.59795,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-09-20T17:42:32Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.8604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-09-21T05:52:23Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.20703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-09-20T19:19:10Z",
      "updated_at": "2021-09-14T18:17:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.21739,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-09-21T01:17:12Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.68121,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-09-21T05:52:23Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.20703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-09-21T05:30:52Z",
      "updated_at": "2021-08-02T13:06:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.51735,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-09-21T01:17:12Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.68121,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations": [
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-09-20T19:19:10Z",
      "updated_at": "2021-09-14T18:17:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.21735,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-09-21T05:30:52Z",
      "updated_at": "2021-08-02T13:06:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.51735,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-09-21T01:17:12Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.68121,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-09-21T05:52:23Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.20699,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-09-20T19:19:10Z",
      "updated_at": "2021-09-14T18:17:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.21735,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-09-21T05:30:52Z",
      "updated_at": "2021-08-02T13:06:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.51735,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/mlops-integrations/algorithmia-mlops-integration": [
    {
      "sections": [
        "NGINX monitoring integration",
        "Tip",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate the integration",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Enabling your NGINX Server",
        "Configure the integration",
        "Important",
        "NGINX Instance Settings",
        "Labels/Custom Attributes",
        "Example configurations",
        "BASIC CONFIGURATION",
        "HTTP BASIC AUTHENTICATION",
        "METRICS ONLY WITH SELF-SIGNED CERTIFICATE",
        "ENVIRONMENT VARIABLES REPLACEMENT",
        "MULTI-INSTANCE MONITORING",
        "Find and use data",
        "Metrics",
        "NGINX Open Source",
        "NGINX Plus",
        "Inventory data",
        "System metadata",
        "Check the source code"
      ],
      "title": "NGINX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "07889751e13462278f4874ff42fd2cdbd11ad12a",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nginx-monitoring-integration/",
      "published_at": "2021-09-26T13:45:10Z",
      "updated_at": "2021-09-26T01:44:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Tip Want to help make this doc better? Try our beta NGINX monitoring integration doc and give us feedback by creating a GitHub issue. Help us continue creating great experiences for you! Our NGINX integration collects and sends inventory and metrics from your NGINX server to our platform, where you can see data on connections and client requests so that you can find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with both NGINX Open Source and NGINX Plus. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. NGINX extension enabled, as described in the Configure the integration section. If NGINX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running NGINX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your NGINX server quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the NGINX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your NGINX server. Install and activate the integration To install the NGINX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the general instructions for installing an integration, using the filename nri-nginx. Then continue on to the steps below. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp nginx-config.yml.sample nginx-config.yml Copy Edit the configuration file nginx-config.yml. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your NGINX Server To capture data from the NGINX integration, you must first enable and configure the applicable extension module: For NGINX Open Source: HTTP stub status module For NGINX Plus: HTTP status module and HTTP API module Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, nginx-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to NGINX are defined using the env section of the configuration file. These settings control the connection to your NGINX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. NGINX Instance Settings The NGINX integration collects both Metrics and Inventory information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To STATUS_URL The URL set up to provide the metrics using the status module. http://127.0.0.1/status Metrics/Inventory STATUS_MODULE Name of NGINX status module. Valid options are: discover ngx_http_stub_status_module ngx_http_status_module ngx_http_api_module\" discover Metrics CONNECTION_TIMEOUT Connection timeout to the NGINX instance in seconds. 5 Metrics VALIDATE_CERTS Set to false if the status URL is HTTPS with a self-signed certificate. true Metrics CONFIG_PATH The path to the NGINX configuration file. N/A Inventory REMOTE_MONITORING Enable multi-tenancy monitoring. true Metrics/Inventory METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here or see example below. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations BASIC CONFIGURATION This is the very basic configuration to collect Metrics and Inventory from your localhost: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: http://127.0.0.1/status STATUS_MODULE: discover REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer - name: nri-nginx env: INVENTORY: \"true\" STATUS_URL: http://127.0.0.1/status CONFIG_PATH: /etc/nginx/nginx.conf REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/nginx Copy HTTP BASIC AUTHENTICATION This configuration collects Metrics and Inventory from your localhost protected with basic authentication. Replace the username and password on the STATUS_URL with your credentials: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: http://username:password@127.0.0.1/status STATUS_MODULE: discover REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer - name: nri-nginx env: INVENTORY: \"true\" STATUS_URL: http://username:password@127.0.0.1/status CONFIG_PATH: /etc/nginx/nginx.conf REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/nginx Copy METRICS ONLY WITH SELF-SIGNED CERTIFICATE In this configuration we only have 1 integration block with METRICS: true to collect only metrics and added VALIDATE_CERTS: false to prevent validation of the server's SSL certificate when using a self-signed one: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: https://my_nginx_host/status STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer Copy ENVIRONMENT VARIABLES REPLACEMENT In this configuration we are using the environment variable NGINX_STATUS to populate the STATUS_URL setting of the integration: integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: {{NGINX_STATUS}} STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer Copy MULTI-INSTANCE MONITORING In this configuration we are monitoring multiple NGINX servers from the same integration. For the first instance (STATUS_URL: https://1st_nginx_host/status) we are collecting metrics and inventory while for the second instance (STATUS_URL: https://2nd_nginx_host/status) we will only collect metrics. integrations: - name: nri-nginx env: METRICS: \"true\" STATUS_URL: https://1st_nginx_host/status STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer - name: nri-nginx env: INVENTORY: \"true\" STATUS_URL: https://1st_nginx_host/status CONFIG_PATH: /etc/nginx/nginx.conf REMOTE_MONITORING: true interval: 60s labels: env: production role: load_balancer inventory_source: config/nginx - name: nri-nginx env: METRICS: \"true\" STATUS_URL: http://2nd_nginx_host/status STATUS_MODULE: discover VALIDATE_CERTS: false REMOTE_MONITORING: true interval: 30s labels: env: production role: load_balancer Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the NginxSample event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metrics The NGINX integration collects the following metric data attributes. To find these attributes, query the NginxSample event type. NGINX Open Source Metric Description net.connectionsActive Number of connections that are currently active net.connectionsAcceptedPerSecond Number of accepted client connections per second net.connectionsDroppedPerSecond Number of connections per second that were accepted but could not he handled and hence dropped net.connectionsReading Current number of connections where NGINX is reading the request header net.connectionsWaiting Current number of idle client connections waiting for a request net.connectionsWriting Current number of connections where NGINX is writing the response back to the client net.requestsPerSecond Total number of client requests per second NGINX Plus Our integration retrieves all available metric data from the following HTTP API endpoints: /nginx, /processes, /connections, /http/requests, and /ssl. Metric Description net.connectionsAcceptedPerSecond Accepted client connections as requests per second net.connectionsDroppedPerSecond Dropped client connections as requests per second net.connectionsActive Current number of active client connections net.connectionsIdle Current number of idle client connections net.requests Current number of requests net.requestsPerSecond Current number of requests per second processes.respawned Current number of abnormally terminated and respawned child processes ssl.handshakes Current number for successful SSL handshakes ssl.failedHandshakes Current number of failed SSL handshakes ssl.sessionReuses Current number of session reuses during SSL handshake Inventory data The integration captures configuration options defined in the NGINX master config file (usually nginx.conf). Tip The master NGINX config file can contain \"include OTHER_FILE_NAME\" commands for splitting the configuration into multiple files. The Infrastructure agent ignores (does not parse) configuration set via include commands. System metadata The integration collects these additional attributes about the NGINX service: Name Description software.edition The NGINX edition: either \"open source\" or \"plus\". software.version The version of NGINX. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 47.068428,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "NGINX monitoring <em>integration</em>",
        "sections": "NGINX monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": ". Then continue on to the steps below. Change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy the sample configuration file: sudo cp nginx-config.yml.sample nginx-config.yml Copy Edit the configuration file nginx-config.yml. Restart"
      },
      "id": "6043a323196a67ee9a960f24"
    },
    {
      "sections": [
        "On-host integrations: Legacy configuration format",
        "Important",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Legacy configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "8f1d23b9999a433e49ff5c2ea7d9d9db95eb57a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-legacy-configuration-format/",
      "published_at": "2021-09-26T11:14:28Z",
      "updated_at": "2021-09-26T11:14:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format, check the update section For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 45.071327,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Legacy configuration format",
        "sections": "On-host <em>integrations</em>: Legacy configuration format",
        "tags": "Create <em>integrations</em>",
        "body": "New Relic Infrastructure on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, legacy configuration format. Important New Relic recommends using the new standard improved configuration format. To update your configuration file to this new format"
      },
      "id": "61505613196a676ce3b70d9a"
    },
    {
      "sections": [
        "Introduction to AWS integrations",
        "Connect AWS and New Relic",
        "Integrations and AWS costs",
        "View your AWS data",
        "Region availability"
      ],
      "title": "Introduction to AWS integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "26a36d0da0ba98b48ccaff2e574ec4e535e68844",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/introduction-aws-integrations/",
      "published_at": "2021-09-20T19:30:51Z",
      "updated_at": "2021-09-20T19:30:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon integrations let you monitor your AWS data in several New Relic features. Enabling the AWS CloudWatch Metric Streams integration is the recommended solution to monitor all CloudWatch metrics from all AWS services (including custom namespaces). On top of this, additional integrations are available to get extended visibility on key AWS services beyond the available CloudWatch metrics. For a full reference of the supported metrics, please check the available CloudWatch metrics for each service in the AWS documentation pages. Connect AWS and New Relic In order to obtain AWS data, follow the procedure to connect AWS to New Relic. Additional API Polling integrations can be enabled on top of the AWS CloudWatch metric streams in order to pull data that's not available as CloudWatch metrics. The following integrations are not replaced by the metric streams: AWS Billing AWS CloudTrail AWS Health AWS Trusted Advisor AWS VPC Finally, other integrations may require additional configurations in your AWS account: AWS VPC Flow Logs AWS CloudFormation Integrations and AWS costs Keep in mind the following items: AWS CloudWatch metric streams pricing is defined based on the number of metric updates. For up-to-date pricing information check AWS CloudWatch Pricing. AWS Kinesis Data Firehose is used as the delivery method. For details, see the AWS Firehose pricing page. AWS Config can be optionally enabled in your AWS account, and used to enrich CloudWatch metrics with custom tags and resource metadata. With AWS Config, you are charged based on the number of configuration items recorded. See the AWS Config pricing page for details. If polling integrations are enabled (instead of metric streams), New Relic uses the Amazon CloudWatch API to obtain metrics from the AWS services you monitor. The number of calls to the CloudWatch API increases as you enable more integrations. Add AWS resources to those integrations, or scale those integrations across more regions. This can cause requests to the CloudWatch API to exceed the 1 million free limits granted by AWS and increase your CloudWatch bill. AWS offers enhanced monitoring for some of their services which allows for more metrics, more often. For example, see RDS enhanced monitoring costs. View your AWS data Once you follow the configuration process, data from your Amazon Web Services report directly to New Relic. AWS entities for most used services will be listed in the New Relic Explorer. Metrics and events will appear in the Data Explorer. AWS data will also be visible in the Infrastructure UI. To view your AWS data: Go to one.newrelic.com > Infrastructure > AWS. For any of the AWS integrations listed: For active streams, select the Explore your data link. OR For other integrations, browse the available dashboard or click on the Explore Data link. You can view and reuse NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Region availability Most AWS services offer regional endpoints to reduce data latency between cloud resources and applications. New Relic can obtain monitoring data from services and endpoints that are located in all AWS regions, except China.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 40.28179,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to AWS <em>integrations</em>",
        "sections": "Introduction to AWS <em>integrations</em>",
        "tags": "<em>Integrations</em>",
        "body": "Amazon <em>integrations</em> let you monitor your AWS data in several New Relic features. Enabling the AWS CloudWatch Metric Streams integration is the recommended solution to monitor all CloudWatch metrics from all AWS services (including custom namespaces). On top of this, additional <em>integrations</em>"
      },
      "id": "603e84ec28ccbc9dffeba789"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/dropwizard/dropwizard-reporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.21307,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.20743,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-09-20T22:57:47Z",
      "updated_at": "2021-07-27T16:01:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.76749,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic provides <em>open</em> <em>source</em> <em>integrations</em> that report <em>telemetry</em> data from <em>telemetry</em> tools to your New Relic account. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>Open</em>Census, <em>OpenTelemetry</em>, <em>DropWizard</em>, Prometheus, and more. With these solutions, you can aggregate"
      },
      "id": "603e95ab28ccbc036aeba789"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent": [
    {
      "sections": [
        "Roku open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Roku open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "f0982a0ff96c8a85683bf3ef27a2e4cde85ff274",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent/",
      "published_at": "2021-09-26T22:20:14Z",
      "updated_at": "2021-04-27T11:09:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Roku behavior with New Relic using the Roku open-source agent. The agent contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Roku Agent README on GitHub. Visit New Relic’s Roku repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Roku agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 358.47598,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Roku behavior with New Relic using the Roku <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em> contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff64441f618a9d8533"
    },
    {
      "sections": [
        ".NET agent configuration",
        "Important",
        "Configuration overview",
        "Configuration methods and precedence levels",
        "Required environment variables",
        "Caution",
        ".NET Framework environment variables",
        ".NET Core environment variables",
        "Profiler conflict explanation",
        "Optional environment variables",
        "Setup options, newrelic.config",
        "Configuration element",
        "agentEnabled",
        "maxStackTraceLines",
        "timingPrecision",
        "Service element",
        "licenseKey (required)",
        "sendEnvironmentInfo",
        "syncStartup",
        "sendDataOnExit",
        "sendDataOnExitThreshold",
        "completeTransactionsOnThread",
        "requestTimeout",
        "autoStart",
        "Obscuring key element",
        "Proxy element",
        "host",
        "port",
        "uriPath",
        "domain",
        "user",
        "password",
        "passwordObfuscated",
        "Log element",
        "level",
        "auditLog",
        "console",
        "directory",
        "fileName",
        "Application element (required)",
        "name",
        "disableSamplers",
        "Data transmission element",
        "putForDataSend",
        "Host name",
        "Set using config file",
        "Set using environment variable",
        "Cloud platform utilization",
        "detectAws",
        "detectAzure",
        "detectGcp",
        "detectPcf",
        "detectDocker",
        "detectKubernetes",
        "Instrumentation options",
        "Instrumentation element",
        "Applications element (instrumentation)",
        "Attributes element",
        "enabled",
        "include",
        "exclude",
        "Feature options",
        "App pools",
        "defaultBehavior",
        "applicationPool",
        "Cross application traces",
        "Error collection",
        "Tip",
        "captureEvents",
        "maxEventSamplesStored",
        "ignoreClasses",
        "ignoreMessages",
        "ignoreErrors (obsolete)",
        "ignoreStatusCodes",
        "expectedClasses",
        "expectedMessages",
        "expectedStatusCodes",
        "attributes",
        "High security mode",
        "Strip exception messages",
        "Transaction events",
        "maximumSamplesStored",
        "Custom events",
        "Custom parameters",
        "Labels (tags)",
        "Browser instrumentation",
        "autoInstrument",
        "requestPathsExcluded",
        "Slow queries",
        "Transaction traces",
        "transactionThreshold",
        "recordSql",
        "explainEnabled",
        "explainThreshold",
        "maxSegments",
        "maxExplainPlans",
        "maxStackTrace",
        "Datastore tracer",
        "instanceReporting",
        "databaseNameReporting",
        "queryParameters",
        "Distributed tracing",
        "excludeNewrelicHeader",
        "Disable span events via config file",
        "Disable span events via environment variable",
        "Infinite Tracing",
        "trace_observer",
        "Span events",
        "Capture HTTP Request Headers",
        "Settings in app.config or web.config",
        "Enable and disable the agent",
        "Application name",
        "License key",
        "Change newrelic.config location",
        "Settings in appsettings.json"
      ],
      "title": ".NET agent configuration",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Configuration"
      ],
      "external_id": "b89fa7fc399f2729bfee8f5106e777798a73177a",
      "image": "https://docs.newrelic.com/static/cffd7eb2d22c8e338531c38f35208c7c/c1b63/net-agent-config-settings-precedence_0.png",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/configuration/net-agent-configuration/",
      "published_at": "2021-09-20T19:41:39Z",
      "updated_at": "2021-09-20T19:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains the configuration options for the APM .NET agent. Important As of September 2021, a small subset of APIs, configuration options, and installation options for .NET will be replaced by new methods. For more details, including how you can easily prepare for this transition, see our Explorers Hub post. Configuration overview APM agent configuration options allow you to control some aspects of how the agent behaves. Some of these config options are part of the basic install process (like setting your license key and app name), but most are more advanced settings, such as: setting a log level, setting up proxy host access, excluding certain attributes, and enabling distributed tracing. The .NET agent gets its configuration from the newrelic.config file, which is generated as part of the install process. By default, only a global newrelic.config file is created, but you can also create app-local newrelic.config files for finer control over a multi-app system. Other ways to set config options include: using environment variables, or setting server-side configuration from the UI. For more on the various config options and what overrides what, see Config settings precedence. Support for both .NET Framework and .NET Core use the same configuration options and have the same APM features, unless otherwise stated. If you make changes to the config file and want to validate that it's in the right format, you can check it against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET Agent\\newrelic.xsd for Windows) with any XSD validator. Important For IIS: after you change your newrelic.config or app.config file, perform an IISRESET from an administrative command prompt. Log level adjustments do not require a reset. Configuration methods and precedence levels Upon installation, the .NET agent's configuration file (newrelic.config) applies to all monitored applications, but you can configure the agent in other ways. Here's a diagram showing how different configuration options take precedence over one another: This diagram explains the order of precedence for different ways you might configure the .NET agent. Here are details about the configuration methods shown in the diagram, and their precedence levels: .NET configuration Details and precedence web.config or app.config or appsettings.json Configuration settings set in these files take highest precedence. However, if the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled settings in these files will be ignored. Environment variables Second-highest precedence. For more about these, see .NET environment variables. Server-side configuration Third-highest precedence. A limited number of server-side configuration settings are available; the other settings will come from other configuration sources. App-local newrelic.config Fourth-highest precedence. You can create app-local newrelic.config files to configure individual apps on a multi-app system. These local configuration files override settings in the global newrelic.config file. The agent looks for app-local config files in the following directories, in this order: A directory specified in your web.config or app.config file with the NewRelic.ConfigFile property The web app's root directory (with the app.config or web.config) The directory containing your app's executable file Note that the app-local config file must be complete and validate against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET Agent\\newrelic.xsd for Windows). Default (global) newrelic.config Default source and the lowest precedence. Will configure all applications on a host in the absence of other config files. The global config file is located in the New Relic agent home directory: %PROGRAMDATA%\\New Relic\\.NET Agent Required environment variables New Relic's .NET agent relies on environment variables to tell the .NET Common Language Runtime (CLR) to attach New Relic to your processes. Some .NET agent install procedures (like the MSI installer) will automatically set these variables for you; some procedures will require you to manually set them. Caution Security recommendation: You should consider what users can set system environment variables. You should also secure the accounts under which your applications execute to prevent user environment variables overriding system environment variables .NET Framework environment variables For .NET Framework, the following variables are required: COR_ENABLE_PROFILING=1 COR_PROFILER={71DA0A04-7777-4EC6-9643-7D28B46A8A41} NEWRELIC_INSTALL_PATH=path\\to\\agent\\directory Copy The .NET agent installer will add these to IIS or as system-wide environment variables. .NET Core environment variables For .NET Core, the following variables are required: Linux: CORECLR_ENABLE_PROFILING=1 CORECLR_PROFILER={36032161-FFC0-4B61-B559-F6C5D41BAE5A} CORECLR_NEWRELIC_HOME=path/to/agent/directory CORECLR_PROFILER_PATH=\"${CORECLR_NEWRELIC_HOME}/libNewRelicProfiler.so\" Copy Windows: CORECLR_ENABLE_PROFILING=1 CORECLR_PROFILER={36032161-FFC0-4B61-B559-F6C5D41BAE5A} NEWRELIC_INSTALL_PATH=path\\to\\agent\\directory CORECLR_NEWRELIC_HOME=path\\to\\agent\\directory Copy The .NET agent installer will add these to IIS or as system-wide environment variables. If your system has previously used monitoring services (non-New Relic), you may have a \"profiler conflict\" when trying to install and use the New Relic agent. More details: Profiler conflict explanation New Relic’s .NET agents rely on environment variables to tell the .NET Common Language Runtime (CLR) to load New Relic into your processes. The install-related environment variables are Microsoft variables, not New Relic variables. They can be used by other .NET profilers, and only one profiler can be attached to a process at a time. For this reason, if you have used previous application monitoring products, you may have profiler conflicts. For specific install instructions, see the .NET agent install documentation. Optional environment variables Some configuration options in New Relic's .NET agent can be set via environment variables as an alternative to setting them in a config file. Below is a list of environment variables recognized by the .NET agent with example values. NEW_RELIC_LICENSE_KEY=XXXXXXXX NEW_RELIC_LOG=MyApp.log NEW_RELIC_APP_NAME=Descriptive Name MAX_TRANSACTION_SAMPLES_STORED=500 MAX_EVENT_SAMPLES_STORED=500 NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_SPAN_EVENTS_ENABLED=false NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED=2000 NEW_RELIC_LABELS=foo:bar;zip:zap NEW_RELIC_CONFIG_OBSCURING_KEY=XXXXXXXX NEW_RELIC_DISABLE_SAMPLERS=true NEWRELIC_PROFILER_LOG_DIRECTORY=path\\to\\a\\directory (not configurable via config file) NEWRELIC_LOG_DIRECTORY=path\\to\\a\\directory (Insert a directory where you want to put the agent and profiler logs. You can't set this directory for both agent and profiler logs in the configuration file.) NEWRELIC_LOG_LEVEL=off|error|warn|info|debug|finest|all Copy Setup options, newrelic.config Use these options to setup and configure your agent via the newrelic.config file. The New Relic .NET agent supports the following categories of setup options: Configuration element Service element Obscuring key element Proxy element Log element Application element (configuration) Data transmission element Host name Configuration element The root element of the configuration document is a configuration element. <configuration xmlns=\"urn:newrelic-config\" agentEnabled=\"true\" maxStackTraceLines=\"50\" timingPrecision=\"low\"> Copy The configuration element supports the following attributes: agentEnabled Type Boolean Default true Enable or disable the New Relic agent. maxStackTraceLines Type Integer Default 80 The maximum number of stack frames to trace in any stack dump. timingPrecision Type String Default low Controls the precision of the timers. High precision will provide better data, but at a lower execution speed. Possible values are high and low. Service element The first child of the configuration element is a service element. The service element configures the agent's connection to the New Relic service. <service licenseKey=\"YOUR_LICENSE_KEY\" sendEnvironmentInfo=\"true\" syncStartup=\"false\" sendDataOnExit=\"false\" sendDataOnExitThreshold=\"60000\" autoStart=\"true\"/> Copy The service element supports the following attributes: licenseKey (required) Type String Default (none) Your New Relic license key. New Relic uses the license key to match your app's data to the correct account in the UI. Set the license key via environment variable. Alternatively, set the NEW_RELIC_LICENSE_KEY environment variable in the application's environment. NEW_RELIC_LICENSE_KEY=XXXXXXXX Copy sendEnvironmentInfo Type Boolean Default true Instructs the agent to record execution environment information. Environment information includes operating system, agent version, and which assemblies are available. syncStartup Type Boolean Default false Block application startup until the agent connects to New Relic. If set to true, the first transaction may take substantially longer to complete, because it is blocked until the connection to New Relic is finished. sendDataOnExit Type Boolean Default false Block application shutdown until the agent sends all data from the latest harvest cycle. sendDataOnExitThreshold Type Integer Default 60000 Unit Milliseconds The minimum amount of time the process must run before the agent blocks it from shutting down. This setting only applies when sendDataOnExit is true. completeTransactionsOnThread Type Boolean Default false If false, the agent uses a pool thread to complete the transaction processing. If true, the agent will complete transaction processing on the request thread. requestTimeout Type Integer Default 2000 (sendDataOnExit enabled) 120000 (sendDataOnExit disabled) Unit Milliseconds The agent's request timeout when communicating with New Relic. autoStart Type Boolean Default True Automatically start the .NET agent when the first instrumented method is hit. Obscuring key element The obscuringKey element is an optional child of the service element. The .NET Agent uses this value to deobfuscate supported configuration values. For example, when an obfuscated proxy password is supplied, it will be deobfuscated using this key. <service licenseKey=\"YOUR_LICENSE_KEY\"> <obscuringKey>OBSCURING_KEY</obscuringKey> </service> Copy The obscuring key may also be configured by setting the NEW_RELIC_CONFIG_OBSCURING_KEY environment variable. Caution Security recommendation: The placement of the obscuring Key in the same configuration file as an obfuscated value may pose a security risk. Consider placing the obscuring key in an environment variable and limiting access to environment variables within your environment. Proxy element The proxy element is an optional child of the service element. The proxy element is used when the agent communicates to the New Relic back-end service via a proxy. <service licenseKey=\"YOUR_LICENSE_KEY\"> <proxy host=\"hostname\" port=\"PROXY_PORT\" uriPath=\"path/to/something.aspx\" domain=\"mydomain.com\" user=\"PROXY_USERNAME\" password=\"PROXY_PASSWORD\" passwordObfuscated=\"OBFUSCATED_PROXY_PASSWORD\"/> </service> Copy The proxy element supports the following attributes: host Type String Default (none) Defines the proxy host. port Type Integer Default 8080 Defines the proxy port. uriPath Type String Default (none) Optionally define a proxy URI path. domain Type String Default (none) Optionally define a domain to use when authenticating with the proxy server. user Type String Default (none) Optionally define a user name for authentication. password Type String Default (none) Optionally define a password for authentication. passwordObfuscated Type String Default (none) For additional security, the .NET Agent supports the use of an obfuscated proxy password with the passwordObfuscated attribute. The obfuscated proxy password is generated using the following New Relic CLI command: newrelic agent config obfuscate --key OBSCURING_KEY --value \"CLEAR_TEXT_PROXY_PASSWORD\" Copy Important When using an obfuscated proxy password, the obscuring key must also be configured. Log element The log element is a child of the configuration element. The log element configures New Relic's logging . The agent generates its own log file to keep its logging information separate from your application's logs. <log level=\"info\" auditLog=\"false\" console=\"false\" directory=\"PATH\\TO\\LOG\\DIRECTORY\" fileName=\"FILENAME.log\" /> Copy The log element supports the following attributes: level Type String Default info Defines the level of detail recorded in the log file. Possible values, in increasing order of detail, are: off error warn info debug finest all Alternatively, set the NEWRELIC_LOG_LEVEL environment variable in the application's environment. Important Increasing the log level will increase New Relic's performance impact. auditLog Type Boolean Default false Records all data sent to and received from New Relic in both an auditlog log file and the standard log file. console Type Boolean Default false Send log messages to the console, in addition to the log file. directory Type String Default C:\\ProgramData\\New Relic\\.NET Agent\\Logs The directory to hold log files generated by the agent. If this is omitted, then a directory named logs in the New Relic agent install area will be used by default. fileName Type String Default (none) Defines a name for the log file. If you do not define a fileName, the name is derived from the name of the monitored process. Alternatively, set the NEW_RELIC_LOG environment variable in the application's environment. NEW_RELIC_LOG=MyApp.log Copy Application element (required) The application element is a child of the configuration element. This required element defines your application name, and disables or enables sampling. name Type String Default My Application The name of your .NET application is a child of the application element. New Relic will aggregate your data according to this name. For example, if you have two running applications named AppA and AppB, you will see two applications in the New Relic interface: AppA and AppB. You can also assign up to three names to your app. The first name is the primary name. For example: <application> <name>MY APPLICATION PRIMARY</name> <name>SECOND APP NAME</name> <name>THIRD APP NAME</name> </application> Copy Alternatively, set the NEW_RELIC_APP_NAME environment variable in the application's environment. NEW_RELIC_APP_NAME=Descriptive Name Copy disableSamplers Type Boolean Default false Samplers collect information about memory and CPU consumption. Set this to true to disable sampling. Alternatively, set the NEW_RELIC_DISABLE_SAMPLERS environment variable in the application's environment. NEW_RELIC_DISABLE_SAMPLERS=true Copy Data transmission element The dataTransmission element is a child of the configuration element. This element affects how data is sent to New Relic and can be used if you have specific data transmission requirements. <dataTransmission putForDataSend=\"false\" compressedContentEncoding=\"deflate\"/> Copy The dataTransmission element supports the following attributes: putForDataSend Type Boolean Default false Defines the HTTP method used when sending data to New Relic. Set this to true to enable using the PUT method when sending data. The POST method is used by default. Host name If the default host name label in the APM UI is not useful, you can decorate that name in the New Relic UI with a display name. After the application process is restarted and the .NET agent is reporting again, the display name will appear in the Servers drop-down list. This host name setting does not affect the list of hosts on your application's Summary page. To set a display name, choose one of the following options. The environment variable takes precedence over the config file value. Then restart your application to see your changes in the New Relic UI. Set using config file Set the displayName attribute in the processHost element in newrelic.config. The processHost element is a child of the configuration element. <configuration . . . > <processHost displayName=\"CUSTOM_NAME\" /> </configuration> Copy Set using environment variable Set the NEW_RELIC_PROCESS_HOST_DISPLAY_NAME environment variable: NEW_RELIC_PROCESS_HOST_DISPLAY_NAME = \"CUSTOM_NAME\" Copy Cloud platform utilization Configures the utilization configuration element to control how the agent collects utilization information and sends it to the New Relic service to determine pricing. The agent can collect information from Amazon Web Services (AWS) EC2 instances, Docker containers, Azure, Google Cloud Platform, Pivotal Cloud Foundry, and Kubernetes. detectAws Type Boolean Default true Determines whether the agent polls AWS metadata API. detectAzure Type Boolean Default true Determines whether the agent polls Azure metadata API. detectGcp Type Boolean Default true Determines whether the agent polls GCP metadata API. detectPcf Type Boolean Default true Determines whether the agent polls PCF information from environment variables. detectDocker Type Boolean Default true Determines whether the agent reads Docker information from the file system. detectKubernetes Type Boolean Default true Determines whether the agent polls Kubernetes information from environment variables. Instrumentation options Use these options to configure which elements of your application and environment to instrument. New Relic for .NET supports the following categories of instrumentation options: Instrumentation element Applications element (instrumentation) Attributes element Instrumentation element The instrumentation element is a child of the configuration element. By default, the .NET agent instruments IIS asp worker processes and Azure web and worker roles. To instrument other processes, see Instrumenting custom applications. Applications element (instrumentation) The applications element is a child of the instrumentation element. The applications element specifies which non-web apps to instrument. It contains a name attribute. Important This is not the same as the application (configuration) element, which is a child of the configuration element. <instrumentation> <applications> <application name=\"MyService1.exe\" /> <application name=\"MyService2.exe\" /> <application name=\"MyService3.exe\" /> </applications> </instrumentation> Copy Attributes element An attribute is a key/value pair that determines the properties of an event or transaction. Each attribute is sent to APM transaction traces, APM error traces, Transaction events, TransactionError events, or PageView events. The primary attributes element enables or disables attribute collection for the .NET agent, and defines specific attributes to collect or exclude. You can also configure attribute settings based on their destination: Error collection, transaction traces, browser instrumentation, and transaction events. In this example, the agent excludes all attributes whose key begins with myApiKey (myApiKey.bar, myApiKey.value), but collects the custom attribute myApiKey.foo. <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> Copy You can view the .NET APM attributes on the .NET agent attributes page. You can also define custom attributes with the agent API call AddCustomAttribute. enabled Type Boolean Default true Enable or disable attribute collection. When set to false in the primary attribute element, this setting overrides all attribute settings for individual destinations. include Type String Default (none) If attributes are enabled, the agent will collect all attribute keys specified in this list. To specify multiple attribute keys, specify each individually. You can also use a * wildcard character at the end of a key to match multiple attributes (for example, myApiKey.*). For more information, see Attribute rules. exclude Type String Default (none) If attributes are enabled, the agent will not collect attribute keys specified in this list. To specify multiple attribute keys, specify each individually. You can also use a * wildcard character at the end of a key to match multiple attributes (for example, myApiKey.*). For more information, see Attribute rules. Feature options Use these options to enable, disable, and configure New Relic features. New Relic for .NET allows you to configure the following features: App pools Cross application traces Error collection High security mode Strip exception messages Transaction events Custom events Custom parameters Tags/labels Browser instrumentation Slow Queries Transaction traces Datastore tracer Distributed tracing Span events Capture HTTP Request Headers App pools Important This is only applicable to a system's global config file. The applicationPools element is a child of the configuration element. The applicationPools element specifies for the profiler exactly which application pools to instrument and uses the same name as the IIS application pool name. This configuration element is useful when you may need to instrument only a small subset of your app pools. For example, a given server might have several hundred application pools, but only a few of those pools need to be instrumented by the .NET agent. Here is an example of disabling instrumentation for specific application pools: <applicationPools> <applicationPool name=\"Foo\" instrument=\"false\"/> <applicationPool name=\"Bar\" instrument=\"false\"/> </applicationPools> Copy Here is an example of disabling instrumentation for all application pools currently executing on the server and enabling instrumentation for specific application pools: <applicationPools> <defaultBehavior instrument=\"false\"/> <applicationPool name=\"Foo\" instrument=\"true\"/> <applicationPool name=\"Bar\" instrument=\"true\"/> </applicationPools> Copy The applicationPools element supports the following elements: defaultBehavior Type Boolean Default false Defines how the .NET agent will behave on a \"global\" level for application pools served via IIS. The .NET agent instruments all application pools by default. When true, application pools listed under applicationPool with an instrument attribute set to false will not be instrumented. Essentially, when set to false, the application pool list acts as an allow list. When set to true, the application pool list acts as a deny list. applicationPool Defines instrumentation behavior for a specific application pool. The name attribute is the name of an application pool. Enable or disable profiling in the instrument attribute. Define this application in the name attribute. Cross application traces The crossApplicationTracer element is a child of the configuration element. crossApplicationTracer links transaction traces across applications. When linked in a service-oriented architecture, all instrumented applications that communicate with each other via HTTP will now \"link\" transaction traces with the applications that they call and the applications they are called by. Cross application tracing makes it easier to understand the performance relationship between services and applications. <crossApplicationTracer enabled=\"true\"/> Copy The crossApplicationTracer element supports the following attribute: enabled Type Boolean Default true Enable or disable cross application tracing Error collection The errorCollector element is a child of the configuration element. errorCollector configures error collection, which captures information about uncaught exceptions and sends them to New Relic. <errorCollector enabled=\"true\" captureEvents=\"true\" maxEventSamplesStored=\"100\"> <ignoreClasses> <errorClass>System.IO.FileNotFoundException</errorClass> <errorClass>System.Threading.ThreadAbortException</errorClass> </ignoreClasses> <ignoreMessages> <errorClass name=\"System.Exception\"> <message>Ignore message</message> <message>Ignore too</message> </errorClass> </ignoreMessages> <ignoreStatusCodes> <code>401</code> <code>404</code> </ignoreStatusCodes> <expectedClasses> <errorClass>System.ArgumentNullException</errorClass> <errorClass>System.ArgumentOutOfRangeException</errorClass> </expectedClasses> <expectedMessages> <errorClass name=\"System.Exception\"> <message>Expected message</message> <message>Expected too</message> </errorClass> </expectedMessages> <expectedStatusCodes>403,500-505</expectedStatusCodes> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </errorCollector> Copy Tip For an overview of error configuration in APM, see Manage errors in APM. Important expectedClasses, expectedMessages, and expectedStatusCodes configuration settings require .NET agent version 8.31.0.0 or higher. The errorCollector element supports the following elements and attributes: enabled Type Boolean Default true Enable or disable the error collector. captureEvents Type Boolean Default true Enable or disable the capturing of error events. maxEventSamplesStored Type Integer Default 100 Reservoir limit for error events. ignoreClasses A list of fully qualified class names to be ignored. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used ignoreMessages An optional map of fully qualified class names to list of strings matching a substring of the message of an error. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used ignoreErrors (obsolete) Type String Default (none) Lists specific exceptions to not report to New Relic. The full name of the exception should be used, such as System.IO.FileNotFoundException. ignoreStatusCodes Type String Default (none) Lists specific HTTP error codes to not report to New Relic. You can use standard integral HTTP error codes, such as just 401, or you may use Microsoft full status codes with decimal points, such as 401.4 or 403.18. The status codes should be equal to or greater than 400. expectedClasses A list of fully qualified class names to be marked as expected. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used expectedMessages An optional map of fully qualified class names to list of strings matching a substring of the message of an error. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used expectedStatusCodes A comma separated list of status codes. The list may include integer ranges, using a single dash (-) and will be inclusive of both the starting and ending integer in the range. attributes Use this sub-element to customize your agent attribute settings for error traces. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. High security mode The highSecurity element is a child of the configuration element. To enable high security mode, set this property to true and enable the high security property in the New Relic user interface. Enabling high security turns SSL on; request parameters, custom parameters and HTTP request headers are not collected; strip exception messages is enabled; and queries can't be sent to New Relic in their raw form. enabled Type Boolean Default false Enable or disable high security mode. Example: <highSecurity enabled=\"true\"/> Copy Strip exception messages The stripExceptionMessages element is a child of the configuration element. To enable strip exception messages, set this property to true. By default, this is set to false, which means that the agent sends messages from all exceptions to the New Relic collector. If you enable high security mode, this is automatically changed to true, and the agent strips the messages from exceptions. enabled Type Boolean Default false Enable or disable strip exception messages. Example: <stripExceptionMessages enabled=\"true\"/> Copy Transaction events The transactionEvents element is a child of the configuration element. Use transactionEvents to configure transaction events. <transactionEvents enabled=\"true\" maximumSamplesStored=\"10000\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </transactionEvents> Copy The transactionEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Integer Default 10000 The maximum number of samples to store in memory at once. Alternatively, set the MAX_TRANSACTION_SAMPLES_STORED environment variable in the application's environment. MAX_TRANSACTION_SAMPLES_STORED=500 Copy attributes Use this sub-element to customize your agent attribute settings for transaction events. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Tip These attribute settings are specific to transaction events. Attribute settings can be applied globally to all event types to with this configuration setting. Caution When distributed tracing and/or Infinite Tracing are enabled, information from transaction events is applied to the root Span Event of the transaction. Consider applying any attribute settings for transaction events to span events and/or apply them as Global Attribute settings. Custom events The customEvents element is a child of the configuration element. Use customEvents to configure custom events. <customEvents enabled=\"true\" maximumSamplesStored=\"10000\"/> Copy The CustomEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Integer Default 10000 The maximum number of samples to store in memory at once. Alternatively, set the MAX_EVENT_SAMPLES_STORED environment variable in the application's environment. MAX_EVENT_SAMPLES_STORED=500 Copy Custom parameters The customParameters element is a child of the configuration element. Use customParameters to configure custom parameters. <customParameters enabled=\"true\" /> Copy The CustomParameters element supports the following attributes: enabled Type Boolean Default true Enable or disable the capture of custom parameters. Labels (tags) The labels element is a child of the configuration element. This sets tag names and values. The list is a semicolon delimited list of colon-separated name and value pairs. You can also use with the NEW_RELIC_LABELS environment variable. Example: <labels>foo:bar;zip:zap</labels> Copy Browser instrumentation The browserMonitoring element is a child of the configuration element. browserMonitoring configures browser monitoring in your .NET application. Browser gives you insight your end users' performance experience. This is accomplished by measuring the time it takes for your users' browsers to download and render your webpages by injecting a small amount of JavaScript code into the header and footer of each page. // If you use both the Exclude and Attribute elements // the Exclude element must be listed first. <browserMonitoring autoInstrument=\"true\"> <requestPathsExcluded> <path regex=\"url-regex-1\"/> <path regex=\"url-regex-2\"/> ... <path regex=\"url-regex-n\"/> </requestPathsExcluded> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </browserMonitoring> Copy The browserMonitoring element supports the following attributes: autoInstrument Type Boolean Default true By default the agent automatically injects the browser agent JavaScript. To turn off automatic injection, set this attribute to false. attributes Use this sub-element to customize your agent attribute settings for browser monitoring. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. requestPathsExcluded Use this sub-element to prevent the browser agent from being injected in specific pages. The element is used as follows: <requestPathsExcluded> <path regex=\"url-regex-1\"/> <path regex=\"url-regex-2\"/> ... <path regex=\"url-regex-n\"/> </requestPathsExcluded> Copy The agent will not inject the browser agent into pages whose URL matches one of the specified regular expressions. The regular expression should follow Microsoft guidelines for the Regex class. It is a reference to the virtual directory of the path in your application and not the full URL of the path you wish to exclude. For example, to exclude the pages in https://www.mywebsite.com/mywebpages/ you would simply insert /mywebpages/ as the path regex value. The requestPathsExcluded element should be used in cases where it is impossible or undesirable to use the DisableBrowserMonitoring() call. To minimize a possible performance impact try to use as few regular expressions as possible and keep them as simple as possible. Slow queries The slowSql element is a child of the configuration element. slowSql configures capturing information about slow query executions, and captures and obfuscates explain plans for these queries. <slowSql enabled=\"true\"/> Copy The slowSql element supports the following attribute: enabled Type Boolean Default true Enable or disable slow query tracing. Transaction traces The transactionTracer element is a child of the configuration element. transactionTracer configures transaction traces. Included in the trace is the exact call sequence of the transactions, including any query statements issued. <transactionTracer enabled=\"true\" transactionThreshold=\"apdex_f\" recordSql=\"obfuscated\" explainEnabled=\"true\" explainThreshold=\"500\" maxSegments=\"3000\" maxExplainPlans=\"20\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </transactionTracer> Copy The transactionTracer element supports the following attributes: enabled Type Boolean Default true Enable or disable transaction traces. transactionThreshold Type String Default apdex_f Defines the threshold for transaction traces. If a transaction takes longer than the threshold, it is eligible for being traced. See transaction trace basics for more about the rules governing traces. The default value is apdex_f, which sets the threshold to four times the application's apdex_t value. For more information about apdex_t, see Apdex. You can also set the threshold to be a specific time value in milliseconds. recordSql Type String Default obfuscated Select a query tracing policy. Options are off, which records nothing; obfuscated, which records an obfuscated version of the query; or raw, which records the query exactly as it is issued to the database. Caution Recording raw queries may capture sensitive information. explainEnabled Type Boolean Default false When true, the agent captures EXPLAIN statements for slow queries. explainThreshold Type Integer Default 500 Unit Milliseconds The agent collects slow query data for queries that exceed this threshold, along with any available explain plans, as part of transaction traces. maxSegments Type Integer Default 3000 The maximum number of segments to collect in a transaction trace. maxExplainPlans Type Integer Default 20 The maximum number of explain plans to collect during a harvest cycle. maxStackTrace Type Integer Default 0 By default maxStackTrace is set to 0, which disables stack traces as part of a transaction trace. If this value is set greater than 0, then stack traces will be captured for transaction traces. attributes Use this sub-element to customize your agent attribute settings for transaction traces. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Datastore tracer The datastoreTracer element is a child of the configuration element. <datastoreTracer> <instanceReporting enabled=\"true\" /> <databaseNameReporting enabled=\"true\" /> <queryParameters enabled=\"false\" /> </datastoreTracer> Copy The datastoreTracer element supports the following sub-elements: instanceReporting Use this sub-element to enable collection of datastore instance metrics (such as the host and port) for some database drivers. These are reported on slow query traces and transaction traces. The default value of attribute enabled is true. databaseNameReporting Use this sub-element to enable collection of the database name on slow query traces and transaction traces for some database drivers. The default value of attribute enabled is true. queryParameters Use this sub-element to enable collection of the SQL query parameters on slow query traces. The default value of attribute enabled is false. Caution Recording query parameters may capture sensitive information. The transactionTracer.recordSql configuration option must be set to raw or this option is ignored. Distributed tracing The distributedTracing element is a child of the configuration element. <distributedTracing enabled=\"false\" excludeNewrelicHeader=\"false\"/> Copy Distributed tracing lets you see the path that a request takes as it travels through a distributed system. Enabling distributed tracing disables cross application tracing, and has other effects on APM features. Before enabling, read the planning guide. Important Requires .NET agent version 8.6.45.0 or higher. The distributedTracing element supports the following attributes: To enable or disable, see Enable distributed tracing. enabled Type Boolean Default false Alternatively, enable distributed tracing via the NEW_RELIC_DISTRIBUTED_TRACING_ENABLED environment variable in the application's environment. NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy excludeNewrelicHeader Type Boolean Default false By default, supported versions of the agent utilize both the newrelic header and W3C Trace Context headers for distributed tracing. The newrelic distributed tracing header allows interoperability with older agents that don't support W3C Trace Context headers. Agent versions that support W3C Trace Context headers will prioritize them over newrelic headers for distributed tracing. If you do not want to utilize the newrelic header, setting this to true will result in the agent excluding the newrelic header and only using W3C Trace Context headers for distributed tracing. Distributed tracing reports span events. Span event reporting is enabled by default, but distributed tracing must be enabled for spans to be reported. To disable span events, choose one of the following options: Disable span events via config file Set the <spanEvents> element to false to disable via the newrelic.config file. This element is a child of the <configuration> element. <configuration . . . > <spanEvents enabled=\"false\" /> </configuration> Copy Disable span events via environment variable Set the NEW_RELIC_SPAN_EVENTS_ENABLED environment variable in the application's environment. NEW_RELIC_SPAN_EVENTS_ENABLED=false Copy Infinite Tracing Infinite Tracing extends the distributed tracing service by employing a trace observer that is external to the agent. It observes 100% of your application traces across various services and provides actionable data so you can solve issues faster. Important Infinite Tracing requires .NET Agent version 8.30 or higher. To turn on Infinite Tracing, enable distributed tracing and add the additional settings below <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy The infiniteTracing element supports the following elements: trace_observer The trace_observer element identifies an observer host that is independent from the agent. For help getting a valid Infinite Tracing trace observer host entry, see Find or create a trace observer endpoint. The trace observer may be configured using the NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST environment variable as well. Important When configuring the trace observer, you should not supply the protocol as part of the host. For example, use myhost.infinitetracing.com instead of https://myhost.infinitetracing.com. Span events The spanEvents element is a child of the configuration element. Use spanEvents to configure span events. <spanEvents enabled=\"true\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </spanEvents> Copy The spanEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Int Default 2000 The maximum number of samples to store in memory at a time. This may be configured using the NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED environment variable as well. Important This configuration option is only available in the .NET Agent v9.0 or higher. attributes Use this sub-element to customize your agent attribute settings for span events. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Tip These attribute settings are specific to span events. Attribute settings may be applied globally to all event types to with this configuration setting. Capture HTTP Request Headers The allowAllHeaders element is a child of the configuration element. Set this to true to allow the .NET Agent to capture all HTTP request headers as request.headers.{http-header-name} attributes. Set this to false to only allow the .NET agent to collect the following HTTP request headers: request.headers.referer request.headers.accept request.headers.content-length request.headers.host request.headers.user-agent Copy enabled Type Boolean Default false Enable or disable HTTP request headers capture. Example: <allowAllHeaders enabled=\"true\" /> <attributes enabled=\"true\"> <include>request.headers.*</include> </attributes> Copy Important The allowAllHeaders setting is only available in the .NET Agent version 8.40.0+. When using allowAllHeaders to capture attributes, the captured request header attributes are still being controlled by the root level and destination level attributes settings. Without setting the request.header.* in the include list under the attributes element (see the following), the .NET Agent still filters out all header attributes. The default newrelic.config is set to include the request.header.*. <allowAllHeaders enabled=\"true\" /> <attributes enabled=\"true\"> <include>request.headers.*</include> ... </attributes> Copy The default newrelic.config is also set to explicitly exclude the following HTTP request headers to prevent the .NET Agent collecting unwanted data. <attributes enabled=\"true\"> <exclude>request.headers.cookie</exclude> <exclude>request.headers.authorization</exclude> <exclude>request.headers.proxy-authorization</exclude> <exclude>request.headers.x-*</exclude> </attributes> Copy Settings in app.config or web.config For ASP.NET and .NET Framework console apps you can also configure the following settings in your app's app.config or web.config, within the outermost element, <configuration>: Enable and disable the agent <appSettings> <add key = \"NewRelic.AgentEnabled\" value=\"false\" /> </appSettings> Copy Important If the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled settings in these files will be ignored. Application name For more information, see Name your .NET application. <appSettings> <add key = \"NewRelic.AppName\" value =\"Descriptive Name\" /> </appSettings> Copy License key <appSettings> <add key = \"NewRelic.LicenseKey\" value =\"XXXXXXXX\" /> </appSettings> Copy Change newrelic.config location Designates an alternative location for the config file outside of the local root of the app or global config location. The location entered must be an absolute path. <appSettings> <add key = \"NewRelic.ConfigFile\" value=\"C:\\Path-to-alternate-config-dir\\newrelic.config\" /> </appSettings> Copy Settings in appsettings.json For .NET Core apps, you can configure the following settings in appsettings.json if the following is true: The appsettings.json file must be located in the current working directory of the application. The application must have the following dependencies: Microsoft.Extensions.Configuration Microsoft.Extensions.Configuration.Json Microsoft.Extensions.Configuration.EnvironmentVariables Enable and disable the agent { \"NewRelic.AgentEnabled\":\"false\" } Copy Important If the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled setting in this file will be ignored. Application name For more information, see Name your .NET application. { \"NewRelic.AppName\": \"Descriptive Name\" } Copy License key { \"NewRelic.LicenseKey\": \"XXXXXXXX\" } Copy Change newrelic.config location Designates an alternative location for the config file outside of the local root of the app or global config location. The location entered must be an absolute path. { \"NewRelic.ConfigFile\": \"C:\\\\Path-to-alternate-config-dir\\\\newrelic.config\" } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 26.201065,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET <em>agent</em> configuration",
        "sections": ".NET <em>agent</em> configuration",
        "tags": "<em>Agents</em>",
        "body": " (with the app.config or web.config) The directory containing your app&#x27;s executable file Note that the app-local config file must be complete and validate against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET <em>Agent</em>\\newrelic.xsd for Windows). Default (global) newrelic.config Default <em>source</em>"
      },
      "id": "60446c3b196a679d6a960f7a"
    },
    {
      "sections": [
        "Enable and disable attributes (.NET)",
        "Attribute rules",
        "Root level takes precedence for enabled.",
        "Destination enabled takes precedence over include and exclude.",
        "Attribute is included if the destination is enabled.",
        "Exclude always supersedes include.",
        "Keys are case sensitive.",
        "Use an asterisk * for wildcards.",
        "Most specific setting for a key takes priority.",
        "Include or exclude affects the specific destination.",
        "Obsolete properties",
        "analyticsEvents replaced by transactionEvents",
        "requestParameters replaced by request.parameters.*",
        "parameterGroups: enable and ignore replaced by attributes true, include and exclude",
        "captureAttributes flag replaced by attributes sub-elements"
      ],
      "title": "Enable and disable attributes (.NET)",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Attributes"
      ],
      "external_id": "71d8c3a6ad5df2a487ce2fe102de9ec875f1901d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/attributes/enable-disable-attributes-net/",
      "published_at": "2021-09-20T19:34:54Z",
      "updated_at": "2021-09-20T19:34:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This describes the rules New Relic uses to determine which attributes to include or exclude for a destination. This also includes a summary of the .NET agent properties that were no longer available with the release of New Relic agent attributes in versions 9.0 or higher. Attribute rules New Relic follows these rules to determine which attributes to include or exclude: Root level takes precedence for enabled. The attributes.enabled field overrides all other settings. When false, no attributes will be reported to New Relic. Example configuration: <attributes enabled=\"false\"> <include>foo</include> <include>bar</include> </attributes> <transactionTracer enabled=\"true\"> <attributes enabled=\"true\"/> </transactionTracer> Copy Example output: Keys passed in: foo, bar, bat Keys included for all destinations: Keys excluded for all destinations: foo, bar, bat Copy Destination enabled takes precedence over include and exclude. The {destination}.attributes.enabled flags take precedence over include and exclude keys. Example configuration: <attributes enabled=\"true\"> <include>one</include> <include>two</include> </attributes> <transactionTracer enabled=\"true\"> <attributes enabled=\"false\"> <include>three</include> <include>four</include> </attributes> </transactionTracer> Copy Example output: Keys passed in: one, two, three, four Keys included for transaction traces: Keys excluded for transaction traces: one, two, three, four Copy Attribute is included if the destination is enabled. If a destination is enabled, all user attributes are sent to that destination by default. All user attributes default to true. However, by default, request attributes and message parameters are disabled for all destinations. Example configuration: <attributes enabled=\"true\"> <exclude>myAttKey</exclude> </attributes> Copy Example output: Keys passed in: foo, bar, myAttKey Keys included: foo, bar Keys excluded: myAttKey Copy Exclude always supersedes include. If the same key is listed in the include and exclude lists, then attributes with the specified key will be excluded. Example configuration: <attributes enabled=\"true\"> <include>foo</include> <include>myCustomAtt</include> <exclude>password</exclude> <exclude>myCustomAtt</exclude> </attributes> Copy Example output: Keys passed in: foo, myCustomAtt, password Keys included: foo Keys excluded: password, myCustomAtt Copy Keys are case sensitive. Keys are case sensitive. Example configuration: <attributes enabled=\"true\"> <exclude>password</exclude> <exclude>PaSsWoRd</exclude> </attributes> Copy Example output: Keys passed in: password, Password, PASSWORD, PaSsWoRd, PassWORD Keys included: Password, PASSWORD, PassWORD Keys excluded: password, PaSsWoRd Copy Use an asterisk * for wildcards. You can use an asterisk * at the end of a key as a wildcard. This will match all attributes with the same prefix. Example configuration: <attributes enabled=\"true\"> <include>custom*</include> <exclude>request.parameters.*</exclude> </attributes> Copy Example output: Keys passed in: custom, custom.key1, custom.key2, request.parameters., request.parameters.foo, request.parameters.bar Keys included: custom, custom.key1, custom.key2 Keys excluded: request.parameters., request.parameters.foo, request.parameters.bar Copy Most specific setting for a key takes priority. If multiple include or exclude attributes affect the same key, the most specific setting will have priority. Example configuration: <attributes enabled=\"true\"> <include>request.parameters.foo</include> <exclude>request.parameters.*</exclude> </attributes> Copy Example output: Keys passed in: request.parameters., request.parameters.foo, request.parameters.bar Keys included: request.parameters.foo Keys excluded: request.parameters., request.parameters.bar Copy Include or exclude affects the specific destination. If the attribute include or exclude is specified on a destination, then it only impacts that destination. Example configuration: <attributes enabled=\"true\"> <include>foo</include> </attributes> <transactionEvents enabled=\"true\"> <attributes enabled=\"true\"> <exclude>foo</exclude> </attributes> </transactionEvents> Copy Example output: Keys passed in: foo Keys included for transaction events: Keys included for other destinations: foo Keys excluded for transaction events: foo Copy Obsolete properties The following properties are not available in the .NET agent v9.0. Please visit the .NET agent 8.x to 9.x migration guide page for replacement properties when upgrading your .NET agent. analyticsEvents replaced by transactionEvents The analyticsEvents element in newrelic.config is obsolete. Enable the transactionEvents element in newrelic.config: <transactionEvents enabled=\"true\"/> Copy requestParameters replaced by request.parameters.* By default, request parameters are not sent to New Relic. Add request.parameters.* to the attributes.include list to enable request parameter collection. <attributes> <include>request.parameters.*</include> </attributes> Copy parameterGroups: enable and ignore replaced by attributes true, include and exclude The parameterGroups value and its enabled and ignore settings are obsolete. By default, customParameters and responseHeaderParameters are instrumented, while serviceRequestParameters are not instrumented. To customize these settings: Use the enabled flag to enable instrumentation. Use include and exclude to toggle instrumentation for specific attributes. For example: <attributes enabled=\"true\"> <include>service.request.*</include> <exclude>response.headers.*</exclude> <exclude>myCustomApiKey.*</exclude> </attributes> Copy For more information on configuring attributes, see the attributes examples. captureAttributes flag replaced by attributes sub-elements The capture attributes flag on browserMonitoring, transactionTracer, transactionEvents, and errorCollector is obsolete. Instead, use the attributes sub-element to configure attribute settings for each of these destinations. New property Example browserMonitoring <browserMonitoring enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </browserMonitoring> Copy transactionTracer <transactionTracer enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </transactionTracer> Copy transactionEvents <transactionEvents enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </transactionEvents> Copy errorCollector <errorCollector enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </errorCollector>> Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 26.196991,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Agents</em>",
        "body": "This describes the rules New Relic uses to determine which attributes to include or exclude for a destination. This also includes a summary of the .NET <em>agent</em> properties that were no longer available with the release of New Relic <em>agent</em> attributes in versions 9.0 or higher. Attribute rules New Relic"
      },
      "id": "603ed634196a67b796a83d9f"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.56424,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: <em>Get</em> the big picture along with the details The New Relic Explorer tab is a good place to <em>start</em> gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.55865,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " off these exporters, follow our quick <em>start</em> guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export <em>OpenTelemetry</em> data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.5549,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to New Relic. Important New Relic&#x27;s exporters for <em>OpenTelemetry</em> are now deprecated in favor of exporting data to New Relic using OTLP. Follow the <em>OpenTelemetry</em> quick <em>start</em> to help you <em>get</em> <em>started</em>. You&#x27;ll also want to review the best practices guide for getting the most out of the data you export to New Relic."
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/istio/istio-adapter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.56415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.55856,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.90776,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/kamon/kamon-reporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.21289,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.20726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.31674,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/micrometer/micrometer-metrics-registry": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.56407,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.55847,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.90771,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opencensus/opencensus-exporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-09-20T22:57:47Z",
      "updated_at": "2021-07-27T16:01:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 303.0723,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic provides <em>open</em> <em>source</em> <em>integrations</em> that report <em>telemetry</em> data from <em>telemetry</em> tools to your New Relic account. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>OpenCensus</em>, <em>OpenTelemetry</em>, DropWizard, Prometheus, and more. With these solutions, you can aggregate"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.2128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.20717,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic": [
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 322.2126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 321.00464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Important",
        "Tip",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-09-21T05:44:20Z",
      "updated_at": "2021-08-08T21:27:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. Important Our support of OTLP ingest is currently in pre-release. To particiapte in the pre-release program, please complete this form, and we'll send you details. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic: Tip If you prefer to export your data to an OpenTelemetry collector, see our instructions below. To complete this step, you'll follow the OTLP exporter documenentation for your language. Before you go to this external documentation, keep in mind you'll need to configure the OTLP exporter to send data to New Relic. This includes setting the header and configuring the endpoint: Configure the OTLP exporter to add a header ( api-key ) whose value is your Account License Key. Configure the endpoint for the exporter to point to New Relic. You'll receive the endpoint URL when you sign up for the pre-release program. Now, follow the instructions for your language: Important Make sure you are following the instructions for the OTLP gRPC exporter for your language. New Relic doesn't currently support OTLP HTTP exporters. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. The difference in this case is that instead of exporting data directly to New Relic, you export it to a collector that you set up. Then, in the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the endpoint you received when signing up for the pre-release and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.64188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Export"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts": [
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 322.2126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 321.00464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.4734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 321.00452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.47333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Important",
        "Tip",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-09-21T05:44:20Z",
      "updated_at": "2021-08-08T21:27:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. Important Our support of OTLP ingest is currently in pre-release. To particiapte in the pre-release program, please complete this form, and we'll send you details. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic: Tip If you prefer to export your data to an OpenTelemetry collector, see our instructions below. To complete this step, you'll follow the OTLP exporter documenentation for your language. Before you go to this external documentation, keep in mind you'll need to configure the OTLP exporter to send data to New Relic. This includes setting the header and configuring the endpoint: Configure the OTLP exporter to add a header ( api-key ) whose value is your Account License Key. Configure the endpoint for the exporter to point to New Relic. You'll receive the endpoint URL when you sign up for the pre-release program. Now, follow the instructions for your language: Important Make sure you are following the instructions for the OTLP gRPC exporter for your language. New Relic doesn't currently support OTLP HTTP exporters. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. The difference in this case is that instead of exporting data directly to New Relic, you export it to a collector that you set up. Then, in the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the endpoint you received when signing up for the pre-release and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.64186,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Export"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start": [
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 322.21246,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "View spans with errors",
        "View span events",
        "Transactions",
        "Databases",
        "Externals",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-09-20T19:21:14Z",
      "updated_at": "2021-09-20T19:21:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. If you need help understanding how we display your data, see the following descriptions of the left-pane options in the UI. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing In Distributed tracing, you can locate traces and examine span details: To narrow the search for traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status. These exceptions are not necessarily associated with a span error status. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 321.00452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.47333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic": [
    {
      "sections": [
        "Legacy New Relic OpenTelemetry Exporters",
        "Important",
        "New Relic Language Exporters",
        "New Relic Exporter for the OpenTelemetry Collector",
        "Endpoint configuration for New Relic exporters",
        "US endpoints",
        "EU region endpoints",
        "Tip"
      ],
      "title": "Legacy New Relic OpenTelemetry Exporters",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "0bcf5013d05814a9a0c8017b8c91a1770c7b8394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters/",
      "published_at": "2021-09-20T19:20:09Z",
      "updated_at": "2021-09-20T19:20:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic published a number of exporters that are used to send OpenTelemetry data over New Relic's proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the OpenTelemetry protocol. Important The exporters are listed here for reference. To migrate off these exporters, follow our quick start guide. New Relic Language Exporters As mentioned above, our language-specific exporters are being deprecated. These exporters allowed applications to export OpenTelemetry data directly to New Relic. The New Relic OTLP endpoint makes this strategy obsolete since it allows applications to export data to New Relic's proprietary endpoints. The exporters and their maintenance status are as follows: Go: To be archived soon Java: Archived Python: Archived .NET: To be archived soon New Relic Exporter for the OpenTelemetry Collector If you deployed your own collector with a New Relic exporter, we'll continue to support this exporter at least until the native OTLP ingest is beyond pre-release. We encourage you to migrate to the OTLP exporter in your collector. If you need help with that see our quick start guide. If you still need information about the New Relic Exporter, see the opentelemetry-collector-contrib repository. The OpenTelemetry Collector with New Relic Exporter example demonstrates what such a setup might look like. Endpoint configuration for New Relic exporters You can change the New Relic endpoints where you send your data. US endpoints By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 Logs: https://log-api.newrelic.com/log/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region endpoints To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing. Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/metric/v1 Logs: https://log-api.eu.newrelic.com/log/v1",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 322.21234,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "sections": "Legacy New Relic <em>OpenTelemetry</em> Exporters",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic published a number of exporters that are used to send <em>OpenTelemetry</em> data over New Relic&#x27;s proprietary protocol. These exporters are being deprecated in favor of sending data to New Relic using the <em>OpenTelemetry</em> protocol. Important The exporters are listed here for reference. To migrate"
      },
      "id": "60f6b9b928ccbc930b4b111c"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-20T22:58:37Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.47327,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Important",
        "Tip",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-09-21T05:44:20Z",
      "updated_at": "2021-08-08T21:27:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. Important Our support of OTLP ingest is currently in pre-release. To particiapte in the pre-release program, please complete this form, and we'll send you details. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic: Tip If you prefer to export your data to an OpenTelemetry collector, see our instructions below. To complete this step, you'll follow the OTLP exporter documenentation for your language. Before you go to this external documentation, keep in mind you'll need to configure the OTLP exporter to send data to New Relic. This includes setting the header and configuring the endpoint: Configure the OTLP exporter to add a header ( api-key ) whose value is your Account License Key. Configure the endpoint for the exporter to point to New Relic. You'll receive the endpoint URL when you sign up for the pre-release program. Now, follow the instructions for your language: Important Make sure you are following the instructions for the OTLP gRPC exporter for your language. New Relic doesn't currently support OTLP HTTP exporters. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. The difference in this case is that instead of exporting data directly to New Relic, you export it to a collector that you set up. Then, in the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the endpoint you received when signing up for the pre-release and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.64185,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Export"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent": [
    {
      "sections": [
        "Elixir open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Elixir open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "aa03e1693b6ecdd06fa2940ddb99187247743772",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent/",
      "published_at": "2021-09-21T01:18:07Z",
      "updated_at": "2021-04-27T11:09:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Elixir behavior with New Relic using the Elixir open-source agent. The agent: Helps you track transactions, distributed traces, and other parts of your application’s behavior Provides an overview of underlying BEAM activity Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Elixir Agent README on GitHub. Visit New Relic’s Elixir repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Elixir agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 358.47595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Elixir behavior with New Relic using the Elixir <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em>: Helps you track transactions, distributed traces, and other parts of your application’s behavior Provides an overview of underlying BEAM activity Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff28ccbceab351c13f"
    },
    {
      "sections": [
        ".NET agent configuration",
        "Important",
        "Configuration overview",
        "Configuration methods and precedence levels",
        "Required environment variables",
        "Caution",
        ".NET Framework environment variables",
        ".NET Core environment variables",
        "Profiler conflict explanation",
        "Optional environment variables",
        "Setup options, newrelic.config",
        "Configuration element",
        "agentEnabled",
        "maxStackTraceLines",
        "timingPrecision",
        "Service element",
        "licenseKey (required)",
        "sendEnvironmentInfo",
        "syncStartup",
        "sendDataOnExit",
        "sendDataOnExitThreshold",
        "completeTransactionsOnThread",
        "requestTimeout",
        "autoStart",
        "Obscuring key element",
        "Proxy element",
        "host",
        "port",
        "uriPath",
        "domain",
        "user",
        "password",
        "passwordObfuscated",
        "Log element",
        "level",
        "auditLog",
        "console",
        "directory",
        "fileName",
        "Application element (required)",
        "name",
        "disableSamplers",
        "Data transmission element",
        "putForDataSend",
        "Host name",
        "Set using config file",
        "Set using environment variable",
        "Cloud platform utilization",
        "detectAws",
        "detectAzure",
        "detectGcp",
        "detectPcf",
        "detectDocker",
        "detectKubernetes",
        "Instrumentation options",
        "Instrumentation element",
        "Applications element (instrumentation)",
        "Attributes element",
        "enabled",
        "include",
        "exclude",
        "Feature options",
        "App pools",
        "defaultBehavior",
        "applicationPool",
        "Cross application traces",
        "Error collection",
        "Tip",
        "captureEvents",
        "maxEventSamplesStored",
        "ignoreClasses",
        "ignoreMessages",
        "ignoreErrors (obsolete)",
        "ignoreStatusCodes",
        "expectedClasses",
        "expectedMessages",
        "expectedStatusCodes",
        "attributes",
        "High security mode",
        "Strip exception messages",
        "Transaction events",
        "maximumSamplesStored",
        "Custom events",
        "Custom parameters",
        "Labels (tags)",
        "Browser instrumentation",
        "autoInstrument",
        "requestPathsExcluded",
        "Slow queries",
        "Transaction traces",
        "transactionThreshold",
        "recordSql",
        "explainEnabled",
        "explainThreshold",
        "maxSegments",
        "maxExplainPlans",
        "maxStackTrace",
        "Datastore tracer",
        "instanceReporting",
        "databaseNameReporting",
        "queryParameters",
        "Distributed tracing",
        "excludeNewrelicHeader",
        "Disable span events via config file",
        "Disable span events via environment variable",
        "Infinite Tracing",
        "trace_observer",
        "Span events",
        "Capture HTTP Request Headers",
        "Settings in app.config or web.config",
        "Enable and disable the agent",
        "Application name",
        "License key",
        "Change newrelic.config location",
        "Settings in appsettings.json"
      ],
      "title": ".NET agent configuration",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Configuration"
      ],
      "external_id": "b89fa7fc399f2729bfee8f5106e777798a73177a",
      "image": "https://docs.newrelic.com/static/cffd7eb2d22c8e338531c38f35208c7c/c1b63/net-agent-config-settings-precedence_0.png",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/configuration/net-agent-configuration/",
      "published_at": "2021-09-20T19:41:39Z",
      "updated_at": "2021-09-20T19:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains the configuration options for the APM .NET agent. Important As of September 2021, a small subset of APIs, configuration options, and installation options for .NET will be replaced by new methods. For more details, including how you can easily prepare for this transition, see our Explorers Hub post. Configuration overview APM agent configuration options allow you to control some aspects of how the agent behaves. Some of these config options are part of the basic install process (like setting your license key and app name), but most are more advanced settings, such as: setting a log level, setting up proxy host access, excluding certain attributes, and enabling distributed tracing. The .NET agent gets its configuration from the newrelic.config file, which is generated as part of the install process. By default, only a global newrelic.config file is created, but you can also create app-local newrelic.config files for finer control over a multi-app system. Other ways to set config options include: using environment variables, or setting server-side configuration from the UI. For more on the various config options and what overrides what, see Config settings precedence. Support for both .NET Framework and .NET Core use the same configuration options and have the same APM features, unless otherwise stated. If you make changes to the config file and want to validate that it's in the right format, you can check it against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET Agent\\newrelic.xsd for Windows) with any XSD validator. Important For IIS: after you change your newrelic.config or app.config file, perform an IISRESET from an administrative command prompt. Log level adjustments do not require a reset. Configuration methods and precedence levels Upon installation, the .NET agent's configuration file (newrelic.config) applies to all monitored applications, but you can configure the agent in other ways. Here's a diagram showing how different configuration options take precedence over one another: This diagram explains the order of precedence for different ways you might configure the .NET agent. Here are details about the configuration methods shown in the diagram, and their precedence levels: .NET configuration Details and precedence web.config or app.config or appsettings.json Configuration settings set in these files take highest precedence. However, if the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled settings in these files will be ignored. Environment variables Second-highest precedence. For more about these, see .NET environment variables. Server-side configuration Third-highest precedence. A limited number of server-side configuration settings are available; the other settings will come from other configuration sources. App-local newrelic.config Fourth-highest precedence. You can create app-local newrelic.config files to configure individual apps on a multi-app system. These local configuration files override settings in the global newrelic.config file. The agent looks for app-local config files in the following directories, in this order: A directory specified in your web.config or app.config file with the NewRelic.ConfigFile property The web app's root directory (with the app.config or web.config) The directory containing your app's executable file Note that the app-local config file must be complete and validate against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET Agent\\newrelic.xsd for Windows). Default (global) newrelic.config Default source and the lowest precedence. Will configure all applications on a host in the absence of other config files. The global config file is located in the New Relic agent home directory: %PROGRAMDATA%\\New Relic\\.NET Agent Required environment variables New Relic's .NET agent relies on environment variables to tell the .NET Common Language Runtime (CLR) to attach New Relic to your processes. Some .NET agent install procedures (like the MSI installer) will automatically set these variables for you; some procedures will require you to manually set them. Caution Security recommendation: You should consider what users can set system environment variables. You should also secure the accounts under which your applications execute to prevent user environment variables overriding system environment variables .NET Framework environment variables For .NET Framework, the following variables are required: COR_ENABLE_PROFILING=1 COR_PROFILER={71DA0A04-7777-4EC6-9643-7D28B46A8A41} NEWRELIC_INSTALL_PATH=path\\to\\agent\\directory Copy The .NET agent installer will add these to IIS or as system-wide environment variables. .NET Core environment variables For .NET Core, the following variables are required: Linux: CORECLR_ENABLE_PROFILING=1 CORECLR_PROFILER={36032161-FFC0-4B61-B559-F6C5D41BAE5A} CORECLR_NEWRELIC_HOME=path/to/agent/directory CORECLR_PROFILER_PATH=\"${CORECLR_NEWRELIC_HOME}/libNewRelicProfiler.so\" Copy Windows: CORECLR_ENABLE_PROFILING=1 CORECLR_PROFILER={36032161-FFC0-4B61-B559-F6C5D41BAE5A} NEWRELIC_INSTALL_PATH=path\\to\\agent\\directory CORECLR_NEWRELIC_HOME=path\\to\\agent\\directory Copy The .NET agent installer will add these to IIS or as system-wide environment variables. If your system has previously used monitoring services (non-New Relic), you may have a \"profiler conflict\" when trying to install and use the New Relic agent. More details: Profiler conflict explanation New Relic’s .NET agents rely on environment variables to tell the .NET Common Language Runtime (CLR) to load New Relic into your processes. The install-related environment variables are Microsoft variables, not New Relic variables. They can be used by other .NET profilers, and only one profiler can be attached to a process at a time. For this reason, if you have used previous application monitoring products, you may have profiler conflicts. For specific install instructions, see the .NET agent install documentation. Optional environment variables Some configuration options in New Relic's .NET agent can be set via environment variables as an alternative to setting them in a config file. Below is a list of environment variables recognized by the .NET agent with example values. NEW_RELIC_LICENSE_KEY=XXXXXXXX NEW_RELIC_LOG=MyApp.log NEW_RELIC_APP_NAME=Descriptive Name MAX_TRANSACTION_SAMPLES_STORED=500 MAX_EVENT_SAMPLES_STORED=500 NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_SPAN_EVENTS_ENABLED=false NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED=2000 NEW_RELIC_LABELS=foo:bar;zip:zap NEW_RELIC_CONFIG_OBSCURING_KEY=XXXXXXXX NEW_RELIC_DISABLE_SAMPLERS=true NEWRELIC_PROFILER_LOG_DIRECTORY=path\\to\\a\\directory (not configurable via config file) NEWRELIC_LOG_DIRECTORY=path\\to\\a\\directory (Insert a directory where you want to put the agent and profiler logs. You can't set this directory for both agent and profiler logs in the configuration file.) NEWRELIC_LOG_LEVEL=off|error|warn|info|debug|finest|all Copy Setup options, newrelic.config Use these options to setup and configure your agent via the newrelic.config file. The New Relic .NET agent supports the following categories of setup options: Configuration element Service element Obscuring key element Proxy element Log element Application element (configuration) Data transmission element Host name Configuration element The root element of the configuration document is a configuration element. <configuration xmlns=\"urn:newrelic-config\" agentEnabled=\"true\" maxStackTraceLines=\"50\" timingPrecision=\"low\"> Copy The configuration element supports the following attributes: agentEnabled Type Boolean Default true Enable or disable the New Relic agent. maxStackTraceLines Type Integer Default 80 The maximum number of stack frames to trace in any stack dump. timingPrecision Type String Default low Controls the precision of the timers. High precision will provide better data, but at a lower execution speed. Possible values are high and low. Service element The first child of the configuration element is a service element. The service element configures the agent's connection to the New Relic service. <service licenseKey=\"YOUR_LICENSE_KEY\" sendEnvironmentInfo=\"true\" syncStartup=\"false\" sendDataOnExit=\"false\" sendDataOnExitThreshold=\"60000\" autoStart=\"true\"/> Copy The service element supports the following attributes: licenseKey (required) Type String Default (none) Your New Relic license key. New Relic uses the license key to match your app's data to the correct account in the UI. Set the license key via environment variable. Alternatively, set the NEW_RELIC_LICENSE_KEY environment variable in the application's environment. NEW_RELIC_LICENSE_KEY=XXXXXXXX Copy sendEnvironmentInfo Type Boolean Default true Instructs the agent to record execution environment information. Environment information includes operating system, agent version, and which assemblies are available. syncStartup Type Boolean Default false Block application startup until the agent connects to New Relic. If set to true, the first transaction may take substantially longer to complete, because it is blocked until the connection to New Relic is finished. sendDataOnExit Type Boolean Default false Block application shutdown until the agent sends all data from the latest harvest cycle. sendDataOnExitThreshold Type Integer Default 60000 Unit Milliseconds The minimum amount of time the process must run before the agent blocks it from shutting down. This setting only applies when sendDataOnExit is true. completeTransactionsOnThread Type Boolean Default false If false, the agent uses a pool thread to complete the transaction processing. If true, the agent will complete transaction processing on the request thread. requestTimeout Type Integer Default 2000 (sendDataOnExit enabled) 120000 (sendDataOnExit disabled) Unit Milliseconds The agent's request timeout when communicating with New Relic. autoStart Type Boolean Default True Automatically start the .NET agent when the first instrumented method is hit. Obscuring key element The obscuringKey element is an optional child of the service element. The .NET Agent uses this value to deobfuscate supported configuration values. For example, when an obfuscated proxy password is supplied, it will be deobfuscated using this key. <service licenseKey=\"YOUR_LICENSE_KEY\"> <obscuringKey>OBSCURING_KEY</obscuringKey> </service> Copy The obscuring key may also be configured by setting the NEW_RELIC_CONFIG_OBSCURING_KEY environment variable. Caution Security recommendation: The placement of the obscuring Key in the same configuration file as an obfuscated value may pose a security risk. Consider placing the obscuring key in an environment variable and limiting access to environment variables within your environment. Proxy element The proxy element is an optional child of the service element. The proxy element is used when the agent communicates to the New Relic back-end service via a proxy. <service licenseKey=\"YOUR_LICENSE_KEY\"> <proxy host=\"hostname\" port=\"PROXY_PORT\" uriPath=\"path/to/something.aspx\" domain=\"mydomain.com\" user=\"PROXY_USERNAME\" password=\"PROXY_PASSWORD\" passwordObfuscated=\"OBFUSCATED_PROXY_PASSWORD\"/> </service> Copy The proxy element supports the following attributes: host Type String Default (none) Defines the proxy host. port Type Integer Default 8080 Defines the proxy port. uriPath Type String Default (none) Optionally define a proxy URI path. domain Type String Default (none) Optionally define a domain to use when authenticating with the proxy server. user Type String Default (none) Optionally define a user name for authentication. password Type String Default (none) Optionally define a password for authentication. passwordObfuscated Type String Default (none) For additional security, the .NET Agent supports the use of an obfuscated proxy password with the passwordObfuscated attribute. The obfuscated proxy password is generated using the following New Relic CLI command: newrelic agent config obfuscate --key OBSCURING_KEY --value \"CLEAR_TEXT_PROXY_PASSWORD\" Copy Important When using an obfuscated proxy password, the obscuring key must also be configured. Log element The log element is a child of the configuration element. The log element configures New Relic's logging . The agent generates its own log file to keep its logging information separate from your application's logs. <log level=\"info\" auditLog=\"false\" console=\"false\" directory=\"PATH\\TO\\LOG\\DIRECTORY\" fileName=\"FILENAME.log\" /> Copy The log element supports the following attributes: level Type String Default info Defines the level of detail recorded in the log file. Possible values, in increasing order of detail, are: off error warn info debug finest all Alternatively, set the NEWRELIC_LOG_LEVEL environment variable in the application's environment. Important Increasing the log level will increase New Relic's performance impact. auditLog Type Boolean Default false Records all data sent to and received from New Relic in both an auditlog log file and the standard log file. console Type Boolean Default false Send log messages to the console, in addition to the log file. directory Type String Default C:\\ProgramData\\New Relic\\.NET Agent\\Logs The directory to hold log files generated by the agent. If this is omitted, then a directory named logs in the New Relic agent install area will be used by default. fileName Type String Default (none) Defines a name for the log file. If you do not define a fileName, the name is derived from the name of the monitored process. Alternatively, set the NEW_RELIC_LOG environment variable in the application's environment. NEW_RELIC_LOG=MyApp.log Copy Application element (required) The application element is a child of the configuration element. This required element defines your application name, and disables or enables sampling. name Type String Default My Application The name of your .NET application is a child of the application element. New Relic will aggregate your data according to this name. For example, if you have two running applications named AppA and AppB, you will see two applications in the New Relic interface: AppA and AppB. You can also assign up to three names to your app. The first name is the primary name. For example: <application> <name>MY APPLICATION PRIMARY</name> <name>SECOND APP NAME</name> <name>THIRD APP NAME</name> </application> Copy Alternatively, set the NEW_RELIC_APP_NAME environment variable in the application's environment. NEW_RELIC_APP_NAME=Descriptive Name Copy disableSamplers Type Boolean Default false Samplers collect information about memory and CPU consumption. Set this to true to disable sampling. Alternatively, set the NEW_RELIC_DISABLE_SAMPLERS environment variable in the application's environment. NEW_RELIC_DISABLE_SAMPLERS=true Copy Data transmission element The dataTransmission element is a child of the configuration element. This element affects how data is sent to New Relic and can be used if you have specific data transmission requirements. <dataTransmission putForDataSend=\"false\" compressedContentEncoding=\"deflate\"/> Copy The dataTransmission element supports the following attributes: putForDataSend Type Boolean Default false Defines the HTTP method used when sending data to New Relic. Set this to true to enable using the PUT method when sending data. The POST method is used by default. Host name If the default host name label in the APM UI is not useful, you can decorate that name in the New Relic UI with a display name. After the application process is restarted and the .NET agent is reporting again, the display name will appear in the Servers drop-down list. This host name setting does not affect the list of hosts on your application's Summary page. To set a display name, choose one of the following options. The environment variable takes precedence over the config file value. Then restart your application to see your changes in the New Relic UI. Set using config file Set the displayName attribute in the processHost element in newrelic.config. The processHost element is a child of the configuration element. <configuration . . . > <processHost displayName=\"CUSTOM_NAME\" /> </configuration> Copy Set using environment variable Set the NEW_RELIC_PROCESS_HOST_DISPLAY_NAME environment variable: NEW_RELIC_PROCESS_HOST_DISPLAY_NAME = \"CUSTOM_NAME\" Copy Cloud platform utilization Configures the utilization configuration element to control how the agent collects utilization information and sends it to the New Relic service to determine pricing. The agent can collect information from Amazon Web Services (AWS) EC2 instances, Docker containers, Azure, Google Cloud Platform, Pivotal Cloud Foundry, and Kubernetes. detectAws Type Boolean Default true Determines whether the agent polls AWS metadata API. detectAzure Type Boolean Default true Determines whether the agent polls Azure metadata API. detectGcp Type Boolean Default true Determines whether the agent polls GCP metadata API. detectPcf Type Boolean Default true Determines whether the agent polls PCF information from environment variables. detectDocker Type Boolean Default true Determines whether the agent reads Docker information from the file system. detectKubernetes Type Boolean Default true Determines whether the agent polls Kubernetes information from environment variables. Instrumentation options Use these options to configure which elements of your application and environment to instrument. New Relic for .NET supports the following categories of instrumentation options: Instrumentation element Applications element (instrumentation) Attributes element Instrumentation element The instrumentation element is a child of the configuration element. By default, the .NET agent instruments IIS asp worker processes and Azure web and worker roles. To instrument other processes, see Instrumenting custom applications. Applications element (instrumentation) The applications element is a child of the instrumentation element. The applications element specifies which non-web apps to instrument. It contains a name attribute. Important This is not the same as the application (configuration) element, which is a child of the configuration element. <instrumentation> <applications> <application name=\"MyService1.exe\" /> <application name=\"MyService2.exe\" /> <application name=\"MyService3.exe\" /> </applications> </instrumentation> Copy Attributes element An attribute is a key/value pair that determines the properties of an event or transaction. Each attribute is sent to APM transaction traces, APM error traces, Transaction events, TransactionError events, or PageView events. The primary attributes element enables or disables attribute collection for the .NET agent, and defines specific attributes to collect or exclude. You can also configure attribute settings based on their destination: Error collection, transaction traces, browser instrumentation, and transaction events. In this example, the agent excludes all attributes whose key begins with myApiKey (myApiKey.bar, myApiKey.value), but collects the custom attribute myApiKey.foo. <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> Copy You can view the .NET APM attributes on the .NET agent attributes page. You can also define custom attributes with the agent API call AddCustomAttribute. enabled Type Boolean Default true Enable or disable attribute collection. When set to false in the primary attribute element, this setting overrides all attribute settings for individual destinations. include Type String Default (none) If attributes are enabled, the agent will collect all attribute keys specified in this list. To specify multiple attribute keys, specify each individually. You can also use a * wildcard character at the end of a key to match multiple attributes (for example, myApiKey.*). For more information, see Attribute rules. exclude Type String Default (none) If attributes are enabled, the agent will not collect attribute keys specified in this list. To specify multiple attribute keys, specify each individually. You can also use a * wildcard character at the end of a key to match multiple attributes (for example, myApiKey.*). For more information, see Attribute rules. Feature options Use these options to enable, disable, and configure New Relic features. New Relic for .NET allows you to configure the following features: App pools Cross application traces Error collection High security mode Strip exception messages Transaction events Custom events Custom parameters Tags/labels Browser instrumentation Slow Queries Transaction traces Datastore tracer Distributed tracing Span events Capture HTTP Request Headers App pools Important This is only applicable to a system's global config file. The applicationPools element is a child of the configuration element. The applicationPools element specifies for the profiler exactly which application pools to instrument and uses the same name as the IIS application pool name. This configuration element is useful when you may need to instrument only a small subset of your app pools. For example, a given server might have several hundred application pools, but only a few of those pools need to be instrumented by the .NET agent. Here is an example of disabling instrumentation for specific application pools: <applicationPools> <applicationPool name=\"Foo\" instrument=\"false\"/> <applicationPool name=\"Bar\" instrument=\"false\"/> </applicationPools> Copy Here is an example of disabling instrumentation for all application pools currently executing on the server and enabling instrumentation for specific application pools: <applicationPools> <defaultBehavior instrument=\"false\"/> <applicationPool name=\"Foo\" instrument=\"true\"/> <applicationPool name=\"Bar\" instrument=\"true\"/> </applicationPools> Copy The applicationPools element supports the following elements: defaultBehavior Type Boolean Default false Defines how the .NET agent will behave on a \"global\" level for application pools served via IIS. The .NET agent instruments all application pools by default. When true, application pools listed under applicationPool with an instrument attribute set to false will not be instrumented. Essentially, when set to false, the application pool list acts as an allow list. When set to true, the application pool list acts as a deny list. applicationPool Defines instrumentation behavior for a specific application pool. The name attribute is the name of an application pool. Enable or disable profiling in the instrument attribute. Define this application in the name attribute. Cross application traces The crossApplicationTracer element is a child of the configuration element. crossApplicationTracer links transaction traces across applications. When linked in a service-oriented architecture, all instrumented applications that communicate with each other via HTTP will now \"link\" transaction traces with the applications that they call and the applications they are called by. Cross application tracing makes it easier to understand the performance relationship between services and applications. <crossApplicationTracer enabled=\"true\"/> Copy The crossApplicationTracer element supports the following attribute: enabled Type Boolean Default true Enable or disable cross application tracing Error collection The errorCollector element is a child of the configuration element. errorCollector configures error collection, which captures information about uncaught exceptions and sends them to New Relic. <errorCollector enabled=\"true\" captureEvents=\"true\" maxEventSamplesStored=\"100\"> <ignoreClasses> <errorClass>System.IO.FileNotFoundException</errorClass> <errorClass>System.Threading.ThreadAbortException</errorClass> </ignoreClasses> <ignoreMessages> <errorClass name=\"System.Exception\"> <message>Ignore message</message> <message>Ignore too</message> </errorClass> </ignoreMessages> <ignoreStatusCodes> <code>401</code> <code>404</code> </ignoreStatusCodes> <expectedClasses> <errorClass>System.ArgumentNullException</errorClass> <errorClass>System.ArgumentOutOfRangeException</errorClass> </expectedClasses> <expectedMessages> <errorClass name=\"System.Exception\"> <message>Expected message</message> <message>Expected too</message> </errorClass> </expectedMessages> <expectedStatusCodes>403,500-505</expectedStatusCodes> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </errorCollector> Copy Tip For an overview of error configuration in APM, see Manage errors in APM. Important expectedClasses, expectedMessages, and expectedStatusCodes configuration settings require .NET agent version 8.31.0.0 or higher. The errorCollector element supports the following elements and attributes: enabled Type Boolean Default true Enable or disable the error collector. captureEvents Type Boolean Default true Enable or disable the capturing of error events. maxEventSamplesStored Type Integer Default 100 Reservoir limit for error events. ignoreClasses A list of fully qualified class names to be ignored. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used ignoreMessages An optional map of fully qualified class names to list of strings matching a substring of the message of an error. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used ignoreErrors (obsolete) Type String Default (none) Lists specific exceptions to not report to New Relic. The full name of the exception should be used, such as System.IO.FileNotFoundException. ignoreStatusCodes Type String Default (none) Lists specific HTTP error codes to not report to New Relic. You can use standard integral HTTP error codes, such as just 401, or you may use Microsoft full status codes with decimal points, such as 401.4 or 403.18. The status codes should be equal to or greater than 400. expectedClasses A list of fully qualified class names to be marked as expected. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used expectedMessages An optional map of fully qualified class names to list of strings matching a substring of the message of an error. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used expectedStatusCodes A comma separated list of status codes. The list may include integer ranges, using a single dash (-) and will be inclusive of both the starting and ending integer in the range. attributes Use this sub-element to customize your agent attribute settings for error traces. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. High security mode The highSecurity element is a child of the configuration element. To enable high security mode, set this property to true and enable the high security property in the New Relic user interface. Enabling high security turns SSL on; request parameters, custom parameters and HTTP request headers are not collected; strip exception messages is enabled; and queries can't be sent to New Relic in their raw form. enabled Type Boolean Default false Enable or disable high security mode. Example: <highSecurity enabled=\"true\"/> Copy Strip exception messages The stripExceptionMessages element is a child of the configuration element. To enable strip exception messages, set this property to true. By default, this is set to false, which means that the agent sends messages from all exceptions to the New Relic collector. If you enable high security mode, this is automatically changed to true, and the agent strips the messages from exceptions. enabled Type Boolean Default false Enable or disable strip exception messages. Example: <stripExceptionMessages enabled=\"true\"/> Copy Transaction events The transactionEvents element is a child of the configuration element. Use transactionEvents to configure transaction events. <transactionEvents enabled=\"true\" maximumSamplesStored=\"10000\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </transactionEvents> Copy The transactionEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Integer Default 10000 The maximum number of samples to store in memory at once. Alternatively, set the MAX_TRANSACTION_SAMPLES_STORED environment variable in the application's environment. MAX_TRANSACTION_SAMPLES_STORED=500 Copy attributes Use this sub-element to customize your agent attribute settings for transaction events. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Tip These attribute settings are specific to transaction events. Attribute settings can be applied globally to all event types to with this configuration setting. Caution When distributed tracing and/or Infinite Tracing are enabled, information from transaction events is applied to the root Span Event of the transaction. Consider applying any attribute settings for transaction events to span events and/or apply them as Global Attribute settings. Custom events The customEvents element is a child of the configuration element. Use customEvents to configure custom events. <customEvents enabled=\"true\" maximumSamplesStored=\"10000\"/> Copy The CustomEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Integer Default 10000 The maximum number of samples to store in memory at once. Alternatively, set the MAX_EVENT_SAMPLES_STORED environment variable in the application's environment. MAX_EVENT_SAMPLES_STORED=500 Copy Custom parameters The customParameters element is a child of the configuration element. Use customParameters to configure custom parameters. <customParameters enabled=\"true\" /> Copy The CustomParameters element supports the following attributes: enabled Type Boolean Default true Enable or disable the capture of custom parameters. Labels (tags) The labels element is a child of the configuration element. This sets tag names and values. The list is a semicolon delimited list of colon-separated name and value pairs. You can also use with the NEW_RELIC_LABELS environment variable. Example: <labels>foo:bar;zip:zap</labels> Copy Browser instrumentation The browserMonitoring element is a child of the configuration element. browserMonitoring configures browser monitoring in your .NET application. Browser gives you insight your end users' performance experience. This is accomplished by measuring the time it takes for your users' browsers to download and render your webpages by injecting a small amount of JavaScript code into the header and footer of each page. // If you use both the Exclude and Attribute elements // the Exclude element must be listed first. <browserMonitoring autoInstrument=\"true\"> <requestPathsExcluded> <path regex=\"url-regex-1\"/> <path regex=\"url-regex-2\"/> ... <path regex=\"url-regex-n\"/> </requestPathsExcluded> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </browserMonitoring> Copy The browserMonitoring element supports the following attributes: autoInstrument Type Boolean Default true By default the agent automatically injects the browser agent JavaScript. To turn off automatic injection, set this attribute to false. attributes Use this sub-element to customize your agent attribute settings for browser monitoring. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. requestPathsExcluded Use this sub-element to prevent the browser agent from being injected in specific pages. The element is used as follows: <requestPathsExcluded> <path regex=\"url-regex-1\"/> <path regex=\"url-regex-2\"/> ... <path regex=\"url-regex-n\"/> </requestPathsExcluded> Copy The agent will not inject the browser agent into pages whose URL matches one of the specified regular expressions. The regular expression should follow Microsoft guidelines for the Regex class. It is a reference to the virtual directory of the path in your application and not the full URL of the path you wish to exclude. For example, to exclude the pages in https://www.mywebsite.com/mywebpages/ you would simply insert /mywebpages/ as the path regex value. The requestPathsExcluded element should be used in cases where it is impossible or undesirable to use the DisableBrowserMonitoring() call. To minimize a possible performance impact try to use as few regular expressions as possible and keep them as simple as possible. Slow queries The slowSql element is a child of the configuration element. slowSql configures capturing information about slow query executions, and captures and obfuscates explain plans for these queries. <slowSql enabled=\"true\"/> Copy The slowSql element supports the following attribute: enabled Type Boolean Default true Enable or disable slow query tracing. Transaction traces The transactionTracer element is a child of the configuration element. transactionTracer configures transaction traces. Included in the trace is the exact call sequence of the transactions, including any query statements issued. <transactionTracer enabled=\"true\" transactionThreshold=\"apdex_f\" recordSql=\"obfuscated\" explainEnabled=\"true\" explainThreshold=\"500\" maxSegments=\"3000\" maxExplainPlans=\"20\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </transactionTracer> Copy The transactionTracer element supports the following attributes: enabled Type Boolean Default true Enable or disable transaction traces. transactionThreshold Type String Default apdex_f Defines the threshold for transaction traces. If a transaction takes longer than the threshold, it is eligible for being traced. See transaction trace basics for more about the rules governing traces. The default value is apdex_f, which sets the threshold to four times the application's apdex_t value. For more information about apdex_t, see Apdex. You can also set the threshold to be a specific time value in milliseconds. recordSql Type String Default obfuscated Select a query tracing policy. Options are off, which records nothing; obfuscated, which records an obfuscated version of the query; or raw, which records the query exactly as it is issued to the database. Caution Recording raw queries may capture sensitive information. explainEnabled Type Boolean Default false When true, the agent captures EXPLAIN statements for slow queries. explainThreshold Type Integer Default 500 Unit Milliseconds The agent collects slow query data for queries that exceed this threshold, along with any available explain plans, as part of transaction traces. maxSegments Type Integer Default 3000 The maximum number of segments to collect in a transaction trace. maxExplainPlans Type Integer Default 20 The maximum number of explain plans to collect during a harvest cycle. maxStackTrace Type Integer Default 0 By default maxStackTrace is set to 0, which disables stack traces as part of a transaction trace. If this value is set greater than 0, then stack traces will be captured for transaction traces. attributes Use this sub-element to customize your agent attribute settings for transaction traces. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Datastore tracer The datastoreTracer element is a child of the configuration element. <datastoreTracer> <instanceReporting enabled=\"true\" /> <databaseNameReporting enabled=\"true\" /> <queryParameters enabled=\"false\" /> </datastoreTracer> Copy The datastoreTracer element supports the following sub-elements: instanceReporting Use this sub-element to enable collection of datastore instance metrics (such as the host and port) for some database drivers. These are reported on slow query traces and transaction traces. The default value of attribute enabled is true. databaseNameReporting Use this sub-element to enable collection of the database name on slow query traces and transaction traces for some database drivers. The default value of attribute enabled is true. queryParameters Use this sub-element to enable collection of the SQL query parameters on slow query traces. The default value of attribute enabled is false. Caution Recording query parameters may capture sensitive information. The transactionTracer.recordSql configuration option must be set to raw or this option is ignored. Distributed tracing The distributedTracing element is a child of the configuration element. <distributedTracing enabled=\"false\" excludeNewrelicHeader=\"false\"/> Copy Distributed tracing lets you see the path that a request takes as it travels through a distributed system. Enabling distributed tracing disables cross application tracing, and has other effects on APM features. Before enabling, read the planning guide. Important Requires .NET agent version 8.6.45.0 or higher. The distributedTracing element supports the following attributes: To enable or disable, see Enable distributed tracing. enabled Type Boolean Default false Alternatively, enable distributed tracing via the NEW_RELIC_DISTRIBUTED_TRACING_ENABLED environment variable in the application's environment. NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy excludeNewrelicHeader Type Boolean Default false By default, supported versions of the agent utilize both the newrelic header and W3C Trace Context headers for distributed tracing. The newrelic distributed tracing header allows interoperability with older agents that don't support W3C Trace Context headers. Agent versions that support W3C Trace Context headers will prioritize them over newrelic headers for distributed tracing. If you do not want to utilize the newrelic header, setting this to true will result in the agent excluding the newrelic header and only using W3C Trace Context headers for distributed tracing. Distributed tracing reports span events. Span event reporting is enabled by default, but distributed tracing must be enabled for spans to be reported. To disable span events, choose one of the following options: Disable span events via config file Set the <spanEvents> element to false to disable via the newrelic.config file. This element is a child of the <configuration> element. <configuration . . . > <spanEvents enabled=\"false\" /> </configuration> Copy Disable span events via environment variable Set the NEW_RELIC_SPAN_EVENTS_ENABLED environment variable in the application's environment. NEW_RELIC_SPAN_EVENTS_ENABLED=false Copy Infinite Tracing Infinite Tracing extends the distributed tracing service by employing a trace observer that is external to the agent. It observes 100% of your application traces across various services and provides actionable data so you can solve issues faster. Important Infinite Tracing requires .NET Agent version 8.30 or higher. To turn on Infinite Tracing, enable distributed tracing and add the additional settings below <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy The infiniteTracing element supports the following elements: trace_observer The trace_observer element identifies an observer host that is independent from the agent. For help getting a valid Infinite Tracing trace observer host entry, see Find or create a trace observer endpoint. The trace observer may be configured using the NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST environment variable as well. Important When configuring the trace observer, you should not supply the protocol as part of the host. For example, use myhost.infinitetracing.com instead of https://myhost.infinitetracing.com. Span events The spanEvents element is a child of the configuration element. Use spanEvents to configure span events. <spanEvents enabled=\"true\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </spanEvents> Copy The spanEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Int Default 2000 The maximum number of samples to store in memory at a time. This may be configured using the NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED environment variable as well. Important This configuration option is only available in the .NET Agent v9.0 or higher. attributes Use this sub-element to customize your agent attribute settings for span events. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Tip These attribute settings are specific to span events. Attribute settings may be applied globally to all event types to with this configuration setting. Capture HTTP Request Headers The allowAllHeaders element is a child of the configuration element. Set this to true to allow the .NET Agent to capture all HTTP request headers as request.headers.{http-header-name} attributes. Set this to false to only allow the .NET agent to collect the following HTTP request headers: request.headers.referer request.headers.accept request.headers.content-length request.headers.host request.headers.user-agent Copy enabled Type Boolean Default false Enable or disable HTTP request headers capture. Example: <allowAllHeaders enabled=\"true\" /> <attributes enabled=\"true\"> <include>request.headers.*</include> </attributes> Copy Important The allowAllHeaders setting is only available in the .NET Agent version 8.40.0+. When using allowAllHeaders to capture attributes, the captured request header attributes are still being controlled by the root level and destination level attributes settings. Without setting the request.header.* in the include list under the attributes element (see the following), the .NET Agent still filters out all header attributes. The default newrelic.config is set to include the request.header.*. <allowAllHeaders enabled=\"true\" /> <attributes enabled=\"true\"> <include>request.headers.*</include> ... </attributes> Copy The default newrelic.config is also set to explicitly exclude the following HTTP request headers to prevent the .NET Agent collecting unwanted data. <attributes enabled=\"true\"> <exclude>request.headers.cookie</exclude> <exclude>request.headers.authorization</exclude> <exclude>request.headers.proxy-authorization</exclude> <exclude>request.headers.x-*</exclude> </attributes> Copy Settings in app.config or web.config For ASP.NET and .NET Framework console apps you can also configure the following settings in your app's app.config or web.config, within the outermost element, <configuration>: Enable and disable the agent <appSettings> <add key = \"NewRelic.AgentEnabled\" value=\"false\" /> </appSettings> Copy Important If the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled settings in these files will be ignored. Application name For more information, see Name your .NET application. <appSettings> <add key = \"NewRelic.AppName\" value =\"Descriptive Name\" /> </appSettings> Copy License key <appSettings> <add key = \"NewRelic.LicenseKey\" value =\"XXXXXXXX\" /> </appSettings> Copy Change newrelic.config location Designates an alternative location for the config file outside of the local root of the app or global config location. The location entered must be an absolute path. <appSettings> <add key = \"NewRelic.ConfigFile\" value=\"C:\\Path-to-alternate-config-dir\\newrelic.config\" /> </appSettings> Copy Settings in appsettings.json For .NET Core apps, you can configure the following settings in appsettings.json if the following is true: The appsettings.json file must be located in the current working directory of the application. The application must have the following dependencies: Microsoft.Extensions.Configuration Microsoft.Extensions.Configuration.Json Microsoft.Extensions.Configuration.EnvironmentVariables Enable and disable the agent { \"NewRelic.AgentEnabled\":\"false\" } Copy Important If the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled setting in this file will be ignored. Application name For more information, see Name your .NET application. { \"NewRelic.AppName\": \"Descriptive Name\" } Copy License key { \"NewRelic.LicenseKey\": \"XXXXXXXX\" } Copy Change newrelic.config location Designates an alternative location for the config file outside of the local root of the app or global config location. The location entered must be an absolute path. { \"NewRelic.ConfigFile\": \"C:\\\\Path-to-alternate-config-dir\\\\newrelic.config\" } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 26.201015,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET <em>agent</em> configuration",
        "sections": ".NET <em>agent</em> configuration",
        "tags": "<em>Agents</em>",
        "body": " (with the app.config or web.config) The directory containing your app&#x27;s executable file Note that the app-local config file must be complete and validate against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET <em>Agent</em>\\newrelic.xsd for Windows). Default (global) newrelic.config Default <em>source</em>"
      },
      "id": "60446c3b196a679d6a960f7a"
    },
    {
      "sections": [
        "Enable and disable attributes (.NET)",
        "Attribute rules",
        "Root level takes precedence for enabled.",
        "Destination enabled takes precedence over include and exclude.",
        "Attribute is included if the destination is enabled.",
        "Exclude always supersedes include.",
        "Keys are case sensitive.",
        "Use an asterisk * for wildcards.",
        "Most specific setting for a key takes priority.",
        "Include or exclude affects the specific destination.",
        "Obsolete properties",
        "analyticsEvents replaced by transactionEvents",
        "requestParameters replaced by request.parameters.*",
        "parameterGroups: enable and ignore replaced by attributes true, include and exclude",
        "captureAttributes flag replaced by attributes sub-elements"
      ],
      "title": "Enable and disable attributes (.NET)",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Attributes"
      ],
      "external_id": "71d8c3a6ad5df2a487ce2fe102de9ec875f1901d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/attributes/enable-disable-attributes-net/",
      "published_at": "2021-09-20T19:34:54Z",
      "updated_at": "2021-09-20T19:34:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This describes the rules New Relic uses to determine which attributes to include or exclude for a destination. This also includes a summary of the .NET agent properties that were no longer available with the release of New Relic agent attributes in versions 9.0 or higher. Attribute rules New Relic follows these rules to determine which attributes to include or exclude: Root level takes precedence for enabled. The attributes.enabled field overrides all other settings. When false, no attributes will be reported to New Relic. Example configuration: <attributes enabled=\"false\"> <include>foo</include> <include>bar</include> </attributes> <transactionTracer enabled=\"true\"> <attributes enabled=\"true\"/> </transactionTracer> Copy Example output: Keys passed in: foo, bar, bat Keys included for all destinations: Keys excluded for all destinations: foo, bar, bat Copy Destination enabled takes precedence over include and exclude. The {destination}.attributes.enabled flags take precedence over include and exclude keys. Example configuration: <attributes enabled=\"true\"> <include>one</include> <include>two</include> </attributes> <transactionTracer enabled=\"true\"> <attributes enabled=\"false\"> <include>three</include> <include>four</include> </attributes> </transactionTracer> Copy Example output: Keys passed in: one, two, three, four Keys included for transaction traces: Keys excluded for transaction traces: one, two, three, four Copy Attribute is included if the destination is enabled. If a destination is enabled, all user attributes are sent to that destination by default. All user attributes default to true. However, by default, request attributes and message parameters are disabled for all destinations. Example configuration: <attributes enabled=\"true\"> <exclude>myAttKey</exclude> </attributes> Copy Example output: Keys passed in: foo, bar, myAttKey Keys included: foo, bar Keys excluded: myAttKey Copy Exclude always supersedes include. If the same key is listed in the include and exclude lists, then attributes with the specified key will be excluded. Example configuration: <attributes enabled=\"true\"> <include>foo</include> <include>myCustomAtt</include> <exclude>password</exclude> <exclude>myCustomAtt</exclude> </attributes> Copy Example output: Keys passed in: foo, myCustomAtt, password Keys included: foo Keys excluded: password, myCustomAtt Copy Keys are case sensitive. Keys are case sensitive. Example configuration: <attributes enabled=\"true\"> <exclude>password</exclude> <exclude>PaSsWoRd</exclude> </attributes> Copy Example output: Keys passed in: password, Password, PASSWORD, PaSsWoRd, PassWORD Keys included: Password, PASSWORD, PassWORD Keys excluded: password, PaSsWoRd Copy Use an asterisk * for wildcards. You can use an asterisk * at the end of a key as a wildcard. This will match all attributes with the same prefix. Example configuration: <attributes enabled=\"true\"> <include>custom*</include> <exclude>request.parameters.*</exclude> </attributes> Copy Example output: Keys passed in: custom, custom.key1, custom.key2, request.parameters., request.parameters.foo, request.parameters.bar Keys included: custom, custom.key1, custom.key2 Keys excluded: request.parameters., request.parameters.foo, request.parameters.bar Copy Most specific setting for a key takes priority. If multiple include or exclude attributes affect the same key, the most specific setting will have priority. Example configuration: <attributes enabled=\"true\"> <include>request.parameters.foo</include> <exclude>request.parameters.*</exclude> </attributes> Copy Example output: Keys passed in: request.parameters., request.parameters.foo, request.parameters.bar Keys included: request.parameters.foo Keys excluded: request.parameters., request.parameters.bar Copy Include or exclude affects the specific destination. If the attribute include or exclude is specified on a destination, then it only impacts that destination. Example configuration: <attributes enabled=\"true\"> <include>foo</include> </attributes> <transactionEvents enabled=\"true\"> <attributes enabled=\"true\"> <exclude>foo</exclude> </attributes> </transactionEvents> Copy Example output: Keys passed in: foo Keys included for transaction events: Keys included for other destinations: foo Keys excluded for transaction events: foo Copy Obsolete properties The following properties are not available in the .NET agent v9.0. Please visit the .NET agent 8.x to 9.x migration guide page for replacement properties when upgrading your .NET agent. analyticsEvents replaced by transactionEvents The analyticsEvents element in newrelic.config is obsolete. Enable the transactionEvents element in newrelic.config: <transactionEvents enabled=\"true\"/> Copy requestParameters replaced by request.parameters.* By default, request parameters are not sent to New Relic. Add request.parameters.* to the attributes.include list to enable request parameter collection. <attributes> <include>request.parameters.*</include> </attributes> Copy parameterGroups: enable and ignore replaced by attributes true, include and exclude The parameterGroups value and its enabled and ignore settings are obsolete. By default, customParameters and responseHeaderParameters are instrumented, while serviceRequestParameters are not instrumented. To customize these settings: Use the enabled flag to enable instrumentation. Use include and exclude to toggle instrumentation for specific attributes. For example: <attributes enabled=\"true\"> <include>service.request.*</include> <exclude>response.headers.*</exclude> <exclude>myCustomApiKey.*</exclude> </attributes> Copy For more information on configuring attributes, see the attributes examples. captureAttributes flag replaced by attributes sub-elements The capture attributes flag on browserMonitoring, transactionTracer, transactionEvents, and errorCollector is obsolete. Instead, use the attributes sub-element to configure attribute settings for each of these destinations. New property Example browserMonitoring <browserMonitoring enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </browserMonitoring> Copy transactionTracer <transactionTracer enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </transactionTracer> Copy transactionEvents <transactionEvents enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </transactionEvents> Copy errorCollector <errorCollector enabled=\"true\"> <attributes enabled=\"false\"> <include>myKey.*</include> <exclude>myKey.foo</exclude> </attributes> </errorCollector>> Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 26.196941,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Agents</em>",
        "body": "This describes the rules New Relic uses to determine which attributes to include or exclude for a destination. This also includes a summary of the .NET <em>agent</em> properties that were no longer available with the release of New Relic <em>agent</em> attributes in versions 9.0 or higher. Attribute rules New Relic"
      },
      "id": "603ed634196a67b796a83d9f"
    }
  ],
  "/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic": [
    {
      "sections": [
        "Introduction to AWS integrations",
        "Connect AWS and New Relic",
        "Integrations and AWS costs",
        "View your AWS data",
        "Region availability"
      ],
      "title": "Introduction to AWS integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "26a36d0da0ba98b48ccaff2e574ec4e535e68844",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/introduction-aws-integrations/",
      "published_at": "2021-09-20T19:30:51Z",
      "updated_at": "2021-09-20T19:30:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon integrations let you monitor your AWS data in several New Relic features. Enabling the AWS CloudWatch Metric Streams integration is the recommended solution to monitor all CloudWatch metrics from all AWS services (including custom namespaces). On top of this, additional integrations are available to get extended visibility on key AWS services beyond the available CloudWatch metrics. For a full reference of the supported metrics, please check the available CloudWatch metrics for each service in the AWS documentation pages. Connect AWS and New Relic In order to obtain AWS data, follow the procedure to connect AWS to New Relic. Additional API Polling integrations can be enabled on top of the AWS CloudWatch metric streams in order to pull data that's not available as CloudWatch metrics. The following integrations are not replaced by the metric streams: AWS Billing AWS CloudTrail AWS Health AWS Trusted Advisor AWS VPC Finally, other integrations may require additional configurations in your AWS account: AWS VPC Flow Logs AWS CloudFormation Integrations and AWS costs Keep in mind the following items: AWS CloudWatch metric streams pricing is defined based on the number of metric updates. For up-to-date pricing information check AWS CloudWatch Pricing. AWS Kinesis Data Firehose is used as the delivery method. For details, see the AWS Firehose pricing page. AWS Config can be optionally enabled in your AWS account, and used to enrich CloudWatch metrics with custom tags and resource metadata. With AWS Config, you are charged based on the number of configuration items recorded. See the AWS Config pricing page for details. If polling integrations are enabled (instead of metric streams), New Relic uses the Amazon CloudWatch API to obtain metrics from the AWS services you monitor. The number of calls to the CloudWatch API increases as you enable more integrations. Add AWS resources to those integrations, or scale those integrations across more regions. This can cause requests to the CloudWatch API to exceed the 1 million free limits granted by AWS and increase your CloudWatch bill. AWS offers enhanced monitoring for some of their services which allows for more metrics, more often. For example, see RDS enhanced monitoring costs. View your AWS data Once you follow the configuration process, data from your Amazon Web Services report directly to New Relic. AWS entities for most used services will be listed in the New Relic Explorer. Metrics and events will appear in the Data Explorer. AWS data will also be visible in the Infrastructure UI. To view your AWS data: Go to one.newrelic.com > Infrastructure > AWS. For any of the AWS integrations listed: For active streams, select the Explore your data link. OR For other integrations, browse the available dashboard or click on the Explore Data link. You can view and reuse NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Region availability Most AWS services offer regional endpoints to reduce data latency between cloud resources and applications. New Relic can obtain monitoring data from services and endpoints that are located in all AWS regions, except China.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.33496,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to AWS <em>integrations</em>",
        "sections": "Introduction to AWS <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "Amazon <em>integrations</em> let you monitor your AWS data in several New Relic features. Enabling the AWS CloudWatch Metric Streams integration is the recommended solution to monitor all CloudWatch metrics from all AWS services (including custom namespaces). On top of this, additional <em>integrations</em>"
      },
      "id": "603e84ec28ccbc9dffeba789"
    },
    {
      "sections": [
        "Connect AWS to New Relic infrastructure monitoring",
        "Connect AWS to New Relic",
        "Important",
        "Connect multiple AWS integrations",
        "Connect multiple AWS accounts",
        "Add or edit custom tags",
        "Disconnect your AWS integrations",
        "Regional support"
      ],
      "title": "Connect AWS to New Relic infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "a00c91900961871b2c48d88bca610d5457473f11",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-new-relic-infrastructure-monitoring/",
      "published_at": "2021-09-20T19:30:49Z",
      "updated_at": "2021-09-20T19:30:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. If you don't have one already, create a New Relic account. It's free, forever. Connect AWS to New Relic Important AWS CloudWatch metric streams is now the recommended solution to monitor AWS services. Learn more in New Relic's CloudWatch solution and AWS CloudWatch blog posts. Follow the steps documented in the AWS CloudWatch metric stream integration to ingest all available CloudWatch metrics. To connect additional API Polling integrations: Go to one.newrelic.com > Infrastructure > AWS. Click on one of the available service tiles. From the IAM console, click Create role, then click Another AWS account. For Account ID, use 754728514883. Check the Require external ID box. For External ID, enter your New Relic account ID. Do not enable the setting to Require MFA (multi-factor authentication). Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Review. Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. For the Role name, enter NewRelicInfrastructure-Integrations, then click Create role. Select the newly created role from the listed roles. On the Role summary page, select and copy the entire Role ARN (required later in this procedure). Configure a Budgets policy: While viewing the Role summary for your new role, select Add inline policy. Create a Custom policy: Enter a policy name (for example, NewRelicBudget), add the following permission statement, and then select Apply policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"budgets:ViewBudget\" ], \"Resource\": \"*\" } ] } Copy Return to the New Relic UI to enter your AWS account name and the ARN for the new role. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then Save. Connect multiple AWS integrations To connect multiple AWS integrations to a single New Relic account: If you previously set up an ARN with the more restrictive AmazonEC2ReadOnlyAccess policy, first unlink your existing integration, then create a new one with a broader policy. Follow the instructions to connect your Amazon account to New Relic . Provide the ARN that contains the ReadOnlyAccess policy. Once setup is complete, select the integrations you want to monitor: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Select the checkbox for each integration you want to monitor. Connect multiple AWS accounts By default, the Amazon EC2 AmazonEC2ReadOnlyAccess permission grants New Relic access to all EC2 instances in the individual Amazon account you specify during the setup steps. If you have multiple AWS accounts, follow the steps to connect an AWS account for each AWS account you want to associate with New Relic. Add or edit custom tags New Relic automatically imports custom tags you have added or edited for your AWS resources. Most metrics received via CloudWatch metric streams will have custom tags as dimensions. For API Polling integrations, if you don't see any tags in the Add filter menu of the Filter sets sidebar within a few minutes, delete the integration and try again: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Remove individual integrations or the entire account linkage as needed. Note that not all integrations support tags collection. You can enable (and disable) tags collection in the integration settings. Disconnect your AWS integrations You can disable one or more integrations anytime and still keep your AWS account connected to New Relic. However, New Relic recommends that you do not disable EC2 or EBS monitoring. These two integrations add important metadata to your EC2 instances and EBS volumes in New Relic. To uninstall your services completely from New Relic infrastructure Integrations, unlink your AWS account. Regional support China regions are not supported.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.92293,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Connect multiple AWS <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "To <em>start</em> receiving Amazon data with New Relic AWS <em>integrations</em>, connect your Amazon account to New Relic. If you don&#x27;t have one already, create a New Relic account. It&#x27;s free, forever. Connect AWS to New Relic Important AWS CloudWatch metric streams is now the recommended solution to monitor AWS"
      },
      "id": "6045079f196a679cc1960f2d"
    },
    {
      "sections": [
        "Connect AWS GovCloud to New Relic",
        "Important",
        "Requirements",
        "How to obtain GovCloud credentials for New Relic"
      ],
      "title": "Connect AWS GovCloud to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "ab6a129f8c50643b9af1c135863572d1ab595e30",
      "image": "https://docs.newrelic.com/static/2700987e921c2d686abb7518317cc2e1/49217/AWS-add-user.png",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-govcloud-new-relic/",
      "published_at": "2021-09-20T19:29:48Z",
      "updated_at": "2021-09-20T19:29:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The AWS GovCloud (US) regions are designed to address the specific regulatory needs of United States (federal, state, and local agencies), education institutions, and the supporting ecosystem. It is an isolated AWS region designed to host sensitive data and regulated workloads in the cloud, helping customers support their US government compliance requirements. The available set of AWS services is a subset of the AWS ecosystem. New Relic provides you with the confidence to deploy your most critical services on GovCloud, allowing you to monitor and observe your entire ecosystem from New Relic One. Important The AWS CloudWatch metric stream capability isn't available on GovCloud regions. Requirements Requirements include: You must have your AWS account connected to New Relic before connecting GovCloud. If you're using our AWS Lambda monitoring: our newrelic-log-ingestion is not deployed in the AWS Serverless Application Repository for AWS GovCloud; it must be installed manually. For instructions, see Enable Lambda monitoring. AWS integrations supported in GovCloud: ALB/NLB API Gateway Autoscaling CloudTrail DirectConnect DynamoDB EBS EC2 Elasticsearch ELB (Classic) EMR IAM Lambda RDS Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. Obtain your credentials. Go to one.newrelic.com > Infrastructure > GovCloud. Click on Add AWS GovCloud account. Give your AWS account a name, provide the credentials to connect your account, and click Submit. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then click Save. How to obtain GovCloud credentials for New Relic From the IAM console, click Add user. For the User name, type NewRelicInfrastructure-Integrations. For Select AWS access type, select as Programmatic access. AWS IAM console > Add user: add NewRelicInfrastructure-Integrations as a user. Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Tags (adding tags is optional). Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. AWS IAM console > Add user > Set permissions: select ReadOnlyAccess. On the Tags page, click Next: Review. Review the user detail summary and click Create user. AWS IAM console > Add user > Set permissions > Tags > Review: verify that the new user information is accurate. Your user should be successfully created. Download the user security credentials by clicking on the Download .csv button and then click Close.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.237595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": " Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To <em>start</em> receiving Amazon data with New Relic AWS <em>integrations</em>, connect your Amazon account to New Relic. Obtain your credentials. Go to one.newrelic.com &gt; Infrastructure &gt; GovCloud. Click on Add AWS GovCloud account. Give"
      },
      "id": "603e85bc196a675469a83dcd"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.27803,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-09-26T22:23:20Z",
      "updated_at": "2021-08-08T19:27:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.71423,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-09-26T22:21:05Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.9908,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-09-26T22:23:20Z",
      "updated_at": "2021-08-08T19:27:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.71423,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-09-26T22:21:05Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.4538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-09-26T22:22:00Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.83136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.0909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-09-26T22:21:45Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.08762,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-rename-or-copy-prometheus-attributes": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-09-26T22:22:00Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.83133,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.0909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-09-26T22:21:45Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.08762,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-09-26T22:22:00Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.83133,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.0909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.96642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.0909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-09-26T22:21:45Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.08762,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.96642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-09-26T22:22:00Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.8313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.0909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-09-26T22:21:45Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.08762,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-09-26T22:22:00Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.83127,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-09-26T22:21:45Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.08762,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-09-26T18:24:45Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.96642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Prometheus OpenMetrics integration for Kubernetes or Docker",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-09-26T22:20:14Z",
      "updated_at": "2021-07-22T05:51:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Examine the benefits, reminders, and recommendations for each option below. Prometheus remote write integration Benefits: Easy access to your combined metrics in New Relic if you already have Prometheus servers. Access only takes one line of yaml in your Prometheus configuration. Access your metrics through both New Relic and Prometheus without making additional adjustments in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Reminders: You will need to manage your Prometheus servers. You can reduce your storage retention. Fewer query loads to the server. Recommendations: Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Prometheus OpenMetrics integration for Kubernetes or Docker Benefits: Best for an alternative to Prometheus servers Store all your metrics directly in New Relic No need to manage any Prometheus servers yourself. No need for local storage. Reminders: Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free — we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relic’s Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1899.2471,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "This page provides an overview of New Relic&#x27;s <em>Prometheus</em> <em>integration</em> options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. <em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>? We currently offer two"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-09-26T22:21:05Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1743.1234,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can configure your <em>remote</em> <em>write</em> <em>integration</em> so that New"
      },
      "id": "6044e621196a67b846960f6b"
    },
    {
      "image": "https://docs.newrelic.com/static/d2a9c929c7541b67b6fe4c87844fc01b/ae694/prometheus_grafana_dashboard.png",
      "url": "https://docs.newrelic.com/whats-new/2020/08/create-grafana-dashboards-prometheus-data-stored-new-relic/",
      "sections": [
        "Create Grafana dashboards with Prometheus data stored in New Relic",
        "Step 1: Get data flowing into New Relic with the Prometheus remote write integration",
        "Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic"
      ],
      "published_at": "2021-09-20T19:48:22Z",
      "title": "Create Grafana dashboards with Prometheus data stored in New Relic",
      "updated_at": "2021-03-11T00:16:19Z",
      "type": "docs",
      "external_id": "da09ab47a2ac806ad3ed1fa67e3a02dd54394383",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We’ve teamed up with Grafana Labs so you can use our Telemetry Data Platform as a data source for Prometheus metrics and see them in your existing dashboards, seamlessly tapping into the reliability, scale, and security provided by New Relic. Follow the steps below or use this more detailed walkthrough to send Prometheus data to New Relic, so that Grafana can populate your existing Prometheus-specific dashboards with that data. This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. You’ll also need to sign up for New Relic. Here's an example of how these Grafana dashboards with Prometheus data look in our new dark mode. Step 1: Get data flowing into New Relic with the Prometheus remote write integration Go to Instrument Everything – US or Instrument Everything – EU, then click the Prometheus tile. You can also go to the Prometheus remote write setup page to get your remote_write URL. For more information on how to set up the Prometheus remote write integration, check out our docs. Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic For more information on how to configure New Relic as a Prometheus data source for Grafana, check out our docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1741.6866,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create Grafana dashboards with <em>Prometheus</em> data stored in New Relic",
        "sections": "Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "body": " these Grafana dashboards with <em>Prometheus</em> data look in our new dark mode. Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> Go to Instrument Everything – US or Instrument Everything – EU, then click the <em>Prometheus</em> tile. You can also go to the <em>Prometheus</em> <em>remote</em> <em>write</em>"
      },
      "id": "60445821e7b9d23b585799e4"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.27786,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-09-26T22:21:05Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.99078,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-09-26T22:21:05Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.4538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration": [
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-09-26T22:23:20Z",
      "updated_at": "2021-08-08T19:27:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.7142,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-09-26T22:21:05Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.99078,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-09-26T22:21:05Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.4538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/debug-issues-data-sent-metric-api-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.437904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.906296,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-09-26T22:24:40Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.46623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.437904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.906296,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-09-26T22:24:40Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.46623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-logs-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.437904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.906296,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-09-26T22:24:40Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.46623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-scraper-metrics-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.4379,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.90628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-09-26T22:24:40Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.46623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.4379,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.90628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-09-26T18:25:41Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 96.607574,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.90627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-09-26T22:24:40Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.46623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-09-26T18:25:41Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 96.607574,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.4379,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.90627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-09-26T22:24:40Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.46623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.43789,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-09-26T22:23:48Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.90626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-09-26T22:24:40Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.46623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features": [
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-09-26T22:26:04Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.66333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-09-26T22:26:05Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.66313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.284096,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem Your <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a <em>query</em> of <em>Prometheus</em> metrics"
      },
      "id": "6045054528ccbc2c342c606b"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-09-26T22:25:41Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.66397,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-09-26T22:26:05Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.66313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.28409,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem Your <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a <em>query</em> of <em>Prometheus</em> metrics"
      },
      "id": "6045054528ccbc2c342c606b"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-09-26T22:25:41Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.66397,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-09-26T22:26:04Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.66333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.28409,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem Your <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a <em>query</em> of <em>Prometheus</em> metrics"
      },
      "id": "6045054528ccbc2c342c606b"
    }
  ],
  "/docs/licenses/index": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 66.40979,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Licenses</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;license-information&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 64.02403,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Licenses</em>"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-09-26T19:02:31Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 57.053898,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.38997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-09-20T19:19:16Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.43376,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.44171,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.3899,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-09-20T19:19:16Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.43376,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-09-26T22:26:42Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.44525,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.3899,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-09-20T19:19:16Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.43376,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-09-26T22:26:42Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.44525,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    }
  ],
  "/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.38983,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.30455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-09-26T22:30:09Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In New Relic APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into our database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-09-26T22:28:25Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 247.99211,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-09-26T22:29:28Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.80911,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-09-26T22:29:28Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.80911,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.49516,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/government-addendum": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-09-26T22:28:25Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 247.99208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-09-26T22:29:28Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.80911,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-09-26T22:28:25Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 247.99205,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.49516,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relics-provision-services": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-09-26T22:28:25Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 247.99205,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-09-26T22:29:28Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.8091,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.4959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/peoples-republic-china": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-09-26T22:28:25Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 247.992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-09-26T22:29:28Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.8091,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-09-26T22:29:10Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.49516,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/other-licenses/services-licenses": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.38965,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-09-26T22:26:42Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.44525,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-09-26T18:26:26Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.44171,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/product-definitions/legacy-product-definitions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-09-26T22:31:05Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.43239,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing: <em>Definitions</em>",
        "sections": "New Relic One pricing: <em>Definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing, invoicing related <em>information</em>, and <em>product</em>-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing."
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.38957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.30432,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    }
  ],
  "/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions": [
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-09-26T22:30:09Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In New Relic APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into our database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 264.08774,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Original <em>product</em>-based pricing <em>definitions</em>",
        "sections": "Original <em>product</em>-based pricing <em>definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original <em>product</em>-based pricing. For New Relic One pricing plan terms, see New Relic One pricing <em>definitions</em>. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.38957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.30432,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.56726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 333.15295,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-09-26T18:27:22Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.96921,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-guide": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.56726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-09-26T18:27:22Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.96921,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-09-26T22:30:09Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.81128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-policy": [
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 333.15283,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-09-26T18:27:22Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.96921,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-09-26T22:30:09Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.81128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/service-level-availability-commitment": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.56714,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 333.15283,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-09-26T22:30:09Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.81128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/data-collector-licenses": [
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-09-26T22:31:54Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-09-26T18:27:21Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-09-26T22:31:12Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.40167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-09-26T18:27:22Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-09-26T22:31:54Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-09-26T18:27:21Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-premium-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-09-26T18:27:22Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-09-26T22:31:54Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-09-26T22:31:12Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.40167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-priority-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-09-26T18:27:22Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-09-26T18:27:21Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.54538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-09-26T22:31:12Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.40167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions": [
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-09-26T18:28:11Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.74548,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "sections": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " reference the <em>usage</em> <em>plans</em> set forth below or where a subscription has indeterminate product pricing or <em>usage</em> quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. <em>Usage</em>"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-09-20T19:36:40Z",
      "updated_at": "2021-09-20T19:36:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 16 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encyrption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our security guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.38931,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-09-20T19:45:30Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.30411,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    }
  ]
}