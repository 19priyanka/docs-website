{
  "/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent": [
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.94962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid inaccurate"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    },
    {
      "sections": [
        "Introduction to New Relic for Python",
        "Monitor app performance",
        "Install the Python agent",
        "Monitor non-web scripts, background tasks, and functions",
        "What's next after installation?",
        "Troubleshooting",
        "Check the source code"
      ],
      "title": "Introduction to New Relic for Python",
      "type": "docs",
      "tags": [
        "Agents",
        "Python agent",
        "Getting started"
      ],
      "external_id": "e3621b5589469c2b3b20d5d140027e5c105e1dd3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apm/agents/python-agent/getting-started/introduction-new-relic-python/",
      "published_at": "2022-01-08T05:18:32Z",
      "updated_at": "2022-01-08T05:18:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Python agent monitors your Python application to help you identify and solve performance issues. You can also extend your performance monitoring to collect and analyze business data to help you improve the customer experience and make data-driven business decisions. With flexible options for custom instrumentation and APIs, The Python agent offers multiple building blocks to customize the data you need from your app. Our Python works with a wide variety of web frameworks and hosting mechanisms, including Django, Gunicorn, WSGI, CherryPy, uWSGI, and more. You can also install the Python agent in a Google App Engine flexible environment. Monitor app performance After you install the Python agent, it begins to collect data about your app. You can view the data as charts and tables in New Relic One. View the big picture of your app: Monitor your app's Apdex (user satisfaction). Get a high-level summary of your app with Summary page. Enable distributed tracing to see activity across an architecture having many services. Install Infrastructure monitoring and view detailed server/host data for your app. Find errors and problems quickly: Track key transactions specific to your business. Create custom dashboards for important metrics. Alert your team when an error or problem occurs before it affects your users. View performance after a deployment. Drill down into performance details: Examine code-level transaction traces. Examine database query traces. Examine error traces. Use thread profiler sessions to see detailed stack traces of sampled threads View logs for your APM and infrastructure data: Bring your logs and application's data together to make troubleshooting easier and faster. No need to switch to another UI page in New Relic One. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. Extend agent instrumentation: Other helpful tools include: Tools Description Browser monitoring Integrate the Python agent with browser monitoring to gain visibility into end-user browser activity. Simple scripts and background tasks Monitor job-based or task queuing systems, like Celery, or other standalone non-web functions. Business data analysis with data exploration. Use the Python agent with our data explorer to organize, query, and visualize your data to answer key questions about application performance and customer experience. Use default transaction attributes, or add your own. Query your data using the New Relic Query Language (NRQL). Send your own event data. Create and share customizable, interactive dashboards. Install the Python agent Before you install the Python agent, make sure your system meets the system requirements. You must also create a New Relic account. We support a number of web frameworks and libraries right out of the box, including Django, WSGI, and Gunicorn. If you use one of the supported web frameworks, installation is easy. If you use an unsupported framework, the process will involve some additions to your app code and/or web server files. For a quick and simple install process that will work for the majority of setups, follow these simple steps: Download and install the Python package. Create config file. Integrate the Python agent with your application. Read the Quick Start guide Add Python data Monitor non-web scripts, background tasks, and functions The Python agent also lets you monitor non-web scripts, worker processes, tasks, and functions. The installation process for these non-web transactions is similar to the one used for a web app, with one major difference: instead of going through the standard integration process described in the install instructions, you would manually \"wrap\" any function you want to monitor. For more information, see Non-web tasks and processes. For instructions on monitoring Celery tasks, see Celery background tasks. What's next after installation? Once you get the agent up and running, some suggested next steps are: Explore your data in and get comfortable with the user interface. Read our docs on our other observability solutions and the APM page. Change your application's name, or other configuration options. Learn about setting up custom instrumentation for application activity not monitored by default. Consider the Python Telemetry SDK. Troubleshooting After you complete the install process, your data should appear in the APM UI within five minutes. If it does not, use these troubleshooting resources: If no data appears, follow these troubleshooting steps. If you experience issues when installing or running the Python agent on a new host, test that the package is installed correctly and that it can contact New Relic's data collector service. For other problems, see the full list of troubleshooting documentation. Check the source code The Python agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.77676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Install</em> <em>the</em> Python <em>agent</em>",
        "tags": "<em>Getting</em> <em>started</em>",
        "body": " <em>agent</em> in a Google App Engine flexible environment. Monitor app performance After you <em>install</em> the Python <em>agent</em>, it begins to collect data about your app. You can view the data as charts and tables in New Relic One. View the big picture of your app: Monitor your app&#x27;s Apdex (user satisfaction). <em>Get</em>"
      },
      "id": "61749d4d196a6727702f1368"
    },
    {
      "sections": [
        "New Relic API keys",
        "API key UI",
        "Overview of keys",
        "Keys for data ingest",
        "Recommendations for managing ingest keys",
        "Keys for querying and configuration",
        "License key",
        "Create and manage license keys",
        "User key",
        "Browser key",
        "Insights insert key",
        "Important",
        "REST API key",
        "Insights query key",
        "Admin key",
        "Account ID"
      ],
      "title": "New Relic API keys",
      "type": "docs",
      "tags": [
        "APIs",
        "Get started",
        "Intro to APIs"
      ],
      "external_id": "b373cd68cf21daeb5d912ffb4b1ae3f14f500fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/",
      "published_at": "2022-01-08T03:11:41Z",
      "updated_at": "2022-01-08T03:11:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has several different APIs that use different API keys. This resource explains our keys, what they're used for, and how to access them. Ready to get started? Make sure you have a New Relic account. It's free, forever! API key UI Most of the keys can be viewed and managed via the API keys UI page: From the account dropdown, click API keys (get a direct link to the API keys page). If you're using NerdGraph, you can manage license keys and user keys from our GraphiQL explorer. Overview of keys If you're using a specific API, see the docs for that API to understand which keys are required and recommended. To learn about our APIs, see Introduction to APIs. Our keys can be broken down into two categories: Keys used for data ingest Keys used for querying and configuration Keys for data ingest There are many ways to get data into New Relic. Here are the API keys used for data ingest: License key: our primary ingest key, used for APM ingest, infrastructure monitoring ingest, and our ingest APIs and the integrations that use them. Browser key: used for browser monitoring ingest. Mobile app token: used for mobile monitoring ingest. Insights insert key: an older key that has been mostly deprecated, it has the same functionality as the license key. We recommend using the license key instead. Recommendations for managing ingest keys Some recommendations for managing data ingest keys: Keep them safe. Because these keys are used for data ingest, we recommend you treat ingest keys securely, like you would a password. This ensures no unwanted data is sent to your New Relic account. If a data ingest key falls into the wrong hands, it would allow someone to send data to your account, which could trigger false alerts and contaminate your data so that detecting actual issues is more difficult. If you believe a data ingest key has been exposed and has generated unwanted data, work with our Support team. Make additional keys. When setting up monitoring solutions that require a data ingest key, we recommend creating a new key, if possible. The original data ingest key for an account or app cannot be deleted or edited, so in order to give you greater management control (for example, deleting a key if exposed), we recommend creating new ones and using those. Keys for querying and configuration Here are keys used for querying New Relic data or configuration of features: User key, also known as a \"personal API key\": used for NerdGraph (our GraphQL API) and for accessing REST API endpoints. REST API key: used for the REST API but we instead recommend using the user key because it has fewer restrictions. Insights query key: used with the Insights query API for querying New Relic data. We recommend using NerdGraph instead of this API. License key Our primary key used for data ingest is called the license key, also referenced in the UI and NerdGraph API as ingest - license. The license key is a 40-character hexadecimal string associated with a New Relic account. Each account in a New Relic organization has at least one license key. When you first sign up for New Relic, that creates an organization with a single account, and that account has its own license key. If more accounts are added, each account will have its own license key. An account's original license key cannot be deleted but you can create additional license keys that can be managed and deleted. The types of data ingest the license key is used for include: APM agent data. Infrastructure agent data. Data sent via our core data ingest APIs (Metric API, Trace API, Event API, Log API), and the SDKs and integrations that use those APIs. The license key is used for almost all New Relic data ingest. The main exceptions are browser monitoring data (which uses a browser key) and mobile monitoring data (which uses a mobile app token). Create and manage license keys For tips on best practices for key management, see Ingest key recommendations. To add, delete, and manage license keys: From the account dropdown, click API keys (get a direct link to the API keys page). You can also create and manage keys with our NerdGraph API. Note that you can't delete or change an account's original license key (the one generated upon account creation). For that, contact New Relic support. User key New Relic user keys, sometimes referred to as \"personal API keys\", are required for using NerdGraph and for the REST API. A user key is tied to both a specific New Relic user and a specific account, and they cannot be transferred. Our APIs that use this key let a user make queries for any accounts that user has been granted access to, not just the specific account the key was created under. If the key's user is deleted, all their user keys will be deactivated and will no longer be valid in API requests. To view and manage the user key and other API keys in the UI: From the account dropdown, click API keys (here's a direct link to the API keys page). To manage this key via API, see Manage keys with NerdGraph. You can also get or generate a user key from the NerdGraph GraphiQL explorer. Browser key One of the New Relic API keys that are used for data ingest is the browser key. The browser key allows the ingestion of data from New Relic browser monitoring. For tips on best practices for key management, see Ingest key recommendations. To view and manage this key: From the account dropdown, click API keys (here's a direct link to the API keys page). You can't manage or delete an original browser key that was created when your account was created. For that, contact New Relic support. Insights insert key Important This key is still in use but we highly recommend using the license key, which can be used for the same things and more. One of the New Relic API keys used for data ingest is the Insights insert key, also known as an \"insert key\"). Note that the license key is used for the same functionality and more, which is why we recommend the license key over this key. This key is used for the ingestion of data via our Event API, Log API, Metric API, and Trace API, or via tools that use those APIs. Tips on availability and access: Because these keys are associated with an account and not a specific user, anyone in the account with access to a key can use it. As a best practice for security purposes, we recommend you use different Insights insert keys for different applications or different data sources. To find and manage Insights insert keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click Insights insert keys. REST API key Important We highly recommend using a user key instead, because that key has fewer restrictions. The REST API key is for using our REST APIs for Alerts, APM, browser, infrastructure alerts, as well as mobile monitoring REST APIs and the API Explorer. Things to consider: We recommend using our newer NerdGraph API over the REST API, if possible. Requires admin-level user permissions. If you don't have access to the REST API key or the REST API explorer, it might be due to lack of permissions. Talk to your New Relic account manager, or use a user key instead. Each New Relic account can have only one REST API key. To find and manage REST API keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click REST API key. Before you configure or delete an API key, ensure you are doing so for the correct account. Insights query key The Insights query key is used for our Insights query API: we now recommend using NerdGraph for querying New Relic data. To find and manage Insights query keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click Insights query keys. Admin key Important As of December 4, 2020, all existing admin keys have been migrated to be user keys. You don’t need to do anything for existing admin keys to remain active. They will be automatically accessible via the API keys UI, labeled as user keys, and granted identical permissions. You can manage them as you would any user key via the same workflow. All migrated admin keys will have a note that says “Migrated from an admin user key” in the key table, so you’ll be able to find them easily. Account ID Looking for the account ID? See Account ID.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.1503,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic has several different APIs that use different API keys. This resource explains our keys, what they&#x27;re used for, and how to access them. Ready to <em>get</em> <em>started</em>? Make sure you have a New Relic account. It&#x27;s free, forever! API key UI Most of the keys can be viewed and managed via the API keys"
      },
      "id": "6043fa3464441f1358378f3b"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent": [
    {
      "sections": [
        "Install the infrastructure agent",
        "Quick start: Use our guided install",
        "Important",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "One agent, many capabilities",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2022-01-08T08:54:45Z",
      "updated_at": "2021-11-24T19:41:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from our on-host integrations to New Relic, as well as log data for log analytics. The infrastructure monitoring agent can currently run on many Linux distributions, Windows, and macOS. There are multiple ways to install and deploy the agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. Ready to get started? You'll need a New Relic account before you can install. Click one of these button to try it out. Get an account Guided install EU guided install The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. For more information on where you can run the agent, check the compatibility and requirements page. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. Install the infrastructure monitoring agent Linux If you don't have a New Relic account yet, the guided install won't work. If you want to follow the procedure manually, see our tutorial. Windows Server and 10 If you don't have a New Relic account yet, the guided install won't work. If you want to follow the procedure manually using our MSI installer, see our tutorial. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Infrastructre can also be deployed in macOS. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.64735,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " distributions, Windows, and macOS. There are multiple ways to <em>install</em> and deploy the <em>agent</em>, depending on your setup and needs. This document describes how the <em>infrastructure</em> monitoring <em>agent</em> works and how to <em>install</em> it. Quick <em>start</em>: Use our guided <em>install</em> The quickest way to <em>get</em> <em>started</em> with our <em>infrastructure</em>"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Introduction to New Relic for Python",
        "Monitor app performance",
        "Install the Python agent",
        "Monitor non-web scripts, background tasks, and functions",
        "What's next after installation?",
        "Troubleshooting",
        "Check the source code"
      ],
      "title": "Introduction to New Relic for Python",
      "type": "docs",
      "tags": [
        "Agents",
        "Python agent",
        "Getting started"
      ],
      "external_id": "e3621b5589469c2b3b20d5d140027e5c105e1dd3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apm/agents/python-agent/getting-started/introduction-new-relic-python/",
      "published_at": "2022-01-08T05:18:32Z",
      "updated_at": "2022-01-08T05:18:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Python agent monitors your Python application to help you identify and solve performance issues. You can also extend your performance monitoring to collect and analyze business data to help you improve the customer experience and make data-driven business decisions. With flexible options for custom instrumentation and APIs, The Python agent offers multiple building blocks to customize the data you need from your app. Our Python works with a wide variety of web frameworks and hosting mechanisms, including Django, Gunicorn, WSGI, CherryPy, uWSGI, and more. You can also install the Python agent in a Google App Engine flexible environment. Monitor app performance After you install the Python agent, it begins to collect data about your app. You can view the data as charts and tables in New Relic One. View the big picture of your app: Monitor your app's Apdex (user satisfaction). Get a high-level summary of your app with Summary page. Enable distributed tracing to see activity across an architecture having many services. Install Infrastructure monitoring and view detailed server/host data for your app. Find errors and problems quickly: Track key transactions specific to your business. Create custom dashboards for important metrics. Alert your team when an error or problem occurs before it affects your users. View performance after a deployment. Drill down into performance details: Examine code-level transaction traces. Examine database query traces. Examine error traces. Use thread profiler sessions to see detailed stack traces of sampled threads View logs for your APM and infrastructure data: Bring your logs and application's data together to make troubleshooting easier and faster. No need to switch to another UI page in New Relic One. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. Extend agent instrumentation: Other helpful tools include: Tools Description Browser monitoring Integrate the Python agent with browser monitoring to gain visibility into end-user browser activity. Simple scripts and background tasks Monitor job-based or task queuing systems, like Celery, or other standalone non-web functions. Business data analysis with data exploration. Use the Python agent with our data explorer to organize, query, and visualize your data to answer key questions about application performance and customer experience. Use default transaction attributes, or add your own. Query your data using the New Relic Query Language (NRQL). Send your own event data. Create and share customizable, interactive dashboards. Install the Python agent Before you install the Python agent, make sure your system meets the system requirements. You must also create a New Relic account. We support a number of web frameworks and libraries right out of the box, including Django, WSGI, and Gunicorn. If you use one of the supported web frameworks, installation is easy. If you use an unsupported framework, the process will involve some additions to your app code and/or web server files. For a quick and simple install process that will work for the majority of setups, follow these simple steps: Download and install the Python package. Create config file. Integrate the Python agent with your application. Read the Quick Start guide Add Python data Monitor non-web scripts, background tasks, and functions The Python agent also lets you monitor non-web scripts, worker processes, tasks, and functions. The installation process for these non-web transactions is similar to the one used for a web app, with one major difference: instead of going through the standard integration process described in the install instructions, you would manually \"wrap\" any function you want to monitor. For more information, see Non-web tasks and processes. For instructions on monitoring Celery tasks, see Celery background tasks. What's next after installation? Once you get the agent up and running, some suggested next steps are: Explore your data in and get comfortable with the user interface. Read our docs on our other observability solutions and the APM page. Change your application's name, or other configuration options. Learn about setting up custom instrumentation for application activity not monitored by default. Consider the Python Telemetry SDK. Troubleshooting After you complete the install process, your data should appear in the APM UI within five minutes. If it does not, use these troubleshooting resources: If no data appears, follow these troubleshooting steps. If you experience issues when installing or running the Python agent on a new host, test that the package is installed correctly and that it can contact New Relic's data collector service. For other problems, see the full list of troubleshooting documentation. Check the source code The Python agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.77676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Install</em> <em>the</em> Python <em>agent</em>",
        "tags": "<em>Getting</em> <em>started</em>",
        "body": " <em>agent</em> in a Google App Engine flexible environment. Monitor app performance After you <em>install</em> the Python <em>agent</em>, it begins to collect data about your app. You can view the data as charts and tables in New Relic One. View the big picture of your app: Monitor your app&#x27;s Apdex (user satisfaction). <em>Get</em>"
      },
      "id": "61749d4d196a6727702f1368"
    },
    {
      "sections": [
        "New Relic API keys",
        "API key UI",
        "Overview of keys",
        "Keys for data ingest",
        "Recommendations for managing ingest keys",
        "Keys for querying and configuration",
        "License key",
        "Create and manage license keys",
        "User key",
        "Browser key",
        "Insights insert key",
        "Important",
        "REST API key",
        "Insights query key",
        "Admin key",
        "Account ID"
      ],
      "title": "New Relic API keys",
      "type": "docs",
      "tags": [
        "APIs",
        "Get started",
        "Intro to APIs"
      ],
      "external_id": "b373cd68cf21daeb5d912ffb4b1ae3f14f500fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/",
      "published_at": "2022-01-08T03:11:41Z",
      "updated_at": "2022-01-08T03:11:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has several different APIs that use different API keys. This resource explains our keys, what they're used for, and how to access them. Ready to get started? Make sure you have a New Relic account. It's free, forever! API key UI Most of the keys can be viewed and managed via the API keys UI page: From the account dropdown, click API keys (get a direct link to the API keys page). If you're using NerdGraph, you can manage license keys and user keys from our GraphiQL explorer. Overview of keys If you're using a specific API, see the docs for that API to understand which keys are required and recommended. To learn about our APIs, see Introduction to APIs. Our keys can be broken down into two categories: Keys used for data ingest Keys used for querying and configuration Keys for data ingest There are many ways to get data into New Relic. Here are the API keys used for data ingest: License key: our primary ingest key, used for APM ingest, infrastructure monitoring ingest, and our ingest APIs and the integrations that use them. Browser key: used for browser monitoring ingest. Mobile app token: used for mobile monitoring ingest. Insights insert key: an older key that has been mostly deprecated, it has the same functionality as the license key. We recommend using the license key instead. Recommendations for managing ingest keys Some recommendations for managing data ingest keys: Keep them safe. Because these keys are used for data ingest, we recommend you treat ingest keys securely, like you would a password. This ensures no unwanted data is sent to your New Relic account. If a data ingest key falls into the wrong hands, it would allow someone to send data to your account, which could trigger false alerts and contaminate your data so that detecting actual issues is more difficult. If you believe a data ingest key has been exposed and has generated unwanted data, work with our Support team. Make additional keys. When setting up monitoring solutions that require a data ingest key, we recommend creating a new key, if possible. The original data ingest key for an account or app cannot be deleted or edited, so in order to give you greater management control (for example, deleting a key if exposed), we recommend creating new ones and using those. Keys for querying and configuration Here are keys used for querying New Relic data or configuration of features: User key, also known as a \"personal API key\": used for NerdGraph (our GraphQL API) and for accessing REST API endpoints. REST API key: used for the REST API but we instead recommend using the user key because it has fewer restrictions. Insights query key: used with the Insights query API for querying New Relic data. We recommend using NerdGraph instead of this API. License key Our primary key used for data ingest is called the license key, also referenced in the UI and NerdGraph API as ingest - license. The license key is a 40-character hexadecimal string associated with a New Relic account. Each account in a New Relic organization has at least one license key. When you first sign up for New Relic, that creates an organization with a single account, and that account has its own license key. If more accounts are added, each account will have its own license key. An account's original license key cannot be deleted but you can create additional license keys that can be managed and deleted. The types of data ingest the license key is used for include: APM agent data. Infrastructure agent data. Data sent via our core data ingest APIs (Metric API, Trace API, Event API, Log API), and the SDKs and integrations that use those APIs. The license key is used for almost all New Relic data ingest. The main exceptions are browser monitoring data (which uses a browser key) and mobile monitoring data (which uses a mobile app token). Create and manage license keys For tips on best practices for key management, see Ingest key recommendations. To add, delete, and manage license keys: From the account dropdown, click API keys (get a direct link to the API keys page). You can also create and manage keys with our NerdGraph API. Note that you can't delete or change an account's original license key (the one generated upon account creation). For that, contact New Relic support. User key New Relic user keys, sometimes referred to as \"personal API keys\", are required for using NerdGraph and for the REST API. A user key is tied to both a specific New Relic user and a specific account, and they cannot be transferred. Our APIs that use this key let a user make queries for any accounts that user has been granted access to, not just the specific account the key was created under. If the key's user is deleted, all their user keys will be deactivated and will no longer be valid in API requests. To view and manage the user key and other API keys in the UI: From the account dropdown, click API keys (here's a direct link to the API keys page). To manage this key via API, see Manage keys with NerdGraph. You can also get or generate a user key from the NerdGraph GraphiQL explorer. Browser key One of the New Relic API keys that are used for data ingest is the browser key. The browser key allows the ingestion of data from New Relic browser monitoring. For tips on best practices for key management, see Ingest key recommendations. To view and manage this key: From the account dropdown, click API keys (here's a direct link to the API keys page). You can't manage or delete an original browser key that was created when your account was created. For that, contact New Relic support. Insights insert key Important This key is still in use but we highly recommend using the license key, which can be used for the same things and more. One of the New Relic API keys used for data ingest is the Insights insert key, also known as an \"insert key\"). Note that the license key is used for the same functionality and more, which is why we recommend the license key over this key. This key is used for the ingestion of data via our Event API, Log API, Metric API, and Trace API, or via tools that use those APIs. Tips on availability and access: Because these keys are associated with an account and not a specific user, anyone in the account with access to a key can use it. As a best practice for security purposes, we recommend you use different Insights insert keys for different applications or different data sources. To find and manage Insights insert keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click Insights insert keys. REST API key Important We highly recommend using a user key instead, because that key has fewer restrictions. The REST API key is for using our REST APIs for Alerts, APM, browser, infrastructure alerts, as well as mobile monitoring REST APIs and the API Explorer. Things to consider: We recommend using our newer NerdGraph API over the REST API, if possible. Requires admin-level user permissions. If you don't have access to the REST API key or the REST API explorer, it might be due to lack of permissions. Talk to your New Relic account manager, or use a user key instead. Each New Relic account can have only one REST API key. To find and manage REST API keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click REST API key. Before you configure or delete an API key, ensure you are doing so for the correct account. Insights query key The Insights query key is used for our Insights query API: we now recommend using NerdGraph for querying New Relic data. To find and manage Insights query keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click Insights query keys. Admin key Important As of December 4, 2020, all existing admin keys have been migrated to be user keys. You don’t need to do anything for existing admin keys to remain active. They will be automatically accessible via the API keys UI, labeled as user keys, and granted identical permissions. You can manage them as you would any user key via the same workflow. All migrated admin keys will have a note that says “Migrated from an admin user key” in the key table, so you’ll be able to find them easily. Account ID Looking for the account ID? See Account ID.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.1503,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic has several different APIs that use different API keys. This resource explains our keys, what they&#x27;re used for, and how to access them. Ready to <em>get</em> <em>started</em>? Make sure you have a New Relic account. It&#x27;s free, forever! API key UI Most of the keys can be viewed and managed via the API keys"
      },
      "id": "6043fa3464441f1358378f3b"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/azure-extensions-infrastructure": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Linux",
        "Step-by-step instructions",
        "Debian",
        "Ubuntu",
        "Amazon Linux, CentOS, RHEL",
        "SLES",
        "Important",
        "Amazon Linux",
        "CentOS / RHEL & Oracle Linux",
        "Root (default)",
        "Privileged user",
        "Unprivileged user",
        "Install using configuration management tools",
        "Install for Docker containers on instrumented hosts",
        "Install using Azure extensions",
        "Install using tarball files",
        "Update the agent",
        "What's next?"
      ],
      "title": "Install the infrastructure monitoring agent for Linux",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "060512e99d6143e7a7e8e6d16ba96cdcc7534e57",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux/",
      "published_at": "2022-01-08T09:40:49Z",
      "updated_at": "2021-11-06T16:34:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The quickest way to get started with our infrastructure monitoring agent is through our guided install. If you're in the EU, try our EU guided install. Either way, you'll need a New Relic account if you don't have one. (It's free, forever.) Get an account Guided install EU guided install Step-by-step instructions If guided install doesn't work, you can install the agent manually. Before installing infrastructure, be sure to: Review the requirements. Have a valid New Relic license key. To install infrastructure in Linux, follow these instructions: Create the configuration file and add your license key: echo \"license_key: YOUR_LICENSE_KEY\" | sudo tee -a /etc/newrelic-infra.yml Copy Determine the distribution version number: Debian cat /etc/os-release Copy Ubuntu cat /etc/lsb-release Copy Amazon Linux, CentOS, RHEL cat /etc/os-release Copy SLES cat /etc/os-release | grep VERSION_ID Copy Enable New Relic's GPG key. Debian curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Ubuntu curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Amazon Linux, CentOS, RHEL yum automatically installs the GPG key using the value in gpgkey. SLES curl https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg -s | sudo gpg --import Copy Important There's a known issue with the zypper package manager where GPG keys may not be validated as expected. If you get errors such as Signature verification failed, see New Relic's Explorers Hub for more information. Add the infrastructure monitoring agent repository: Debian Debian 8 (\"Jessie\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt jessie main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 9 (\"Stretch\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt stretch main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 10 (\"Buster\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt buster main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 11 (\"Bullseye\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bullseye main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu Ubuntu 16 (\"Xenial\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 18 (\"Bionic\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bionic main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20 (\"Focal\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt focal main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20.10 (\"Groovy\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt groovy main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 21.04 (\"Hirsute Hippo\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt hirsute main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Amazon Linux Amazon Linux 2 (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64/newrelic-infra.repo Copy Amazon Linux 2 (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/aarch64/newrelic-infra.repo Copy CentOS / RHEL & Oracle Linux CentOS, RHEL, Oracle Linux 7.x (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo Copy CentOS RHEL, Oracle Linux 7.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/aarch64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/x86_64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux 8.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/aarch64/newrelic-infra.repo Copy SLES SLES 11.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/11.4/x86_64/newrelic-infra.repo Copy SLES 12.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/x86_64/newrelic-infra.repo Copy SLES 12.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/aarch64/newrelic-infra.repo Copy SLES 12.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/x86_64/newrelic-infra.repo Copy SLES 12.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/aarch64/newrelic-infra.repo Copy SLES 12.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/x86_64/newrelic-infra.repo Copy SLES 12.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/aarch64/newrelic-infra.repo Copy SLES 12.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/x86_64/newrelic-infra.repo Copy SLES 12.4 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/aarch64/newrelic-infra.repo Copy SLES 12.5 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/x86_64/newrelic-infra.repo Copy SLES 12.5 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/aarch64/newrelic-infra.repo Copy SLES 15.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/x86_64/newrelic-infra.repo Copy SLES 15.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/aarch64/newrelic-infra.repo Copy SLES 15.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/x86_64/newrelic-infra.repo Copy SLES 15.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/aarch64/newrelic-infra.repo Copy SLES 15.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/x86_64/newrelic-infra.repo Copy SLES 15.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/aarch64/newrelic-infra.repo Copy Refresh the repositories: Debian sudo apt-get update Copy Ubuntu sudo apt-get update Copy Amazon Linux, CentOS, RHEL sudo yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' Copy SLES sudo zypper -n ref -r newrelic-infra Copy Install the newrelic-infra package in root (default), privileged user, or unprivileged user mode. For more information on each running mode, see Linux agent running modes. Root (default) Debian and Ubuntu: sudo apt-get install newrelic-infra -y Copy Amazon Linux, CentOS, RHEL: sudo yum install newrelic-infra -y Copy SLES: sudo zypper -n install newrelic-infra Copy Privileged user Install the libcap library and set the NRIA_MODE environment variable to PRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"PRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"PRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"PRIVILEGED\" zypper install newrelic-infra Copy Unprivileged user Install the libcap library and set the NRIA_MODE environment variable to UNPRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"UNPRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"UNPRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"UNPRIVILEGED\" zypper install newrelic-infra Copy Once the infrastructure monitoring agent is installed or updated, you can start, stop, or check the agent status. Important As of version 1.4.0, the infrastructure monitoring agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. Install using configuration management tools To install the infrastructure monitoring agent with a configuration management tool, see the documentation for: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Install for Docker containers on instrumented hosts See Docker instrumentation for infrastructure monitoring. Install using Azure extensions See Azure extensions for infrastructure monitoring. Install using tarball files For custom setup scenarios, you can install the infrastructure monitoring agent using our tarball files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment, or to install the infrastructure monitoring agent on distributions that lack the newrelic-infra package in their repositories. Important Note that custom installation of the infrastructure agent using tarball files is not officially supported. Update the agent Follow standard procedures to update the infrastructure monitoring agent. If you are using sudo to install or update the agent, use the -E argument to allow bypassing the environment variables, or specify the NRIA_MODE environment variable just after sudo. export NRIA_MODE=\"SET_MODE_HERE\" Copy OR sudo -E YOUR_PACKAGE_MANAGER_UPDATE_COMMAND Copy What's next? Generate some traffic and wait a few minutes, then view your hosts in the New Relic One UI. If necessary, follow our troubleshooting procedures if no data appears. Important The hostname for your server cannot be localhost. Data will not be reported for servers with that name. Make sure the host name uses a unique name. The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 265.015,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "-by-step instructions If guided <em>install</em> doesn&#x27;t work, you can <em>install</em> the <em>agent</em> manually. Before installing <em>infrastructure</em>, be sure to: Review the requirements. Have a valid New Relic license key. To <em>install</em> <em>infrastructure</em> in <em>Linux</em>, follow these instructions: Create the configuration file and add your"
      },
      "id": "6043edce64441f5335378f15"
    },
    {
      "sections": [
        "Docker container for infrastructure monitoring",
        "What you need",
        "Custom setup (recommended)",
        "Docker CLI",
        "Docker Compose",
        "Basic setup",
        "Required container privileges",
        "Next steps after install",
        "Inventory collected",
        "Container data",
        "Containerized agent image",
        "Check the source code"
      ],
      "title": "Docker container for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "022f4fba474d662414d9542a107d4d8a30d24895",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring/",
      "published_at": "2022-01-08T12:34:17Z",
      "updated_at": "2021-08-02T23:06:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure monitoring agent for Linux supports Docker environments by default. If you're running a container OS or have restrictions that require deploying the agent as a container, you can run a containerized version of our infrastructure monitoring agent. This can monitor metrics for the container itself, as well as the underlying host. Using the custom (recommended) or basic setup allows the infrastructure agent to run inside a container environment. A host can only run one instance of the agent at a time, whether that's the containerized agent or the non-containerized version. What you need The containerized version of the infrastructure agent requires Docker 1.12 or higher. The container must run any of the Linux distributions and versions supported by our agent. The container image is available and supported on AMD64 and ARM64 architectures. The log forwarder is not included with the containerized agent. We recommend installing the agent on the underlying host which provides all capabilities. Custom setup (recommended) The following are basic instructions for creating a custom Docker image on Linux. This allows you to deploy the infrastructure agent as a container that can monitor its underlying host. Recommendation: Extend the newrelic/infrastructure image, and use your own newrelic-infra.yml agent config file. Once your image is built, you can easily spin up a container without having to provide more launch time configurations. Do not provide secrets using environment variables with Docker. Docker CLI Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. license_key: YOUR_LICENSE_KEY Copy Create the Dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Build and tag your image: docker build -t YOUR_IMAGE_NAME . Copy Run the container from the image you built with the required required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ YOUR_IMAGE_NAME Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create a folder to store the configuration files: mkdir ~/newrelic-infra-setup Copy Change directory to the one you've just created: cd ~/newrelic-infra-setup Copy Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. echo \"license_key: YOUR_LICENSE_KEY\" > newrelic-infra.yml Copy Create the newrelic-infra.dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: touch newrelic-infra.dockerfile Copy vim newrelic-infra.dockerfile #you can use any text editor Copy Put the following content in the file: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra build: context: . dockerfile: newrelic-infra.dockerfile cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Basic setup To use the basic setup with a base New Relic infrastructure image: Docker CLI Run the container with the required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ -e NRIA_LICENSE_KEY=YOUR_LICENSE_KEY \\ newrelic/infrastructure:latest Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra image: newrelic/infrastructure:latest cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" environment: NRIA_LICENSE_KEY: \"YOUR_LICENSE_KEY\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Required container privileges Due to resource isolation from the host and other containers via Linux namespaces, a container has a very restricted view and control of its underlying host's resources by default. Without these extra privileges, the infrastructure agent cannot monitor the host and its containers. The infrastructure agent collects data about its host using system files and system calls. For more information about how the infrastructure agent collects data, see our documentation about infrastructure monitoring and security. Required privileges include: Privilege Description --network=host Sets the container's network namespace to the host's network namespace. This allows the agent to collect the network metrics about the host. -v \"/:/host:ro\" Bind mounts the host's root volume to the container. This read-only access to the host's root allows the agent to collect process and storage metrics as well as Inventory data from the host. --cap-add=SYS_PTRACE Adds the Linux capability to trace system processes. This allows the agent to gather data about processes running on the host. Read more here. --privileged --pid=host -v \"/var/run/docker.sock:/var/run/docker.sock\" Bind mounts the host's Docker daemon socket to the container. This allows the agent to connect to the Engine API via the Docker daemon socket to collect the host's container data. Next steps after install For next steps after install is completed, see What's next? Inventory collected Inventory is collected from the infrastructure agent's built-in data collectors. The infrastructure agent collects this data for Linux systems running with containers. Category Source Data collected using metadata agent_config Agent's complete config file system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name Container data Once the infrastructure agent is running in a Docker container, it can collect the same host compute data and event data that the infrastructure agent is capable of collecting when running natively on a host. For more information, see our documentation about how to view your Docker container data. Containerized agent image The containerized agent image is built from an Alpine base image. A CentOS base image is also available. Alpine is used as the base image since version 0.0.55. This is the one pointed by latest tag. Earlier versions used CentOS 7 as base image. In order to keep using that legacy image, some backports may be included there. To fetch the latest CentOS 7 based image, point to the latest-centos tag. Check the source code This integration is open source software. You can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.85773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker container for <em>infrastructure</em> monitoring",
        "sections": "Docker container for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em> supports Docker environments by default. If you&#x27;re running a container OS or have restrictions that require deploying the <em>agent</em> as a container, you can run a containerized version of our <em>infrastructure</em> monitoring <em>agent</em>. This can monitor metrics"
      },
      "id": "6043ef6a28ccbce71b2c6062"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-07-27T11:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.29668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Linux",
        "Step-by-step instructions",
        "Debian",
        "Ubuntu",
        "Amazon Linux, CentOS, RHEL",
        "SLES",
        "Important",
        "Amazon Linux",
        "CentOS / RHEL & Oracle Linux",
        "Root (default)",
        "Privileged user",
        "Unprivileged user",
        "Install using configuration management tools",
        "Install for Docker containers on instrumented hosts",
        "Install using Azure extensions",
        "Install using tarball files",
        "Update the agent",
        "What's next?"
      ],
      "title": "Install the infrastructure monitoring agent for Linux",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "060512e99d6143e7a7e8e6d16ba96cdcc7534e57",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux/",
      "published_at": "2022-01-08T09:40:49Z",
      "updated_at": "2021-11-06T16:34:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The quickest way to get started with our infrastructure monitoring agent is through our guided install. If you're in the EU, try our EU guided install. Either way, you'll need a New Relic account if you don't have one. (It's free, forever.) Get an account Guided install EU guided install Step-by-step instructions If guided install doesn't work, you can install the agent manually. Before installing infrastructure, be sure to: Review the requirements. Have a valid New Relic license key. To install infrastructure in Linux, follow these instructions: Create the configuration file and add your license key: echo \"license_key: YOUR_LICENSE_KEY\" | sudo tee -a /etc/newrelic-infra.yml Copy Determine the distribution version number: Debian cat /etc/os-release Copy Ubuntu cat /etc/lsb-release Copy Amazon Linux, CentOS, RHEL cat /etc/os-release Copy SLES cat /etc/os-release | grep VERSION_ID Copy Enable New Relic's GPG key. Debian curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Ubuntu curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Amazon Linux, CentOS, RHEL yum automatically installs the GPG key using the value in gpgkey. SLES curl https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg -s | sudo gpg --import Copy Important There's a known issue with the zypper package manager where GPG keys may not be validated as expected. If you get errors such as Signature verification failed, see New Relic's Explorers Hub for more information. Add the infrastructure monitoring agent repository: Debian Debian 8 (\"Jessie\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt jessie main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 9 (\"Stretch\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt stretch main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 10 (\"Buster\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt buster main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 11 (\"Bullseye\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bullseye main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu Ubuntu 16 (\"Xenial\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 18 (\"Bionic\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bionic main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20 (\"Focal\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt focal main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20.10 (\"Groovy\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt groovy main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 21.04 (\"Hirsute Hippo\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt hirsute main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Amazon Linux Amazon Linux 2 (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64/newrelic-infra.repo Copy Amazon Linux 2 (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/aarch64/newrelic-infra.repo Copy CentOS / RHEL & Oracle Linux CentOS, RHEL, Oracle Linux 7.x (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo Copy CentOS RHEL, Oracle Linux 7.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/aarch64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/x86_64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux 8.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/aarch64/newrelic-infra.repo Copy SLES SLES 11.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/11.4/x86_64/newrelic-infra.repo Copy SLES 12.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/x86_64/newrelic-infra.repo Copy SLES 12.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/aarch64/newrelic-infra.repo Copy SLES 12.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/x86_64/newrelic-infra.repo Copy SLES 12.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/aarch64/newrelic-infra.repo Copy SLES 12.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/x86_64/newrelic-infra.repo Copy SLES 12.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/aarch64/newrelic-infra.repo Copy SLES 12.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/x86_64/newrelic-infra.repo Copy SLES 12.4 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/aarch64/newrelic-infra.repo Copy SLES 12.5 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/x86_64/newrelic-infra.repo Copy SLES 12.5 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/aarch64/newrelic-infra.repo Copy SLES 15.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/x86_64/newrelic-infra.repo Copy SLES 15.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/aarch64/newrelic-infra.repo Copy SLES 15.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/x86_64/newrelic-infra.repo Copy SLES 15.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/aarch64/newrelic-infra.repo Copy SLES 15.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/x86_64/newrelic-infra.repo Copy SLES 15.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/aarch64/newrelic-infra.repo Copy Refresh the repositories: Debian sudo apt-get update Copy Ubuntu sudo apt-get update Copy Amazon Linux, CentOS, RHEL sudo yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' Copy SLES sudo zypper -n ref -r newrelic-infra Copy Install the newrelic-infra package in root (default), privileged user, or unprivileged user mode. For more information on each running mode, see Linux agent running modes. Root (default) Debian and Ubuntu: sudo apt-get install newrelic-infra -y Copy Amazon Linux, CentOS, RHEL: sudo yum install newrelic-infra -y Copy SLES: sudo zypper -n install newrelic-infra Copy Privileged user Install the libcap library and set the NRIA_MODE environment variable to PRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"PRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"PRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"PRIVILEGED\" zypper install newrelic-infra Copy Unprivileged user Install the libcap library and set the NRIA_MODE environment variable to UNPRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"UNPRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"UNPRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"UNPRIVILEGED\" zypper install newrelic-infra Copy Once the infrastructure monitoring agent is installed or updated, you can start, stop, or check the agent status. Important As of version 1.4.0, the infrastructure monitoring agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. Install using configuration management tools To install the infrastructure monitoring agent with a configuration management tool, see the documentation for: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Install for Docker containers on instrumented hosts See Docker instrumentation for infrastructure monitoring. Install using Azure extensions See Azure extensions for infrastructure monitoring. Install using tarball files For custom setup scenarios, you can install the infrastructure monitoring agent using our tarball files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment, or to install the infrastructure monitoring agent on distributions that lack the newrelic-infra package in their repositories. Important Note that custom installation of the infrastructure agent using tarball files is not officially supported. Update the agent Follow standard procedures to update the infrastructure monitoring agent. If you are using sudo to install or update the agent, use the -E argument to allow bypassing the environment variables, or specify the NRIA_MODE environment variable just after sudo. export NRIA_MODE=\"SET_MODE_HERE\" Copy OR sudo -E YOUR_PACKAGE_MANAGER_UPDATE_COMMAND Copy What's next? Generate some traffic and wait a few minutes, then view your hosts in the New Relic One UI. If necessary, follow our troubleshooting procedures if no data appears. Important The hostname for your server cannot be localhost. Data will not be reported for servers with that name. Make sure the host name uses a unique name. The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 265.01498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "-by-step instructions If guided <em>install</em> doesn&#x27;t work, you can <em>install</em> the <em>agent</em> manually. Before installing <em>infrastructure</em>, be sure to: Review the requirements. Have a valid New Relic license key. To <em>install</em> <em>infrastructure</em> in <em>Linux</em>, follow these instructions: Create the configuration file and add your"
      },
      "id": "6043edce64441f5335378f15"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-07-27T11:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.29668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2022-01-08T09:09:08Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.99733,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Linux",
        "Step-by-step instructions",
        "Debian",
        "Ubuntu",
        "Amazon Linux, CentOS, RHEL",
        "SLES",
        "Important",
        "Amazon Linux",
        "CentOS / RHEL & Oracle Linux",
        "Root (default)",
        "Privileged user",
        "Unprivileged user",
        "Install using configuration management tools",
        "Install for Docker containers on instrumented hosts",
        "Install using Azure extensions",
        "Install using tarball files",
        "Update the agent",
        "What's next?"
      ],
      "title": "Install the infrastructure monitoring agent for Linux",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "060512e99d6143e7a7e8e6d16ba96cdcc7534e57",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux/",
      "published_at": "2022-01-08T09:40:49Z",
      "updated_at": "2021-11-06T16:34:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The quickest way to get started with our infrastructure monitoring agent is through our guided install. If you're in the EU, try our EU guided install. Either way, you'll need a New Relic account if you don't have one. (It's free, forever.) Get an account Guided install EU guided install Step-by-step instructions If guided install doesn't work, you can install the agent manually. Before installing infrastructure, be sure to: Review the requirements. Have a valid New Relic license key. To install infrastructure in Linux, follow these instructions: Create the configuration file and add your license key: echo \"license_key: YOUR_LICENSE_KEY\" | sudo tee -a /etc/newrelic-infra.yml Copy Determine the distribution version number: Debian cat /etc/os-release Copy Ubuntu cat /etc/lsb-release Copy Amazon Linux, CentOS, RHEL cat /etc/os-release Copy SLES cat /etc/os-release | grep VERSION_ID Copy Enable New Relic's GPG key. Debian curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Ubuntu curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Amazon Linux, CentOS, RHEL yum automatically installs the GPG key using the value in gpgkey. SLES curl https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg -s | sudo gpg --import Copy Important There's a known issue with the zypper package manager where GPG keys may not be validated as expected. If you get errors such as Signature verification failed, see New Relic's Explorers Hub for more information. Add the infrastructure monitoring agent repository: Debian Debian 8 (\"Jessie\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt jessie main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 9 (\"Stretch\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt stretch main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 10 (\"Buster\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt buster main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 11 (\"Bullseye\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bullseye main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu Ubuntu 16 (\"Xenial\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 18 (\"Bionic\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bionic main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20 (\"Focal\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt focal main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20.10 (\"Groovy\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt groovy main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 21.04 (\"Hirsute Hippo\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt hirsute main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Amazon Linux Amazon Linux 2 (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64/newrelic-infra.repo Copy Amazon Linux 2 (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/aarch64/newrelic-infra.repo Copy CentOS / RHEL & Oracle Linux CentOS, RHEL, Oracle Linux 7.x (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo Copy CentOS RHEL, Oracle Linux 7.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/aarch64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/x86_64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux 8.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/aarch64/newrelic-infra.repo Copy SLES SLES 11.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/11.4/x86_64/newrelic-infra.repo Copy SLES 12.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/x86_64/newrelic-infra.repo Copy SLES 12.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/aarch64/newrelic-infra.repo Copy SLES 12.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/x86_64/newrelic-infra.repo Copy SLES 12.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/aarch64/newrelic-infra.repo Copy SLES 12.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/x86_64/newrelic-infra.repo Copy SLES 12.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/aarch64/newrelic-infra.repo Copy SLES 12.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/x86_64/newrelic-infra.repo Copy SLES 12.4 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/aarch64/newrelic-infra.repo Copy SLES 12.5 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/x86_64/newrelic-infra.repo Copy SLES 12.5 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/aarch64/newrelic-infra.repo Copy SLES 15.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/x86_64/newrelic-infra.repo Copy SLES 15.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/aarch64/newrelic-infra.repo Copy SLES 15.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/x86_64/newrelic-infra.repo Copy SLES 15.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/aarch64/newrelic-infra.repo Copy SLES 15.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/x86_64/newrelic-infra.repo Copy SLES 15.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/aarch64/newrelic-infra.repo Copy Refresh the repositories: Debian sudo apt-get update Copy Ubuntu sudo apt-get update Copy Amazon Linux, CentOS, RHEL sudo yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' Copy SLES sudo zypper -n ref -r newrelic-infra Copy Install the newrelic-infra package in root (default), privileged user, or unprivileged user mode. For more information on each running mode, see Linux agent running modes. Root (default) Debian and Ubuntu: sudo apt-get install newrelic-infra -y Copy Amazon Linux, CentOS, RHEL: sudo yum install newrelic-infra -y Copy SLES: sudo zypper -n install newrelic-infra Copy Privileged user Install the libcap library and set the NRIA_MODE environment variable to PRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"PRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"PRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"PRIVILEGED\" zypper install newrelic-infra Copy Unprivileged user Install the libcap library and set the NRIA_MODE environment variable to UNPRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"UNPRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"UNPRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"UNPRIVILEGED\" zypper install newrelic-infra Copy Once the infrastructure monitoring agent is installed or updated, you can start, stop, or check the agent status. Important As of version 1.4.0, the infrastructure monitoring agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. Install using configuration management tools To install the infrastructure monitoring agent with a configuration management tool, see the documentation for: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Install for Docker containers on instrumented hosts See Docker instrumentation for infrastructure monitoring. Install using Azure extensions See Azure extensions for infrastructure monitoring. Install using tarball files For custom setup scenarios, you can install the infrastructure monitoring agent using our tarball files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment, or to install the infrastructure monitoring agent on distributions that lack the newrelic-infra package in their repositories. Important Note that custom installation of the infrastructure agent using tarball files is not officially supported. Update the agent Follow standard procedures to update the infrastructure monitoring agent. If you are using sudo to install or update the agent, use the -E argument to allow bypassing the environment variables, or specify the NRIA_MODE environment variable just after sudo. export NRIA_MODE=\"SET_MODE_HERE\" Copy OR sudo -E YOUR_PACKAGE_MANAGER_UPDATE_COMMAND Copy What's next? Generate some traffic and wait a few minutes, then view your hosts in the New Relic One UI. If necessary, follow our troubleshooting procedures if no data appears. Important The hostname for your server cannot be localhost. Data will not be reported for servers with that name. Make sure the host name uses a unique name. The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 265.01498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "-by-step instructions If guided <em>install</em> doesn&#x27;t work, you can <em>install</em> the <em>agent</em> manually. Before installing <em>infrastructure</em>, be sure to: Review the requirements. Have a valid New Relic license key. To <em>install</em> <em>infrastructure</em> in <em>Linux</em>, follow these instructions: Create the configuration file and add your"
      },
      "id": "6043edce64441f5335378f15"
    },
    {
      "sections": [
        "Docker container for infrastructure monitoring",
        "What you need",
        "Custom setup (recommended)",
        "Docker CLI",
        "Docker Compose",
        "Basic setup",
        "Required container privileges",
        "Next steps after install",
        "Inventory collected",
        "Container data",
        "Containerized agent image",
        "Check the source code"
      ],
      "title": "Docker container for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "022f4fba474d662414d9542a107d4d8a30d24895",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring/",
      "published_at": "2022-01-08T12:34:17Z",
      "updated_at": "2021-08-02T23:06:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure monitoring agent for Linux supports Docker environments by default. If you're running a container OS or have restrictions that require deploying the agent as a container, you can run a containerized version of our infrastructure monitoring agent. This can monitor metrics for the container itself, as well as the underlying host. Using the custom (recommended) or basic setup allows the infrastructure agent to run inside a container environment. A host can only run one instance of the agent at a time, whether that's the containerized agent or the non-containerized version. What you need The containerized version of the infrastructure agent requires Docker 1.12 or higher. The container must run any of the Linux distributions and versions supported by our agent. The container image is available and supported on AMD64 and ARM64 architectures. The log forwarder is not included with the containerized agent. We recommend installing the agent on the underlying host which provides all capabilities. Custom setup (recommended) The following are basic instructions for creating a custom Docker image on Linux. This allows you to deploy the infrastructure agent as a container that can monitor its underlying host. Recommendation: Extend the newrelic/infrastructure image, and use your own newrelic-infra.yml agent config file. Once your image is built, you can easily spin up a container without having to provide more launch time configurations. Do not provide secrets using environment variables with Docker. Docker CLI Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. license_key: YOUR_LICENSE_KEY Copy Create the Dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Build and tag your image: docker build -t YOUR_IMAGE_NAME . Copy Run the container from the image you built with the required required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ YOUR_IMAGE_NAME Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create a folder to store the configuration files: mkdir ~/newrelic-infra-setup Copy Change directory to the one you've just created: cd ~/newrelic-infra-setup Copy Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. echo \"license_key: YOUR_LICENSE_KEY\" > newrelic-infra.yml Copy Create the newrelic-infra.dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: touch newrelic-infra.dockerfile Copy vim newrelic-infra.dockerfile #you can use any text editor Copy Put the following content in the file: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra build: context: . dockerfile: newrelic-infra.dockerfile cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Basic setup To use the basic setup with a base New Relic infrastructure image: Docker CLI Run the container with the required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ -e NRIA_LICENSE_KEY=YOUR_LICENSE_KEY \\ newrelic/infrastructure:latest Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra image: newrelic/infrastructure:latest cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" environment: NRIA_LICENSE_KEY: \"YOUR_LICENSE_KEY\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Required container privileges Due to resource isolation from the host and other containers via Linux namespaces, a container has a very restricted view and control of its underlying host's resources by default. Without these extra privileges, the infrastructure agent cannot monitor the host and its containers. The infrastructure agent collects data about its host using system files and system calls. For more information about how the infrastructure agent collects data, see our documentation about infrastructure monitoring and security. Required privileges include: Privilege Description --network=host Sets the container's network namespace to the host's network namespace. This allows the agent to collect the network metrics about the host. -v \"/:/host:ro\" Bind mounts the host's root volume to the container. This read-only access to the host's root allows the agent to collect process and storage metrics as well as Inventory data from the host. --cap-add=SYS_PTRACE Adds the Linux capability to trace system processes. This allows the agent to gather data about processes running on the host. Read more here. --privileged --pid=host -v \"/var/run/docker.sock:/var/run/docker.sock\" Bind mounts the host's Docker daemon socket to the container. This allows the agent to connect to the Engine API via the Docker daemon socket to collect the host's container data. Next steps after install For next steps after install is completed, see What's next? Inventory collected Inventory is collected from the infrastructure agent's built-in data collectors. The infrastructure agent collects this data for Linux systems running with containers. Category Source Data collected using metadata agent_config Agent's complete config file system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name Container data Once the infrastructure agent is running in a Docker container, it can collect the same host compute data and event data that the infrastructure agent is capable of collecting when running natively on a host. For more information, see our documentation about how to view your Docker container data. Containerized agent image The containerized agent image is built from an Alpine base image. A CentOS base image is also available. Alpine is used as the base image since version 0.0.55. This is the one pointed by latest tag. Earlier versions used CentOS 7 as base image. In order to keep using that legacy image, some backports may be included there. To fetch the latest CentOS 7 based image, point to the latest-centos tag. Check the source code This integration is open source software. You can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.85773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker container for <em>infrastructure</em> monitoring",
        "sections": "Docker container for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em> supports Docker environments by default. If you&#x27;re running a container OS or have restrictions that require deploying the <em>agent</em> as a container, you can run a containerized version of our <em>infrastructure</em> monitoring <em>agent</em>. This can monitor metrics"
      },
      "id": "6043ef6a28ccbce71b2c6062"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2022-01-08T09:09:08Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.99733,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux": [
    {
      "sections": [
        "Docker container for infrastructure monitoring",
        "What you need",
        "Custom setup (recommended)",
        "Docker CLI",
        "Docker Compose",
        "Basic setup",
        "Required container privileges",
        "Next steps after install",
        "Inventory collected",
        "Container data",
        "Containerized agent image",
        "Check the source code"
      ],
      "title": "Docker container for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "022f4fba474d662414d9542a107d4d8a30d24895",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring/",
      "published_at": "2022-01-08T12:34:17Z",
      "updated_at": "2021-08-02T23:06:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure monitoring agent for Linux supports Docker environments by default. If you're running a container OS or have restrictions that require deploying the agent as a container, you can run a containerized version of our infrastructure monitoring agent. This can monitor metrics for the container itself, as well as the underlying host. Using the custom (recommended) or basic setup allows the infrastructure agent to run inside a container environment. A host can only run one instance of the agent at a time, whether that's the containerized agent or the non-containerized version. What you need The containerized version of the infrastructure agent requires Docker 1.12 or higher. The container must run any of the Linux distributions and versions supported by our agent. The container image is available and supported on AMD64 and ARM64 architectures. The log forwarder is not included with the containerized agent. We recommend installing the agent on the underlying host which provides all capabilities. Custom setup (recommended) The following are basic instructions for creating a custom Docker image on Linux. This allows you to deploy the infrastructure agent as a container that can monitor its underlying host. Recommendation: Extend the newrelic/infrastructure image, and use your own newrelic-infra.yml agent config file. Once your image is built, you can easily spin up a container without having to provide more launch time configurations. Do not provide secrets using environment variables with Docker. Docker CLI Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. license_key: YOUR_LICENSE_KEY Copy Create the Dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Build and tag your image: docker build -t YOUR_IMAGE_NAME . Copy Run the container from the image you built with the required required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ YOUR_IMAGE_NAME Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create a folder to store the configuration files: mkdir ~/newrelic-infra-setup Copy Change directory to the one you've just created: cd ~/newrelic-infra-setup Copy Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. echo \"license_key: YOUR_LICENSE_KEY\" > newrelic-infra.yml Copy Create the newrelic-infra.dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: touch newrelic-infra.dockerfile Copy vim newrelic-infra.dockerfile #you can use any text editor Copy Put the following content in the file: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra build: context: . dockerfile: newrelic-infra.dockerfile cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Basic setup To use the basic setup with a base New Relic infrastructure image: Docker CLI Run the container with the required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ -e NRIA_LICENSE_KEY=YOUR_LICENSE_KEY \\ newrelic/infrastructure:latest Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra image: newrelic/infrastructure:latest cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" environment: NRIA_LICENSE_KEY: \"YOUR_LICENSE_KEY\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Required container privileges Due to resource isolation from the host and other containers via Linux namespaces, a container has a very restricted view and control of its underlying host's resources by default. Without these extra privileges, the infrastructure agent cannot monitor the host and its containers. The infrastructure agent collects data about its host using system files and system calls. For more information about how the infrastructure agent collects data, see our documentation about infrastructure monitoring and security. Required privileges include: Privilege Description --network=host Sets the container's network namespace to the host's network namespace. This allows the agent to collect the network metrics about the host. -v \"/:/host:ro\" Bind mounts the host's root volume to the container. This read-only access to the host's root allows the agent to collect process and storage metrics as well as Inventory data from the host. --cap-add=SYS_PTRACE Adds the Linux capability to trace system processes. This allows the agent to gather data about processes running on the host. Read more here. --privileged --pid=host -v \"/var/run/docker.sock:/var/run/docker.sock\" Bind mounts the host's Docker daemon socket to the container. This allows the agent to connect to the Engine API via the Docker daemon socket to collect the host's container data. Next steps after install For next steps after install is completed, see What's next? Inventory collected Inventory is collected from the infrastructure agent's built-in data collectors. The infrastructure agent collects this data for Linux systems running with containers. Category Source Data collected using metadata agent_config Agent's complete config file system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name Container data Once the infrastructure agent is running in a Docker container, it can collect the same host compute data and event data that the infrastructure agent is capable of collecting when running natively on a host. For more information, see our documentation about how to view your Docker container data. Containerized agent image The containerized agent image is built from an Alpine base image. A CentOS base image is also available. Alpine is used as the base image since version 0.0.55. This is the one pointed by latest tag. Earlier versions used CentOS 7 as base image. In order to keep using that legacy image, some backports may be included there. To fetch the latest CentOS 7 based image, point to the latest-centos tag. Check the source code This integration is open source software. You can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.85773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker container for <em>infrastructure</em> monitoring",
        "sections": "Docker container for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em> supports Docker environments by default. If you&#x27;re running a container OS or have restrictions that require deploying the <em>agent</em> as a container, you can run a containerized version of our <em>infrastructure</em> monitoring <em>agent</em>. This can monitor metrics"
      },
      "id": "6043ef6a28ccbce71b2c6062"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-07-27T11:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.29668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2022-01-08T09:09:08Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.99733,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Linux",
        "Step-by-step instructions",
        "Debian",
        "Ubuntu",
        "Amazon Linux, CentOS, RHEL",
        "SLES",
        "Important",
        "Amazon Linux",
        "CentOS / RHEL & Oracle Linux",
        "Root (default)",
        "Privileged user",
        "Unprivileged user",
        "Install using configuration management tools",
        "Install for Docker containers on instrumented hosts",
        "Install using Azure extensions",
        "Install using tarball files",
        "Update the agent",
        "What's next?"
      ],
      "title": "Install the infrastructure monitoring agent for Linux",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "060512e99d6143e7a7e8e6d16ba96cdcc7534e57",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux/",
      "published_at": "2022-01-08T09:40:49Z",
      "updated_at": "2021-11-06T16:34:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The quickest way to get started with our infrastructure monitoring agent is through our guided install. If you're in the EU, try our EU guided install. Either way, you'll need a New Relic account if you don't have one. (It's free, forever.) Get an account Guided install EU guided install Step-by-step instructions If guided install doesn't work, you can install the agent manually. Before installing infrastructure, be sure to: Review the requirements. Have a valid New Relic license key. To install infrastructure in Linux, follow these instructions: Create the configuration file and add your license key: echo \"license_key: YOUR_LICENSE_KEY\" | sudo tee -a /etc/newrelic-infra.yml Copy Determine the distribution version number: Debian cat /etc/os-release Copy Ubuntu cat /etc/lsb-release Copy Amazon Linux, CentOS, RHEL cat /etc/os-release Copy SLES cat /etc/os-release | grep VERSION_ID Copy Enable New Relic's GPG key. Debian curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Ubuntu curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Amazon Linux, CentOS, RHEL yum automatically installs the GPG key using the value in gpgkey. SLES curl https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg -s | sudo gpg --import Copy Important There's a known issue with the zypper package manager where GPG keys may not be validated as expected. If you get errors such as Signature verification failed, see New Relic's Explorers Hub for more information. Add the infrastructure monitoring agent repository: Debian Debian 8 (\"Jessie\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt jessie main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 9 (\"Stretch\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt stretch main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 10 (\"Buster\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt buster main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 11 (\"Bullseye\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bullseye main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu Ubuntu 16 (\"Xenial\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 18 (\"Bionic\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bionic main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20 (\"Focal\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt focal main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20.10 (\"Groovy\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt groovy main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 21.04 (\"Hirsute Hippo\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt hirsute main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Amazon Linux Amazon Linux 2 (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64/newrelic-infra.repo Copy Amazon Linux 2 (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/aarch64/newrelic-infra.repo Copy CentOS / RHEL & Oracle Linux CentOS, RHEL, Oracle Linux 7.x (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo Copy CentOS RHEL, Oracle Linux 7.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/aarch64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/x86_64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux 8.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/aarch64/newrelic-infra.repo Copy SLES SLES 11.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/11.4/x86_64/newrelic-infra.repo Copy SLES 12.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/x86_64/newrelic-infra.repo Copy SLES 12.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/aarch64/newrelic-infra.repo Copy SLES 12.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/x86_64/newrelic-infra.repo Copy SLES 12.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/aarch64/newrelic-infra.repo Copy SLES 12.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/x86_64/newrelic-infra.repo Copy SLES 12.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/aarch64/newrelic-infra.repo Copy SLES 12.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/x86_64/newrelic-infra.repo Copy SLES 12.4 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/aarch64/newrelic-infra.repo Copy SLES 12.5 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/x86_64/newrelic-infra.repo Copy SLES 12.5 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/aarch64/newrelic-infra.repo Copy SLES 15.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/x86_64/newrelic-infra.repo Copy SLES 15.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/aarch64/newrelic-infra.repo Copy SLES 15.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/x86_64/newrelic-infra.repo Copy SLES 15.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/aarch64/newrelic-infra.repo Copy SLES 15.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/x86_64/newrelic-infra.repo Copy SLES 15.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/aarch64/newrelic-infra.repo Copy Refresh the repositories: Debian sudo apt-get update Copy Ubuntu sudo apt-get update Copy Amazon Linux, CentOS, RHEL sudo yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' Copy SLES sudo zypper -n ref -r newrelic-infra Copy Install the newrelic-infra package in root (default), privileged user, or unprivileged user mode. For more information on each running mode, see Linux agent running modes. Root (default) Debian and Ubuntu: sudo apt-get install newrelic-infra -y Copy Amazon Linux, CentOS, RHEL: sudo yum install newrelic-infra -y Copy SLES: sudo zypper -n install newrelic-infra Copy Privileged user Install the libcap library and set the NRIA_MODE environment variable to PRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"PRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"PRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"PRIVILEGED\" zypper install newrelic-infra Copy Unprivileged user Install the libcap library and set the NRIA_MODE environment variable to UNPRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"UNPRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"UNPRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"UNPRIVILEGED\" zypper install newrelic-infra Copy Once the infrastructure monitoring agent is installed or updated, you can start, stop, or check the agent status. Important As of version 1.4.0, the infrastructure monitoring agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. Install using configuration management tools To install the infrastructure monitoring agent with a configuration management tool, see the documentation for: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Install for Docker containers on instrumented hosts See Docker instrumentation for infrastructure monitoring. Install using Azure extensions See Azure extensions for infrastructure monitoring. Install using tarball files For custom setup scenarios, you can install the infrastructure monitoring agent using our tarball files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment, or to install the infrastructure monitoring agent on distributions that lack the newrelic-infra package in their repositories. Important Note that custom installation of the infrastructure agent using tarball files is not officially supported. Update the agent Follow standard procedures to update the infrastructure monitoring agent. If you are using sudo to install or update the agent, use the -E argument to allow bypassing the environment variables, or specify the NRIA_MODE environment variable just after sudo. export NRIA_MODE=\"SET_MODE_HERE\" Copy OR sudo -E YOUR_PACKAGE_MANAGER_UPDATE_COMMAND Copy What's next? Generate some traffic and wait a few minutes, then view your hosts in the New Relic One UI. If necessary, follow our troubleshooting procedures if no data appears. Important The hostname for your server cannot be localhost. Data will not be reported for servers with that name. Make sure the host name uses a unique name. The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 265.01498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "-by-step instructions If guided <em>install</em> doesn&#x27;t work, you can <em>install</em> the <em>agent</em> manually. Before installing <em>infrastructure</em>, be sure to: Review the requirements. Have a valid New Relic license key. To <em>install</em> <em>infrastructure</em> in <em>Linux</em>, follow these instructions: Create the configuration file and add your"
      },
      "id": "6043edce64441f5335378f15"
    },
    {
      "sections": [
        "Docker container for infrastructure monitoring",
        "What you need",
        "Custom setup (recommended)",
        "Docker CLI",
        "Docker Compose",
        "Basic setup",
        "Required container privileges",
        "Next steps after install",
        "Inventory collected",
        "Container data",
        "Containerized agent image",
        "Check the source code"
      ],
      "title": "Docker container for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "022f4fba474d662414d9542a107d4d8a30d24895",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring/",
      "published_at": "2022-01-08T12:34:17Z",
      "updated_at": "2021-08-02T23:06:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure monitoring agent for Linux supports Docker environments by default. If you're running a container OS or have restrictions that require deploying the agent as a container, you can run a containerized version of our infrastructure monitoring agent. This can monitor metrics for the container itself, as well as the underlying host. Using the custom (recommended) or basic setup allows the infrastructure agent to run inside a container environment. A host can only run one instance of the agent at a time, whether that's the containerized agent or the non-containerized version. What you need The containerized version of the infrastructure agent requires Docker 1.12 or higher. The container must run any of the Linux distributions and versions supported by our agent. The container image is available and supported on AMD64 and ARM64 architectures. The log forwarder is not included with the containerized agent. We recommend installing the agent on the underlying host which provides all capabilities. Custom setup (recommended) The following are basic instructions for creating a custom Docker image on Linux. This allows you to deploy the infrastructure agent as a container that can monitor its underlying host. Recommendation: Extend the newrelic/infrastructure image, and use your own newrelic-infra.yml agent config file. Once your image is built, you can easily spin up a container without having to provide more launch time configurations. Do not provide secrets using environment variables with Docker. Docker CLI Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. license_key: YOUR_LICENSE_KEY Copy Create the Dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Build and tag your image: docker build -t YOUR_IMAGE_NAME . Copy Run the container from the image you built with the required required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ YOUR_IMAGE_NAME Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create a folder to store the configuration files: mkdir ~/newrelic-infra-setup Copy Change directory to the one you've just created: cd ~/newrelic-infra-setup Copy Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. echo \"license_key: YOUR_LICENSE_KEY\" > newrelic-infra.yml Copy Create the newrelic-infra.dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: touch newrelic-infra.dockerfile Copy vim newrelic-infra.dockerfile #you can use any text editor Copy Put the following content in the file: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra build: context: . dockerfile: newrelic-infra.dockerfile cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Basic setup To use the basic setup with a base New Relic infrastructure image: Docker CLI Run the container with the required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ -e NRIA_LICENSE_KEY=YOUR_LICENSE_KEY \\ newrelic/infrastructure:latest Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra image: newrelic/infrastructure:latest cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" environment: NRIA_LICENSE_KEY: \"YOUR_LICENSE_KEY\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Required container privileges Due to resource isolation from the host and other containers via Linux namespaces, a container has a very restricted view and control of its underlying host's resources by default. Without these extra privileges, the infrastructure agent cannot monitor the host and its containers. The infrastructure agent collects data about its host using system files and system calls. For more information about how the infrastructure agent collects data, see our documentation about infrastructure monitoring and security. Required privileges include: Privilege Description --network=host Sets the container's network namespace to the host's network namespace. This allows the agent to collect the network metrics about the host. -v \"/:/host:ro\" Bind mounts the host's root volume to the container. This read-only access to the host's root allows the agent to collect process and storage metrics as well as Inventory data from the host. --cap-add=SYS_PTRACE Adds the Linux capability to trace system processes. This allows the agent to gather data about processes running on the host. Read more here. --privileged --pid=host -v \"/var/run/docker.sock:/var/run/docker.sock\" Bind mounts the host's Docker daemon socket to the container. This allows the agent to connect to the Engine API via the Docker daemon socket to collect the host's container data. Next steps after install For next steps after install is completed, see What's next? Inventory collected Inventory is collected from the infrastructure agent's built-in data collectors. The infrastructure agent collects this data for Linux systems running with containers. Category Source Data collected using metadata agent_config Agent's complete config file system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name Container data Once the infrastructure agent is running in a Docker container, it can collect the same host compute data and event data that the infrastructure agent is capable of collecting when running natively on a host. For more information, see our documentation about how to view your Docker container data. Containerized agent image The containerized agent image is built from an Alpine base image. A CentOS base image is also available. Alpine is used as the base image since version 0.0.55. This is the one pointed by latest tag. Earlier versions used CentOS 7 as base image. In order to keep using that legacy image, some backports may be included there. To fetch the latest CentOS 7 based image, point to the latest-centos tag. Check the source code This integration is open source software. You can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.85773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker container for <em>infrastructure</em> monitoring",
        "sections": "Docker container for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em> supports Docker environments by default. If you&#x27;re running a container OS or have restrictions that require deploying the <em>agent</em> as a container, you can run a containerized version of our <em>infrastructure</em> monitoring <em>agent</em>. This can monitor metrics"
      },
      "id": "6043ef6a28ccbce71b2c6062"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-07-27T11:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.29666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Linux",
        "Step-by-step instructions",
        "Debian",
        "Ubuntu",
        "Amazon Linux, CentOS, RHEL",
        "SLES",
        "Important",
        "Amazon Linux",
        "CentOS / RHEL & Oracle Linux",
        "Root (default)",
        "Privileged user",
        "Unprivileged user",
        "Install using configuration management tools",
        "Install for Docker containers on instrumented hosts",
        "Install using Azure extensions",
        "Install using tarball files",
        "Update the agent",
        "What's next?"
      ],
      "title": "Install the infrastructure monitoring agent for Linux",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "060512e99d6143e7a7e8e6d16ba96cdcc7534e57",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux/",
      "published_at": "2022-01-08T09:40:49Z",
      "updated_at": "2021-11-06T16:34:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The quickest way to get started with our infrastructure monitoring agent is through our guided install. If you're in the EU, try our EU guided install. Either way, you'll need a New Relic account if you don't have one. (It's free, forever.) Get an account Guided install EU guided install Step-by-step instructions If guided install doesn't work, you can install the agent manually. Before installing infrastructure, be sure to: Review the requirements. Have a valid New Relic license key. To install infrastructure in Linux, follow these instructions: Create the configuration file and add your license key: echo \"license_key: YOUR_LICENSE_KEY\" | sudo tee -a /etc/newrelic-infra.yml Copy Determine the distribution version number: Debian cat /etc/os-release Copy Ubuntu cat /etc/lsb-release Copy Amazon Linux, CentOS, RHEL cat /etc/os-release Copy SLES cat /etc/os-release | grep VERSION_ID Copy Enable New Relic's GPG key. Debian curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Ubuntu curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Amazon Linux, CentOS, RHEL yum automatically installs the GPG key using the value in gpgkey. SLES curl https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg -s | sudo gpg --import Copy Important There's a known issue with the zypper package manager where GPG keys may not be validated as expected. If you get errors such as Signature verification failed, see New Relic's Explorers Hub for more information. Add the infrastructure monitoring agent repository: Debian Debian 8 (\"Jessie\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt jessie main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 9 (\"Stretch\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt stretch main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 10 (\"Buster\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt buster main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 11 (\"Bullseye\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bullseye main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu Ubuntu 16 (\"Xenial\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 18 (\"Bionic\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bionic main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20 (\"Focal\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt focal main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20.10 (\"Groovy\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt groovy main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 21.04 (\"Hirsute Hippo\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt hirsute main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Amazon Linux Amazon Linux 2 (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64/newrelic-infra.repo Copy Amazon Linux 2 (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/aarch64/newrelic-infra.repo Copy CentOS / RHEL & Oracle Linux CentOS, RHEL, Oracle Linux 7.x (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo Copy CentOS RHEL, Oracle Linux 7.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/aarch64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/x86_64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux 8.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/aarch64/newrelic-infra.repo Copy SLES SLES 11.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/11.4/x86_64/newrelic-infra.repo Copy SLES 12.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/x86_64/newrelic-infra.repo Copy SLES 12.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/aarch64/newrelic-infra.repo Copy SLES 12.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/x86_64/newrelic-infra.repo Copy SLES 12.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/aarch64/newrelic-infra.repo Copy SLES 12.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/x86_64/newrelic-infra.repo Copy SLES 12.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/aarch64/newrelic-infra.repo Copy SLES 12.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/x86_64/newrelic-infra.repo Copy SLES 12.4 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/aarch64/newrelic-infra.repo Copy SLES 12.5 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/x86_64/newrelic-infra.repo Copy SLES 12.5 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/aarch64/newrelic-infra.repo Copy SLES 15.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/x86_64/newrelic-infra.repo Copy SLES 15.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/aarch64/newrelic-infra.repo Copy SLES 15.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/x86_64/newrelic-infra.repo Copy SLES 15.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/aarch64/newrelic-infra.repo Copy SLES 15.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/x86_64/newrelic-infra.repo Copy SLES 15.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/aarch64/newrelic-infra.repo Copy Refresh the repositories: Debian sudo apt-get update Copy Ubuntu sudo apt-get update Copy Amazon Linux, CentOS, RHEL sudo yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' Copy SLES sudo zypper -n ref -r newrelic-infra Copy Install the newrelic-infra package in root (default), privileged user, or unprivileged user mode. For more information on each running mode, see Linux agent running modes. Root (default) Debian and Ubuntu: sudo apt-get install newrelic-infra -y Copy Amazon Linux, CentOS, RHEL: sudo yum install newrelic-infra -y Copy SLES: sudo zypper -n install newrelic-infra Copy Privileged user Install the libcap library and set the NRIA_MODE environment variable to PRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"PRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"PRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"PRIVILEGED\" zypper install newrelic-infra Copy Unprivileged user Install the libcap library and set the NRIA_MODE environment variable to UNPRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"UNPRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"UNPRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"UNPRIVILEGED\" zypper install newrelic-infra Copy Once the infrastructure monitoring agent is installed or updated, you can start, stop, or check the agent status. Important As of version 1.4.0, the infrastructure monitoring agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. Install using configuration management tools To install the infrastructure monitoring agent with a configuration management tool, see the documentation for: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Install for Docker containers on instrumented hosts See Docker instrumentation for infrastructure monitoring. Install using Azure extensions See Azure extensions for infrastructure monitoring. Install using tarball files For custom setup scenarios, you can install the infrastructure monitoring agent using our tarball files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment, or to install the infrastructure monitoring agent on distributions that lack the newrelic-infra package in their repositories. Important Note that custom installation of the infrastructure agent using tarball files is not officially supported. Update the agent Follow standard procedures to update the infrastructure monitoring agent. If you are using sudo to install or update the agent, use the -E argument to allow bypassing the environment variables, or specify the NRIA_MODE environment variable just after sudo. export NRIA_MODE=\"SET_MODE_HERE\" Copy OR sudo -E YOUR_PACKAGE_MANAGER_UPDATE_COMMAND Copy What's next? Generate some traffic and wait a few minutes, then view your hosts in the New Relic One UI. If necessary, follow our troubleshooting procedures if no data appears. Important The hostname for your server cannot be localhost. Data will not be reported for servers with that name. Make sure the host name uses a unique name. The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 265.01498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "-by-step instructions If guided <em>install</em> doesn&#x27;t work, you can <em>install</em> the <em>agent</em> manually. Before installing <em>infrastructure</em>, be sure to: Review the requirements. Have a valid New Relic license key. To <em>install</em> <em>infrastructure</em> in <em>Linux</em>, follow these instructions: Create the configuration file and add your"
      },
      "id": "6043edce64441f5335378f15"
    },
    {
      "sections": [
        "Docker container for infrastructure monitoring",
        "What you need",
        "Custom setup (recommended)",
        "Docker CLI",
        "Docker Compose",
        "Basic setup",
        "Required container privileges",
        "Next steps after install",
        "Inventory collected",
        "Container data",
        "Containerized agent image",
        "Check the source code"
      ],
      "title": "Docker container for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "022f4fba474d662414d9542a107d4d8a30d24895",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring/",
      "published_at": "2022-01-08T12:34:17Z",
      "updated_at": "2021-08-02T23:06:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure monitoring agent for Linux supports Docker environments by default. If you're running a container OS or have restrictions that require deploying the agent as a container, you can run a containerized version of our infrastructure monitoring agent. This can monitor metrics for the container itself, as well as the underlying host. Using the custom (recommended) or basic setup allows the infrastructure agent to run inside a container environment. A host can only run one instance of the agent at a time, whether that's the containerized agent or the non-containerized version. What you need The containerized version of the infrastructure agent requires Docker 1.12 or higher. The container must run any of the Linux distributions and versions supported by our agent. The container image is available and supported on AMD64 and ARM64 architectures. The log forwarder is not included with the containerized agent. We recommend installing the agent on the underlying host which provides all capabilities. Custom setup (recommended) The following are basic instructions for creating a custom Docker image on Linux. This allows you to deploy the infrastructure agent as a container that can monitor its underlying host. Recommendation: Extend the newrelic/infrastructure image, and use your own newrelic-infra.yml agent config file. Once your image is built, you can easily spin up a container without having to provide more launch time configurations. Do not provide secrets using environment variables with Docker. Docker CLI Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. license_key: YOUR_LICENSE_KEY Copy Create the Dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Build and tag your image: docker build -t YOUR_IMAGE_NAME . Copy Run the container from the image you built with the required required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ YOUR_IMAGE_NAME Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create a folder to store the configuration files: mkdir ~/newrelic-infra-setup Copy Change directory to the one you've just created: cd ~/newrelic-infra-setup Copy Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. echo \"license_key: YOUR_LICENSE_KEY\" > newrelic-infra.yml Copy Create the newrelic-infra.dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: touch newrelic-infra.dockerfile Copy vim newrelic-infra.dockerfile #you can use any text editor Copy Put the following content in the file: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra build: context: . dockerfile: newrelic-infra.dockerfile cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Basic setup To use the basic setup with a base New Relic infrastructure image: Docker CLI Run the container with the required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ -e NRIA_LICENSE_KEY=YOUR_LICENSE_KEY \\ newrelic/infrastructure:latest Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra image: newrelic/infrastructure:latest cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" environment: NRIA_LICENSE_KEY: \"YOUR_LICENSE_KEY\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Required container privileges Due to resource isolation from the host and other containers via Linux namespaces, a container has a very restricted view and control of its underlying host's resources by default. Without these extra privileges, the infrastructure agent cannot monitor the host and its containers. The infrastructure agent collects data about its host using system files and system calls. For more information about how the infrastructure agent collects data, see our documentation about infrastructure monitoring and security. Required privileges include: Privilege Description --network=host Sets the container's network namespace to the host's network namespace. This allows the agent to collect the network metrics about the host. -v \"/:/host:ro\" Bind mounts the host's root volume to the container. This read-only access to the host's root allows the agent to collect process and storage metrics as well as Inventory data from the host. --cap-add=SYS_PTRACE Adds the Linux capability to trace system processes. This allows the agent to gather data about processes running on the host. Read more here. --privileged --pid=host -v \"/var/run/docker.sock:/var/run/docker.sock\" Bind mounts the host's Docker daemon socket to the container. This allows the agent to connect to the Engine API via the Docker daemon socket to collect the host's container data. Next steps after install For next steps after install is completed, see What's next? Inventory collected Inventory is collected from the infrastructure agent's built-in data collectors. The infrastructure agent collects this data for Linux systems running with containers. Category Source Data collected using metadata agent_config Agent's complete config file system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name Container data Once the infrastructure agent is running in a Docker container, it can collect the same host compute data and event data that the infrastructure agent is capable of collecting when running natively on a host. For more information, see our documentation about how to view your Docker container data. Containerized agent image The containerized agent image is built from an Alpine base image. A CentOS base image is also available. Alpine is used as the base image since version 0.0.55. This is the one pointed by latest tag. Earlier versions used CentOS 7 as base image. In order to keep using that legacy image, some backports may be included there. To fetch the latest CentOS 7 based image, point to the latest-centos tag. Check the source code This integration is open source software. You can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.85773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker container for <em>infrastructure</em> monitoring",
        "sections": "Docker container for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em> supports Docker environments by default. If you&#x27;re running a container OS or have restrictions that require deploying the <em>agent</em> as a container, you can run a containerized version of our <em>infrastructure</em> monitoring <em>agent</em>. This can monitor metrics"
      },
      "id": "6043ef6a28ccbce71b2c6062"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-07-27T11:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.29666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Linux",
        "Step-by-step instructions",
        "Debian",
        "Ubuntu",
        "Amazon Linux, CentOS, RHEL",
        "SLES",
        "Important",
        "Amazon Linux",
        "CentOS / RHEL & Oracle Linux",
        "Root (default)",
        "Privileged user",
        "Unprivileged user",
        "Install using configuration management tools",
        "Install for Docker containers on instrumented hosts",
        "Install using Azure extensions",
        "Install using tarball files",
        "Update the agent",
        "What's next?"
      ],
      "title": "Install the infrastructure monitoring agent for Linux",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "060512e99d6143e7a7e8e6d16ba96cdcc7534e57",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux/",
      "published_at": "2022-01-08T09:40:49Z",
      "updated_at": "2021-11-06T16:34:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The quickest way to get started with our infrastructure monitoring agent is through our guided install. If you're in the EU, try our EU guided install. Either way, you'll need a New Relic account if you don't have one. (It's free, forever.) Get an account Guided install EU guided install Step-by-step instructions If guided install doesn't work, you can install the agent manually. Before installing infrastructure, be sure to: Review the requirements. Have a valid New Relic license key. To install infrastructure in Linux, follow these instructions: Create the configuration file and add your license key: echo \"license_key: YOUR_LICENSE_KEY\" | sudo tee -a /etc/newrelic-infra.yml Copy Determine the distribution version number: Debian cat /etc/os-release Copy Ubuntu cat /etc/lsb-release Copy Amazon Linux, CentOS, RHEL cat /etc/os-release Copy SLES cat /etc/os-release | grep VERSION_ID Copy Enable New Relic's GPG key. Debian curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Ubuntu curl -s https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg | sudo apt-key add - Copy Amazon Linux, CentOS, RHEL yum automatically installs the GPG key using the value in gpgkey. SLES curl https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg -s | sudo gpg --import Copy Important There's a known issue with the zypper package manager where GPG keys may not be validated as expected. If you get errors such as Signature verification failed, see New Relic's Explorers Hub for more information. Add the infrastructure monitoring agent repository: Debian Debian 8 (\"Jessie\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt jessie main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 9 (\"Stretch\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt stretch main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 10 (\"Buster\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt buster main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Debian 11 (\"Bullseye\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bullseye main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu Ubuntu 16 (\"Xenial\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt xenial main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 18 (\"Bionic\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt bionic main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20 (\"Focal\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt focal main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 20.10 (\"Groovy\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt groovy main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Ubuntu 21.04 (\"Hirsute Hippo\") printf \"deb https://download.newrelic.com/infrastructure_agent/linux/apt hirsute main\" | sudo tee -a /etc/apt/sources.list.d/newrelic-infra.list Copy Amazon Linux Amazon Linux 2 (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64/newrelic-infra.repo Copy Amazon Linux 2 (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/aarch64/newrelic-infra.repo Copy CentOS / RHEL & Oracle Linux CentOS, RHEL, Oracle Linux 7.x (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo Copy CentOS RHEL, Oracle Linux 7.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/aarch64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux (x86) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/x86_64/newrelic-infra.repo Copy CentOS, RHEL, Oracle Linux 8.x (arm64) sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/8/aarch64/newrelic-infra.repo Copy SLES SLES 11.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/11.4/x86_64/newrelic-infra.repo Copy SLES 12.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/x86_64/newrelic-infra.repo Copy SLES 12.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.1/aarch64/newrelic-infra.repo Copy SLES 12.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/x86_64/newrelic-infra.repo Copy SLES 12.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.2/aarch64/newrelic-infra.repo Copy SLES 12.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/x86_64/newrelic-infra.repo Copy SLES 12.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.3/aarch64/newrelic-infra.repo Copy SLES 12.4 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/x86_64/newrelic-infra.repo Copy SLES 12.4 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.4/aarch64/newrelic-infra.repo Copy SLES 12.5 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/x86_64/newrelic-infra.repo Copy SLES 12.5 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/12.5/aarch64/newrelic-infra.repo Copy SLES 15.1 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/x86_64/newrelic-infra.repo Copy SLES 15.1 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.1/aarch64/newrelic-infra.repo Copy SLES 15.2 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/x86_64/newrelic-infra.repo Copy SLES 15.2 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.2/aarch64/newrelic-infra.repo Copy SLES 15.3 (x86) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/x86_64/newrelic-infra.repo Copy SLES 15.3 (ARM) sudo curl -o /etc/zypp/repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/zypp/sles/15.3/aarch64/newrelic-infra.repo Copy Refresh the repositories: Debian sudo apt-get update Copy Ubuntu sudo apt-get update Copy Amazon Linux, CentOS, RHEL sudo yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' Copy SLES sudo zypper -n ref -r newrelic-infra Copy Install the newrelic-infra package in root (default), privileged user, or unprivileged user mode. For more information on each running mode, see Linux agent running modes. Root (default) Debian and Ubuntu: sudo apt-get install newrelic-infra -y Copy Amazon Linux, CentOS, RHEL: sudo yum install newrelic-infra -y Copy SLES: sudo zypper -n install newrelic-infra Copy Privileged user Install the libcap library and set the NRIA_MODE environment variable to PRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"PRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"PRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"PRIVILEGED\" zypper install newrelic-infra Copy Unprivileged user Install the libcap library and set the NRIA_MODE environment variable to UNPRIVILEGED. Debian and Ubuntu: sudo apt-get install libcap2-bin Copy sudo NRIA_MODE=\"UNPRIVILEGED\" apt-get install newrelic-infra Copy Amazon Linux, CentOS, RHEL: sudo yum install libcap Copy sudo NRIA_MODE=\"UNPRIVILEGED\" yum install newrelic-infra Copy SLES: sudo zypper install libcap-progs Copy sudo NRIA_MODE=\"UNPRIVILEGED\" zypper install newrelic-infra Copy Once the infrastructure monitoring agent is installed or updated, you can start, stop, or check the agent status. Important As of version 1.4.0, the infrastructure monitoring agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. Install using configuration management tools To install the infrastructure monitoring agent with a configuration management tool, see the documentation for: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Install for Docker containers on instrumented hosts See Docker instrumentation for infrastructure monitoring. Install using Azure extensions See Azure extensions for infrastructure monitoring. Install using tarball files For custom setup scenarios, you can install the infrastructure monitoring agent using our tarball files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment, or to install the infrastructure monitoring agent on distributions that lack the newrelic-infra package in their repositories. Important Note that custom installation of the infrastructure agent using tarball files is not officially supported. Update the agent Follow standard procedures to update the infrastructure monitoring agent. If you are using sudo to install or update the agent, use the -E argument to allow bypassing the environment variables, or specify the NRIA_MODE environment variable just after sudo. export NRIA_MODE=\"SET_MODE_HERE\" Copy OR sudo -E YOUR_PACKAGE_MANAGER_UPDATE_COMMAND Copy What's next? Generate some traffic and wait a few minutes, then view your hosts in the New Relic One UI. If necessary, follow our troubleshooting procedures if no data appears. Important The hostname for your server cannot be localhost. Data will not be reported for servers with that name. Make sure the host name uses a unique name. The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 265.01498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "-by-step instructions If guided <em>install</em> doesn&#x27;t work, you can <em>install</em> the <em>agent</em> manually. Before installing <em>infrastructure</em>, be sure to: Review the requirements. Have a valid New Relic license key. To <em>install</em> <em>infrastructure</em> in <em>Linux</em>, follow these instructions: Create the configuration file and add your"
      },
      "id": "6043edce64441f5335378f15"
    },
    {
      "sections": [
        "Docker container for infrastructure monitoring",
        "What you need",
        "Custom setup (recommended)",
        "Docker CLI",
        "Docker Compose",
        "Basic setup",
        "Required container privileges",
        "Next steps after install",
        "Inventory collected",
        "Container data",
        "Containerized agent image",
        "Check the source code"
      ],
      "title": "Docker container for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "022f4fba474d662414d9542a107d4d8a30d24895",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring/",
      "published_at": "2022-01-08T12:34:17Z",
      "updated_at": "2021-08-02T23:06:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure monitoring agent for Linux supports Docker environments by default. If you're running a container OS or have restrictions that require deploying the agent as a container, you can run a containerized version of our infrastructure monitoring agent. This can monitor metrics for the container itself, as well as the underlying host. Using the custom (recommended) or basic setup allows the infrastructure agent to run inside a container environment. A host can only run one instance of the agent at a time, whether that's the containerized agent or the non-containerized version. What you need The containerized version of the infrastructure agent requires Docker 1.12 or higher. The container must run any of the Linux distributions and versions supported by our agent. The container image is available and supported on AMD64 and ARM64 architectures. The log forwarder is not included with the containerized agent. We recommend installing the agent on the underlying host which provides all capabilities. Custom setup (recommended) The following are basic instructions for creating a custom Docker image on Linux. This allows you to deploy the infrastructure agent as a container that can monitor its underlying host. Recommendation: Extend the newrelic/infrastructure image, and use your own newrelic-infra.yml agent config file. Once your image is built, you can easily spin up a container without having to provide more launch time configurations. Do not provide secrets using environment variables with Docker. Docker CLI Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. license_key: YOUR_LICENSE_KEY Copy Create the Dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Build and tag your image: docker build -t YOUR_IMAGE_NAME . Copy Run the container from the image you built with the required required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ YOUR_IMAGE_NAME Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create a folder to store the configuration files: mkdir ~/newrelic-infra-setup Copy Change directory to the one you've just created: cd ~/newrelic-infra-setup Copy Create the newrelic-infra.yml agent config file with your New Relic license key. For config option explanations, see configuration settings. echo \"license_key: YOUR_LICENSE_KEY\" > newrelic-infra.yml Copy Create the newrelic-infra.dockerfile extending the newrelic/infrastructure image, and add your config to /etc/newrelic-infra.yml: touch newrelic-infra.dockerfile Copy vim newrelic-infra.dockerfile #you can use any text editor Copy Put the following content in the file: FROM newrelic/infrastructure:latest ADD newrelic-infra.yml /etc/newrelic-infra.yml Copy Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra build: context: . dockerfile: newrelic-infra.dockerfile cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Basic setup To use the basic setup with a base New Relic infrastructure image: Docker CLI Run the container with the required run flags: docker run \\ -d \\ --name newrelic-infra \\ --network=host \\ --cap-add=SYS_PTRACE \\ --privileged \\ --pid=host \\ -v \"/:/host:ro\" \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ -e NRIA_LICENSE_KEY=YOUR_LICENSE_KEY \\ newrelic/infrastructure:latest Copy For potential next steps, like how to see data in the UI, see What's next? Docker Compose Create docker-compose.yaml: touch docker-compose.yaml Copy vim docker-compose.yaml #you can use any text editor Copy Put following content in the file: version: '3' services: agent: container_name: newrelic-infra image: newrelic/infrastructure:latest cap_add: - SYS_PTRACE network_mode: host pid: host privileged: true volumes: - \"/:/host:ro\" - \"/var/run/docker.sock:/var/run/docker.sock\" environment: NRIA_LICENSE_KEY: \"YOUR_LICENSE_KEY\" restart: unless-stopped Copy Build and start docker-compose: docker-compose -f docker-compose.yaml up -d Copy For potential next steps, like how to see data in the UI, see What's next? Required container privileges Due to resource isolation from the host and other containers via Linux namespaces, a container has a very restricted view and control of its underlying host's resources by default. Without these extra privileges, the infrastructure agent cannot monitor the host and its containers. The infrastructure agent collects data about its host using system files and system calls. For more information about how the infrastructure agent collects data, see our documentation about infrastructure monitoring and security. Required privileges include: Privilege Description --network=host Sets the container's network namespace to the host's network namespace. This allows the agent to collect the network metrics about the host. -v \"/:/host:ro\" Bind mounts the host's root volume to the container. This read-only access to the host's root allows the agent to collect process and storage metrics as well as Inventory data from the host. --cap-add=SYS_PTRACE Adds the Linux capability to trace system processes. This allows the agent to gather data about processes running on the host. Read more here. --privileged --pid=host -v \"/var/run/docker.sock:/var/run/docker.sock\" Bind mounts the host's Docker daemon socket to the container. This allows the agent to connect to the Engine API via the Docker daemon socket to collect the host's container data. Next steps after install For next steps after install is completed, see What's next? Inventory collected Inventory is collected from the infrastructure agent's built-in data collectors. The infrastructure agent collects this data for Linux systems running with containers. Category Source Data collected using metadata agent_config Agent's complete config file system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name Container data Once the infrastructure agent is running in a Docker container, it can collect the same host compute data and event data that the infrastructure agent is capable of collecting when running natively on a host. For more information, see our documentation about how to view your Docker container data. Containerized agent image The containerized agent image is built from an Alpine base image. A CentOS base image is also available. Alpine is used as the base image since version 0.0.55. This is the one pointed by latest tag. Earlier versions used CentOS 7 as base image. In order to keep using that legacy image, some backports may be included there. To fetch the latest CentOS 7 based image, point to the latest-centos tag. Check the source code This integration is open source software. You can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.85773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker container for <em>infrastructure</em> monitoring",
        "sections": "Docker container for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> monitoring <em>agent</em> for <em>Linux</em> supports Docker environments by default. If you&#x27;re running a container OS or have restrictions that require deploying the <em>agent</em> as a container, you can run a containerized version of our <em>infrastructure</em> monitoring <em>agent</em>. This can monitor metrics"
      },
      "id": "6043ef6a28ccbce71b2c6062"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-07-27T11:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.29666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Set Docker-related alert conditions. Requirements Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/macos-installation/install-infrastructure-monitoring-agent-macos": [
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.2728,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " are also supported (with the exception of the Oracle integration). <em>macOS</em>: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The <em>infrastructure</em> <em>agent</em> supports these operating systems up to their manufacturer&#x27;s end-of-life. Operating system Supported by the <em>infrastructure</em> <em>agent</em>"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    },
    {
      "sections": [
        "Infrastructure agent overhead",
        "Linux single-task host",
        "Linux Docker host",
        "Windows host",
        "Linux ARM64 host",
        "Manage data",
        "Resource utilization"
      ],
      "title": "Infrastructure agent overhead",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "cd4b0d49bf6d11a12ff3a8357b223786b4c3f881",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead/",
      "published_at": "2022-01-08T13:03:25Z",
      "updated_at": "2021-12-25T15:24:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent is a lightweight piece of software, designed to minimize its impact on the performance of your hosts. However, the exact load varies depending on your host's workload, particularly on the number of processes running on the host. This is because the agent collects detailed data from each individual process. As a general guideline, New Relic has collected benchmarks for some common types of hosts: Linux single-task host The agent has very low performance overhead on a classic, single-task host. For example, a server running Apache, Unicorn, or a single Java application. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Operating system: CentOS 7 For this type of classic, single-task host, typical usage is: CPU: about 0.3% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Linux Docker host The agent has very low performance overhead on a host running Docker, with exact usage depending on the number of Docker containers your machine hosts, and whether those processes are long- or short-lived. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Number of containers: 25 containers, about 100 long-lived processes running in containers Operating system: CentOS 7 For this type of Docker host, typical usage is: CPU: about 0.8% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Windows host The agent has very low performance overhead on a typical Windows host serving web apps and running the Windows/IIS stack. Our benchmarks for this type of host are based on an Amazon EC2 t2.small: vCPUs: 1 Memory: 2.0 GB Storage: 30.0 GB Operating system: Windows Server 2012 R2 For this type of Windows host, typical usage is: CPU: 2 to 3% Resident Memory: 30 MB Storage on disk: about 50 MB Linux ARM64 host The agent has similar performance overhead on an ARM64 (Graviton 2) host on EC2 when compared with AMD64 machines. The benchmark is based on Amazon EC2 t3.2xlarge vs. t4g.2xlarge instances. Amazon Linux 2 EC2 instance with infrastructure agent default settings: CPU: about 0.1% on ARM vs 0.13% AMD Virtual memory: about 0.75GB ARM vs 1 GB AMD Resident memory: 20MB ARM vs 22 MB AMD We are always improving the performance of the infrastructure agent. If you see unusually high agent performance overhead, get support at support.newrelic.com. Manage data To learn how to adjust how much data our infrastructure monitoring ingests and reports, see Manage infrastructure data. Resource utilization On Linux systems, infrastructure is installed with default settings for each supported service manager. A memory limit of 1 Gigabyte is enforced. Please consider reviewing and adjusting the default configuration based on your system requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.02628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> overhead",
        "sections": "<em>Infrastructure</em> <em>agent</em> overhead",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> is a lightweight piece of software, designed to minimize its impact on the performance of your hosts. However, the exact load varies depending on your host&#x27;s workload, particularly on the number of processes running on the host. This is because the <em>agent</em> collects detailed"
      },
      "id": "6043fa3464441f329a378f18"
    },
    {
      "sections": [
        "Infrastructure agent configuration settings",
        "Get started",
        "Agent variables",
        "license_key (REQUIRED)",
        "fedramp",
        "max_procs",
        "payload_compression_level",
        "Important",
        "startup_connection_retries",
        "startup_connection_retry_time",
        "startup_connection_timeout",
        "Cloud variables",
        "cloud_max_retry_count",
        "cloud_metadata_expiry_sec",
        "cloud_retry_backoff_sec",
        "disable_cloud_instance_id",
        "disable_cloud_metadata",
        "Debug variables",
        "trace",
        "Docker variables",
        "container_cache_metadata_limit",
        "docker_api_version",
        "File system variables",
        "custom_supported_file_systems",
        "file_devices_ignored",
        "Hostname variables",
        "display_name",
        "dns_hostname_resolution",
        "override_hostname",
        "override_hostname_short",
        "Installation variables",
        "agent_dir",
        "plugin_dir",
        "custom_plugin_installation_dir",
        "Integrations variables",
        "passthrough_environment",
        "entityname_integrations_v2_update",
        "http_server_enabled",
        "http_server_host",
        "http_server_port",
        "remove_entities_period",
        "Inventory variables",
        "offline_time_to_reset",
        "ignored_inventory",
        "Linux variables",
        "pid_file",
        "ignore_reclaimable",
        "Logging variables",
        "log_file",
        "log_format",
        "log_to_stdout",
        "verbose",
        "smart_verbose_mode_entry_limit",
        "Metrics variables",
        "custom_attributes",
        "Tip",
        "enable_process_metrics",
        "include_matching_metrics",
        "network_interface_filters",
        "Linux",
        "Windows",
        "disable_zero_mem_process_filter",
        "Plugins variables",
        "disable_all_plugins",
        "cloud_security_group_refresh_sec",
        "daemontools_interval_sec",
        "dpkg_interval_sec",
        "facter_interval_sec",
        "kernel_modules_refresh_sec",
        "network_interface_interval_sec",
        "rpm_interval_sec",
        "selinux_interval_sec",
        "sshd_config_refresh_sec",
        "supervisor_interval_sec",
        "sysctl_interval_sec",
        "systemd_interval_sec",
        "sysvinit_interval_sec",
        "upstart_interval_sec",
        "users_refresh_sec",
        "supervisor_rpc_sock",
        "facter_home_dir",
        "Proxy variables",
        "proxy",
        "ignore_system_proxy",
        "ca_bundle_dir",
        "ca_bundle_file",
        "proxy_validate_certificates",
        "proxy_config_plugin",
        "Samples variables",
        "metrics_network_sample_rate",
        "metrics_process_sample_rate",
        "metrics_storage_sample_rate",
        "metrics_system_sample_rate",
        "metrics_nfs_sample_rate",
        "detailed_nfs",
        "Security variables",
        "selinux_enable_semodule",
        "strip_command_line",
        "Windows variables",
        "windows_services_refresh_sec",
        "windows_updates_refresh_sec",
        "app_data_dir",
        "enable_win_update_plugin",
        "legacy_storage_sampler",
        "win_process_priority_class",
        "win_removable_drives",
        "What's next?"
      ],
      "title": "Infrastructure agent configuration settings ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Configuration"
      ],
      "external_id": "7d016d3f5af6ed1615904578f0ed8bce02aa335b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/",
      "published_at": "2022-01-08T05:44:36Z",
      "updated_at": "2021-11-25T09:08:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent has a large set of configuration settings to fine-tune its behavior. Here we: List all the configuration options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any). List the minimum required agent version as applicable. Get started You'll be able to configure our infrastructure agent to suit your environnment after you create a New Relic account (it's free, forever) and install the infrastructure agent. The license_key is the only required setting. For an example of how all these variables can be used, see our sample configuration template in GitHub. Agent variables license_key (REQUIRED) Specifies the license key for your New Relic account. The agent uses this key to associate your server's metrics with your New Relic account. This setting is created as part of the standard installation process. YML option name Environment variable Type Default Version license_key NRIA_LICENSE_KEY string Example: license_key: 1234567890abcdefghijklmnopqrstuvwxyz1234 Copy fedramp Specifies whether Fedramp endpoints should be used. YML option name Environment variable Type Default Version fedramp NRIA_FEDRAMP boolean false 1.15.0 max_procs Specifies the number of logical processors available to the agent. Increasing this value helps to distribute the load between different cores. If set to -1, the agent will try to read the environment variable GOMAXPROCS. If this variable is not set, the default value will be the total number of cores available in the host. YML option name Environment variable Type Default Version max_procs NRIA_MAX_PROCS integer 1 1.0.1002 payload_compression_level Since version 1.0.804 or higher, data sent from the agent is compressed by default. To disable payload compression, set payload_compression_level to 0. Important Recommendation: Do not change this setting. YML option name Environment variable Type Default Version payload_compression_level NRIA_PAYLOAD_COMPRESSION_LEVEL integer 6 1.0.804 startup_connection_retries Number of times the agent will retry the request to check New Relic's platform availability at startup before throwing an error. If set to a negative value, the agent will keep checking the connection until it succeeds. YML option name Environment variable Type Default Version startup_connection_retries NRIA_STARTUP_CONNECTION_RETRIES integer 6 1.0.936 startup_connection_retry_time After a request has timed out, time the agent waits to retry a request to check New Relic's platform availability at startup. YML option name Environment variable Type Default Version startup_connection_retry_time NRIA_STARTUP_CONNECTION_RETRY_TIME string 5s 1.0.936 - 1.2.30 startup_connection_timeout Time the agent waits until a request to check New Relic's platform availability at startup is considered timed out. YML option name Environment variable Type Default Version startup_connection_timeout NRIA_STARTUP_CONNECTION_TIMEOUT string 10s 1.0.936 Cloud variables If the agent is running in a cloud instance, the agent will try to detect the cloud type and fetch metadata. cloud_max_retry_count Sets the number of times the agent retries to connect in case that cloud detection fails. If cloud detection fails during the initialization of the agent, the agent will retry after waiting for CloudRetryBackOffSec seconds. YML option name Environment variable Type Default Version cloud_max_retry_count NRIA_CLOUD_MAX_RETRY_COUNT integer 10 1.2.6 cloud_metadata_expiry_sec Sets the interval of time the agent will wait until discarding the metadata, in seconds. After this period metadata expires and the agent will fetch it again. YML option name Environment variable Type Default Version cloud_metadata_expiry_sec NRIA_CLOUD_METADATA_EXPIRY_SEC integer 300 1.2.6 cloud_retry_backoff_sec Sets the interval of time the agent waits between cloud detection retries in case that cloud detection failed, in seconds. If cloud detection fails during the initialization of the agent, it will retry for CloudMaxRetryCount times. YML option name Environment variable Type Default Version cloud_retry_backoff_sec NRIA_CLOUD_RETRY_BACKOFF_SEC integer 60 1.2.6 disable_cloud_instance_id Similar to DisableCloudMetadata, but it disables the collection of cloud metadata only for the host alias plugin. YML option name Environment variable Type Default Version disable_cloud_instance_id NRIA_DISABLE_CLOUD_INSTANCE_ID boolean false 1.0.220 disable_cloud_metadata Disables the collection of cloud metadata. YML option name Environment variable Type Default Version disable_cloud_metadata NRIA_DISABLE_CLOUD_METADATA boolean false 1.0.690 Debug variables trace Enables traces to be printed in the logs for the given features. Traces are additional log entries used for debugging purposes of specific features, they are only shown when the verbose log config option is enabled. Currently two features have traces defined: connect and attributes. YML option name Environment variable Type Default Version trace NRIA_TRACE [ ]string [] 1.0.1015 Example as a YAML attribute: trace: [connect, attributes] Copy Example as an environment variable: NRIA_TRACE='connect, attributes' Copy Docker variables container_cache_metadata_limit Time, in seconds, before the cached containers metadata expires and the agent needs to fetch them again. YML option name Environment variable Type Default Version container_cache_metadata_limit NRIA_CONTAINER_CACHE_METADATA_LIMIT integer 60 1.0.801 docker_api_version Specifies the Docker client API version. YML option name Environment variable Type Default Version docker_api_version NRIA_DOCKER_API_VERSION string 1.24 1.1.4 File system variables custom_supported_file_systems List of the types of file systems that the agent supports. This value needs to be a subset of the default list, and items that are not in the default list will be discarded. YML option name Environment variable Type Default Version custom_supported_file_systems NRIA_CUSTOM_SUPPORTED_FILESYSTEMS [ ]string Linux: [\"xfs\", \"btrfs\", \"ext\", \"ext2\", \"ext3\", \"ext4\", \"hfs\", \"vxfs\"] Windows: [\"NTFS\", \"ReFS\"] 1.0.220 file_devices_ignored List of storage devices to be ignored by the agent when gathering StorageSample data. YML option name Environment variable Type Default Version file_devices_ignored NRIA_FILE_DEVICES_IGNORED [ ]string 1.0.220 Example as a YAML attribute: file_devices_ignored: - sda1 - sda2 Copy Example as an environment variable: FILE_DEVICES_IGNORED=\"sda1,sda2\" Copy Hostname variables display_name Overrides the auto-generated hostname for reporting. This is useful when you have multiple hosts with the same name, since our infrastructure monitoring uses the hostname as the unique identifier for each host. Keep in mind this value is also used for the loopback address replacement on entity names. For more information, see our documentation on how entity name resolution works. YML option name Environment variable Type Default Version display_name NRIA_DISPLAY_NAME string empty 1.0.266 Example: display_name: teslaOne Copy dns_hostname_resolution When true, the full hostname is resolved by performing a reverse lookup of the host's address. Otherwise, it will be retrieved with the hostname command on Linux and from the TCP/IP parameters of the registry on Windows. YML option name Environment variable Type Default Version dns_hostname_resolution NRIA_DNS_HOSTNAME_RESOLUTION boolean true 1.2.6 override_hostname When set, this is the value that will be reported for the full hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname NRIA_OVERRIDE_HOSTNAME string 1.0.1015 Example: my.custom-hostname.co.org Copy override_hostname_short When set, this is the value that will be reported for the hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname_short NRIA_OVERRIDE_HOSTNAME_SHORT string 1.0.1015 Example: my.custom-hostname Copy Installation variables agent_dir Directory where the agent stores files for cache, inventory, integrations, etc. YML option name Environment variable Type Default Version agent_dir NRIA_AGENT_DIR string Linux: /var/db/newrelic-infra Windows: C:\\Program Files\\NewRelic\\newrelic-infra\\ 1.0.2 plugin_dir Directory containing the configuration files of the integrations. Each integration has its own configuration file, named by default <integration_name>-config.yml, placed in a predefined location from which the agent loads on initialization. YML option name Environment variable Type Default Version plugin_dir NRIA_PLUGIN_DIR string Linux: etc/newrelic-infra/integrations.d/ Windows: \\Program Files\\NewRelic\\newrelic-infra\\inregrations.d 1.0.2 Important With secrets management, you can configure on-host integrations with New Relic Infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. custom_plugin_installation_dir Specifies a custom path to install integrations, which allows to install them outside the agent_dir. It has priority when the agent is looking for installed integrations. YML option name Environment variable Type Default Version custom_plugin_installation_dir NRIA_CUSTOM_PLUGIN_INSTALLATION_DIR string Empty 1.0.2 Integrations variables passthrough_environment A list of environment variables that will be passed to all integrations. If an integration already has an existing configuration option with the same name, then the environment variable takes precedence. YML option name Environment variable Type Default Version passthrough_environment NRIA_PASSTHROUGH_ENVIRONMENT [ ]string Empty 1.0.719 Example as a YAML attribute (inside the agent's configuration file, located by default in /etc/newrelic-infra.yml): passthrough_environment: - HOST - PORT Copy Example as an environment variable: NRIA_PASSTHROUGH_ENVIRONMENT=\"HOST,PORT\" Copy entityname_integrations_v2_update The agent enables loopback-address replacement on the entity name (and therefore key) automatically for version 3 of the integration protocol. If you are using version 2 of the protocol and want this behavior, enable the entityname_integrations_v2_update option. YML option name Environment variable Type Default Version entityname_integrations_v2_update NRIA_ENTITYNAME_INTEGRATIONS_V2_UPDATE boolean false 1.2.15 http_server_enabled By setting this configuration parameter to true the agent will open an HTTP port (by default, 8001) to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_enabled NRIA_HTTP_SERVER_ENABLED boolean false 1.0.818 http_server_host By setting this value, the agent will start listening on the HTTPServerPort to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_host NRIA_HTTP_SERVER_HOST string localhost 1.0.818 http_server_port Sets the port for the http server to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_port NRIA_HTTP_SERVER_PORT integer 8001 1.0.818 remove_entities_period Starts the process of deleting entities that haven't reported information during this interval. Valid time units: s (seconds), m (minutes), and h (hour). YML option name Environment variable Type Default Version remove_entities_period NRIA_REMOVE_ENTITIES_PERIOD string 48h 1.0.859 Example: 1h Copy Inventory variables offline_time_to_reset If the cached inventory becomes older than this value (for example, because the agent is offline), the agent automatically removes and recreates the delta store. YML option name Environment variable Type Default Version offline_time_to_reset NRIA_OFFLINE_TIME_TO_RESET string 24h 1.0.888 ignored_inventory The list of inventory paths ignored by the agent. YML option name Environment variable Type Default Version ignored_inventory NRIA_IGNORED_INVENTORY string [ ] Empty list 1.0.336 Example as a YAML attribute: ignored_inventory: - files/config/stuff.bar - files/config/stuff.foo Copy Example as an environment variable: NRIA_IGNORED_INVENTORY=\"files/config/stuff.bar,files/config/stuff.foo\" Copy Linux variables pid_file Location on Linux where the pid file of the agent process is created. It is used at startup to ensure that no other instances of the agent are running. YML option name Environment variable Type Default Version pid_file NRIA_PID_FILE string /var/run/newrelic-infra/newrelic-infra.pid 1.0.2 ignore_reclaimable When true, formulation of the host virtual memory considers SReclaimable as available memory; otherwise SReclaimable will be considered part of the used memory. YML option name Environment variable Type Default Version ignore_reclaimable NRIA_IGNORE_RECLAIMABLE boolean false 1.2.6 Logging variables log_file Defines the file path for the logs. The default installation creates a log directory and it sets this filepath value in the log_file configuration option for you. This log directory is different for each OS, as shown below. Change this configuration option to customize the file path for the logs. YML option name Environment variable Type Default Version log_file NRIA_LOG_FILE string See below * Default paths: Linux: If not defined, it logs only in the standard output. Windows, agent version 1.0.752 or lower: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.775 to 1.0.944: C:\\%APPDATA%\\Roaming\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.944 or higher: C:\\%ProgramData%\\New Relic\\newrelic-infra\\newrelic-infra.log Copy If the directory can't be created: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy log_format Defines the log output format. Available values: text: Plain text output, one line per log entry. json: JSON-formatted output, one line per log entry. YML option name Environment variable Type Default Version log_format NRIA_LOG_FORMAT string text 1.4.9 log_to_stdout By default all logs are displayed in both standard output and a log file. To disable logs in the standard output, set this configuration option to false. YML option name Environment variable Type Default Version log_to_stdout NRIA_LOG_TO_STDOUT boolean true 1.0.703 verbose When verbose is set to 0, verbose logging is off, but the agent still creates logs. Set this to 1 to create verbose logs to use in troubleshooting the agent; set verbose to 2 to use the smart verbose mode logging. Smart verbose mode logs the last smart_verbose_mode_entry_limit debug messages when an error is logged. Setting verbose to 3 enables forwarding the agent logs to New Relic Logs. Important Verbose logging can generate a lot of data very quickly. Run the agent in verbose mode only for as long as necessary to reproduce your issue, then set verbose: 0 and restart your agent to disable verbose logging. Alternatively, you can set verbose: 2, which will enable smart verbose mode. YML option name Environment variable Type Default Version verbose NRIA_VERBOSE Integer (0, 1, 2 or 3) 0 1.9.0 when using smart mode (2) 1.11.4 when forwarding the New Relic Logs (3) smart_verbose_mode_entry_limit smart_verbose_mode_entry_limit refers to the number of previous debug messages that will be logged when an error is logged. For example, if the limit is set to 5, debug logs will be cached in memory until an error is logged, at which point the previous 5 debug messages will also be logged Important This configuration option is only used when verbose is set to 2 (Smart Verbose Mode enabled). YML option name Environment variable Type Default Version smart_verbose_mode_entry_limit NRIA_SMART_VERBOSE_MODE_ENTRY_LIMIT Integer 1000 1.9.0 Metrics variables custom_attributes Custom attributes are key-value pairs (similar to tags in other tools) used to annotate the data from the Infrastructure agent. You can use this metadata to build filter sets, group your results, and annotate your data. For example, you might indicate a machine's environment (staging or production), the service a machine hosts (login service, for example), or the team responsible for that machine. Tip The agent collects many details about your environment as part of its default attributes, including Amazon Elastic Compute Cloud (Amazon EC2) tags. YML option name Environment variable Type custom_attributes NRIA_CUSTOM_ATTRIBUTES map [ string]interface { } Use a list of custom attributes to annotate the data from this agent instance. Separate keys and values with colons :, as in KEY: VALUE, and separate each key-value pair with a line break. Keys can be any valid YAML except slashes /. Values can be any YAML string, including spaces. Example as a YAML attribute: custom_attributes: environment: production service: login service team: alpha-team Copy Example as an environment variable: NRIA_CUSTOM_ATTRIBUTES='{\"customAttribute_1\":\"SOME_ATTRIBUTE\",\"customAttribute_2\": \"SOME_ATTRIBUTE_2\"}' Copy NRQL example filtering by custom attribute: FROM SystemSample SELECT * WHERE environment = 'production' Copy enable_process_metrics Important Requires infrastructure agent version 1.12.0 or higher. Accounts created before July 20, 2020 and/or infrastructure agents installed using the new Guided Install have this variable enabled by default. Enables the sending of process metrics to New Relic. By default, the infrastructure agent doesn't send data about the operating system's processes. The agent still collects such data, unless metrics_process_sample_rate is set to -1. To report metric data about all the operating system's processes, set enable_process_metrics to true. To disable, set to false. Sending all process data could increase the volume of data sent to New Relic. To fine-tune which processes you want to monitor, configure include_matching_metrics. By default, processes using low memory are excluded from being sampled. For more information, see disable-zero-mem-process-filter. YML option name Environment variable Type Default Version enable_process_metrics NRIA_ENABLE_PROCESS_METRICS boolean false include_matching_metrics Important Currently, this setting only applies to an operating system's processes metrics. You can control how much data is sent to New Relic by configuring include_matching_metrics, which allows you to restrict the transmission of metric data based on the values of metric attributes. You include metric data by defining literal or partial values for any of the attributes of the metric. For example, you can choose to send the host.process.cpuPercent of all processes whose process.name match the ^java regular expression. In this example, we include process metrics using executable files and names: include_matching_metrics: # You can combine attributes from different metrics process.name: - regex “^java” # Include all processes starting with \"java\" process.executable: - “/usr/bin/python2” # Include the Python 2.x executable - regex “\\\\System32\\\\svchost” # Include all svchost executables Copy If you need to include command line arguments in any of the values, set strip_command_line to false (the infrastructure agents removes CLI arguments by default to prevent secrets from leaking). To configure include_matching_metrics as an environment variable for the Kubernetes integration, add it in the manifest inside the env: object: env: - name: NRIA_INCLUDE_MATCHING_METRICS value: | process.name: - regex \"^java\" process.executable: - \"/usr/bin/python2\" - regex \"\\\\System32\\\\svchost\" Copy Default YML option name Environment variable Type Default Version include_matching_metrics NRIA_INCLUDE_MATCHING_METRICS metric.attribute: - regex \"pattern\" - \"string\" - \"string-with-wildcard * \" network_interface_filters You can use the network interface filters configuration to hide unused or uninteresting network interfaces from the infrastructure agent. This helps reduce resource usage, work, and noise in your data. Important Environment variables are not supported for this configuration setting. The configuration uses a simple pattern-matching mechanism that can look for interfaces that start with a specific sequence of letters or numbers following either pattern: {name}[other characters], where you specify the name using the prefix option [number]{name}[other characters], where you specify the name using the index-1 option New Relic infrastructure implements a curated default list of filters, available for both Linux and Windows, that you can modify. YML option name Environment variable Type Default Version network_interface_filters not supported map [ string] [ ]string 1.0.220 Linux Default network interface filters for Linux: Network interfaces that start with dummy, lo, vmnet, sit, tun, tap, or veth Network interfaces that contain tun or tap The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with dummy or lo , or contain tun preceded by a sequence of numbers, and followed by other characters: network_interface_filters: prefix: - dummy - lo index-1: - tun Copy Windows Default network interface filters for Windows: Network interfaces that start with Loop, isatap, or Local The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with Loop: network_interface_filters: prefix: - Loop Copy disable_zero_mem_process_filter The ZeroRSSFilter excludes processes that are not using memory from being sampled. Disable the filter so that the agent includes these processes in the ProcessSample. YML option name Environment variable Type Default Version disable_zero_mem_process_filter NRIA_DISABLE_ZERO_MEM_PROCESS_FILTER boolean false 1.0.832 Plugins variables Tip You can quickly disable all the variables by setting DisableAllPlugins to true, and turn on only those options you need. disable_all_plugins To disable all the plugins, set this option to true. YML option name Environment variable Type Default Version disable_all_plugins NRIA_DISABLE_ALL_PLUGINS boolean false 1.2.1 cloud_security_group_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important This plugin is activated only if the agent is running in an AWS instance. YML option name Environment variable Type Default Version cloud_security_group_refresh_sec NRIA_CLOUD_SECURITY_GROUP_REFRESH_SEC int64 60 1.0.692 daemontools_interval_sec Sampling period for the Daemontoolsplugin, in seconds. The minimum value is a 10. To disable it, set it to -1. YML option name Environment variable Type Default Version daemontools_interval_sec NRIA_DAEMONTOOLS_INTERVAL_SEC int64 15 1.0.316 dpkg_interval_sec Sampling period for the Dpkg plugin, in seconds. The minimum value is 30. To disable it, set it to -1. If the parameter is not explicitly set in the config file, it can be disabled by setting DisableAllPlugins to true. Important This is only activated in root or privileged running modes and on Debian-based distributions. YML option name Environment variable Type Default Version dpkg_interval_sec NRIA_DPKG_INTERVAL_SEC int64 30 1.0.316 facter_interval_sec Sampling period for the Facter plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version facter_interval_sec NRIA_FACTER_INTERVAL_SEC int64 30 1.0.316 kernel_modules_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 10. To disable it, set it to -1. Important kernel_modules_refresh_sec is only activated in root or privileged running modes. YML option name Environment variable Type Default Version kernel_modules_refresh_sec NRIA_KERNEL_MODULES_REFRESH_SEC int64 10 1.0.755 network_interface_interval_sec Sampling period for the NetworkInterface plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version network_interface_interval_sec NRIA_NETWORK_INTERFACE_INTERVAL_SEC int64 60 1.0.329 rpm_interval_sec Sampling period for the Rpm plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important rpm_interval_sec is only activated when the agent runs in root or privileged modes for RedHat, RedHat AWS, or SUSE distributions. YML option name Environment variable Type Default Version rpm_interval_sec NRIA_RPM_INTERVAL_SEC int64 30 1.0.316 selinux_interval_sec Sampling period for the SELinux plugin, in seconds. The minimum value is 30. To disable it, set it to -1. This option is ignored if SelinuxEnableSemodule is set to false. For more information, see our troubleshooting doc on disabling the SELinux module. Important SELinux is only activated when the agent runs in root mode. YML option name Environment variable Type Default Version selinux_interval_sec NRIA_SELINUX_INTERVAL_SEC int64 30 1.0.316 sshd_config_refresh_sec Sampling period for the Sshd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sshd_config_refresh_sec NRIA_SSHD_CONFIG_REFRESH_SEC int64 15 1.0.755 supervisor_interval_sec Sampling period for the Supervisor plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version supervisor_interval_sec NRIA_SUPERVISOR_INTERVAL_SEC int64 15 1.0.316 sysctl_interval_sec Sampling period for the Sysctl plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version sysctl_interval_sec NRIA_SYSCTL_INTERVAL_SEC int64 60 1.0.316 systemd_interval_sec Sampling period for the Systemd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version systemd_interval_sec NRIA_SYSTEMD_INTERVAL_SEC int64 30 1.0.316 sysvinit_interval_sec Sampling period for the sysv plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sysvinit_interval_sec NRIA_SYSVINIT_INTERVAL_SEC int64 30 1.0.316 upstart_interval_sec Sampling period for the Upstart plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version upstart_interval_sec NRIA_UPSTART_INTERVAL_SEC int64 30 1.0.316 users_refresh_sec Sampling period for the Users plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version users_refresh_sec NRIA_USERS_REFRESH_SEC int64 30 1.0.755 supervisor_rpc_sock Location of the supervisor socket. YML option name Environment variable Type Default Version supervisor_rpc_sock NRIA_SUPERVISOR_RPC_SOCK string /var/run/supervisor.sock 1.0.2 facter_home_dir Sets the HOME environment variable for Puppet's Facter. If not defined, it defaults to the current user's home directory. YML option name Environment variable Type Default Version facter_home_dir NRIA_FACTER_HOME_DIR string 1.1.7 Proxy variables For infrastructure agent version 1.3.1 or higher, the precedence of the proxy configuration settings is: NRIA_PROXY proxy HTTP_PROXY HTTPS_PROXY proxy Your system may have firewall rules that require the agent to use a proxy to communicate with New Relic. If so, set the proxy URL in the form https://user:password@hostname:port. It can be HTTP or HTTPS. YML option name Environment variable Type Default Version proxy NRIA_PROXY string Empty 1.0.308 Example: https://proxy_user:access_10@proxy_01:1080 Copy ignore_system_proxy When set to true, the HTTPS_PROXY and HTTP_PROXY environment variables are ignored. This is useful when the agent needs to connect directly to the metrics collector and skip the existing system proxy. YML option name Environment variable Type Default Version ignore_system_proxy NRIA_IGNORE_SYSTEM_PROXY boolean false 1.0.1002 ca_bundle_dir If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the the directory where the proxy certificate is available. The certificates in the directory must end with the .pem extension. YML option name Environment variable Type Default Version ca_bundle_dir NRIA_CA_BUNDLE_DIR string 1.0.296 ca_bundle_file If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the certificate file. YML option name Environment variable Type Default Version ca_bundle_file NRIA_CA_BUNDLE_FILE string 1.0.296 proxy_validate_certificates If set to true, when the proxy is configured to use an HTTPS connection, it will only work: If the HTTPS proxy has certificates from a valid Certificate Authority. If the ca_bundle_file or ca_bundle_dir configuration properties contain the HTTPS proxy certificates. YML option name Environment variable Type Default Version proxy_validate_certificates NRIA_PROXY_VALIDATE_CERTIFICATES boolean false 1.3.0 proxy_config_plugin Sends the following proxy configuration information as inventory: HTTPS_PROXY HTTP_PROXY proxy ca_bundle_dir ca_bundle_file ignore_system_proxy proxy_validate_certificates YML option name Environment variable Type Default Version proxy_config_plugin NRIA_PROXY_CONFIG_PLUGIN boolean true 1.3.0 If you are having problems with proxy configuration, see Proxy troubleshooting. Samples variables metrics_network_sample_rate Sample rate of network samples, in seconds. Minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_network_sample_rate NRIA_METRICS_NETWORK_SAMPLE_RATE integer 10 1.0.308 metrics_process_sample_rate Sample rate of process samples, in seconds. Minimum value is 20. To disable process samples entirely, set metrics_process_sample_rate to -1. YML option name Environment variable Type Default Version metrics_process_sample_rate NRIA_METRICS_PROCESS_SAMPLE_RATE integer 20 1.0.308 metrics_storage_sample_rate Sample rate of storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_storage_sample_rate NRIA_METRICS_STORAGE_SAMPLE_RATE integer 5 1.0.308 metrics_system_sample_rate Sample rate of system samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_system_sample_rate NRIA_METRICS_SYSTEM_SAMPLE_RATE integer 5 1.0.308 metrics_nfs_sample_rate Sample rate of NFS storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_nfs_sample_rate NRIA_METRICS_NFS_SAMPLE_RATE integer 20 1.5.40 detailed_nfs Detailed NFS metrics. When enabled, the agent will provide a complete list of NFS metrics. YML option name Environment variable Type Default Version detailed_nfs NRIA_DETAILED_NFS boolean false 1.5.40 Security variables selinux_enable_semodule Get versions of policy modules installed using SEModule. If disabled, the SELinux plugin will only retrieve the status using SEStatus. YML option name Environment variable Type Default Version selinux_enable_semodule NRIA_SELINUX_ENABLE_SEMODULE boolean true 1.0.864 strip_command_line When true, the agent removes the command arguments from the commandLine attribute of the ProcessSample. Tip This is a security measure to prevent leaking sensitive information. YML option name Environment variable Type Default Version strip_command_line NRIA_STRIP_COMMAND_LINE boolean true 1.0.149 Windows variables windows_services_refresh_sec Sampling period for the Windows services plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_services_refresh_sec NRIA_WINDOWS_SERVICES_REFRESH_SEC int64 30 1.0.755 windows_updates_refresh_sec Sampling period for the Windows updates plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_updates_refresh_sec NRIA_WINDOWS_UPDATES_REFRESH_SEC int64 60 1.0.755 app_data_dir Defines the path to store data in a different path than the program files directory: %AppDir%/data: Used for storing the delta data %AppDir%/user_data: External directory for user-generated JSON files %AppDir%/newrelic-infra.log: If log file config option is not defined, then we use this directory path as default. YML option name Environment variable Type Default Version app_data_dir NRIA_APP_DATA_DIR string Windows: env(ProgramData)\\New Relic\\newrelic-infra Linux: Not applicable 1.0.755 enable_win_update_plugin Enables the Windows updates plugin, which retrieves the lists of hotfixes that are installed on the host. YML option name Environment variable Type Default Version enable_win_update_plugin NRIA_ENABLE_WIN_UPDATE_PLUGIN boolean false 1.0.274 legacy_storage_sampler If true, the agent will be forced to use Windows WMI (the agent's legacy method to grab metrics for Windows; for example, StorageSampler) and will disable the new method (which uses the PDH library). YML option name Environment variable Type Default Version legacy_storage_sampler NRIA_LEGACY_STORAGE_SAMPLER boolean 1.0.1051 win_process_priority_class This configuration option allows to increase the newrelic-infra.exe process priority to any of the following values: Normal Idle High RealTime BelowNormal AboveNormal YML option name Environment variable Type Default Version win_process_priority_class NRIA_WIN_PROCESS_PRIORITY_CLASS string 1.0.989 Example: Normal Copy win_removable_drives Enables the Windows agent to report drives A: and B: when they are mapped as removable drives. YML option name Environment variable Type Default Version win_removable_drives NRIA_WIN_REMOVABLE_DRIVES boolean true 1.3.1 What's next? You can also: Further understand the configuration of the agent. Create a configuration file using our template. See how you can manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.80705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> configuration settings ",
        "sections": "<em>Infrastructure</em> <em>agent</em> configuration settings",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "). List the minimum required <em>agent</em> version as applicable. Get started You&#x27;ll be able to configure our <em>infrastructure</em> <em>agent</em> to suit your environnment after you create a New Relic account (it&#x27;s free, forever) and <em>install</em> the <em>infrastructure</em> <em>agent</em>. The license_key is the only required setting. For an example"
      },
      "id": "603ea542196a67a38aa83dd8"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size": [
    {
      "sections": [
        "Infrastructure agent overhead",
        "Linux single-task host",
        "Linux Docker host",
        "Windows host",
        "Linux ARM64 host",
        "Manage data",
        "Resource utilization"
      ],
      "title": "Infrastructure agent overhead",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "cd4b0d49bf6d11a12ff3a8357b223786b4c3f881",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead/",
      "published_at": "2022-01-08T13:03:25Z",
      "updated_at": "2021-12-25T15:24:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent is a lightweight piece of software, designed to minimize its impact on the performance of your hosts. However, the exact load varies depending on your host's workload, particularly on the number of processes running on the host. This is because the agent collects detailed data from each individual process. As a general guideline, New Relic has collected benchmarks for some common types of hosts: Linux single-task host The agent has very low performance overhead on a classic, single-task host. For example, a server running Apache, Unicorn, or a single Java application. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Operating system: CentOS 7 For this type of classic, single-task host, typical usage is: CPU: about 0.3% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Linux Docker host The agent has very low performance overhead on a host running Docker, with exact usage depending on the number of Docker containers your machine hosts, and whether those processes are long- or short-lived. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Number of containers: 25 containers, about 100 long-lived processes running in containers Operating system: CentOS 7 For this type of Docker host, typical usage is: CPU: about 0.8% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Windows host The agent has very low performance overhead on a typical Windows host serving web apps and running the Windows/IIS stack. Our benchmarks for this type of host are based on an Amazon EC2 t2.small: vCPUs: 1 Memory: 2.0 GB Storage: 30.0 GB Operating system: Windows Server 2012 R2 For this type of Windows host, typical usage is: CPU: 2 to 3% Resident Memory: 30 MB Storage on disk: about 50 MB Linux ARM64 host The agent has similar performance overhead on an ARM64 (Graviton 2) host on EC2 when compared with AMD64 machines. The benchmark is based on Amazon EC2 t3.2xlarge vs. t4g.2xlarge instances. Amazon Linux 2 EC2 instance with infrastructure agent default settings: CPU: about 0.1% on ARM vs 0.13% AMD Virtual memory: about 0.75GB ARM vs 1 GB AMD Resident memory: 20MB ARM vs 22 MB AMD We are always improving the performance of the infrastructure agent. If you see unusually high agent performance overhead, get support at support.newrelic.com. Manage data To learn how to adjust how much data our infrastructure monitoring ingests and reports, see Manage infrastructure data. Resource utilization On Linux systems, infrastructure is installed with default settings for each supported service manager. A memory limit of 1 Gigabyte is enforced. Please consider reviewing and adjusting the default configuration based on your system requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.21625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> overhead",
        "sections": "<em>Infrastructure</em> <em>agent</em> overhead",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> is a lightweight piece of software, designed to minimize its impact on the performance of <em>your</em> hosts. However, the exact load varies depending on <em>your</em> host&#x27;s workload, particularly on the number of processes running on the host. This is because the <em>agent</em> collects detailed"
      },
      "id": "6043fa3464441f329a378f18"
    },
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T21:39:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. macOS: Manually install agent versions through HomeBrew. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.40501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Start, stop, and restart the infrastructure agent",
        "Linux: Start, stop, restart, or check agent status",
        "Windows: Start, stop, restart, or check agent status",
        "Important",
        "Command prompt (cmd.exe)",
        "PowerShell",
        "macOS: Start, stop, restart, or check agent status",
        "Customize agent logs",
        "Determine your init system",
        "Configuration management tools"
      ],
      "title": "Start, stop, and restart the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "81bb8174fac415ee0ec94c51e123c32eba700d6e",
      "image": "https://docs.newrelic.com/static/19e41f9fd7395eb6f1c816aa87182bb9/103b3/otherlinux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T17:29:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent starts automatically after you run the installation script. However, there are situations where you may need to manually restart the agent (for example, after changing your agent configuration). Linux: Start, stop, restart, or check agent status For Linux, ensure you use the correct command for your init system. Select start, stop, restart, or status as appropriate: SystemD (Amazon Linux 2, SLES 12, CentOS 7 or higher, Debian 8 or higher, RHEL 7 or higher, Ubuntu 15.04 or higher): sudo systemctl <start|stop|restart|status> newrelic-infra Copy System V (Debian 7, SLES 11.4, RHEL 5): sudo /etc/init.d/newrelic-infra <start|stop|restart|status> Copy Upstart (Amazon Linux, RHEL 6, Ubuntu 14.04 or lower): sudo initctl <start|stop|restart|status> newrelic-infra Copy Windows: Start, stop, restart, or check agent status Important To start, stop, or restart the agent, you must run cmd.exe or PowerShell as Administrator. For Windows Server, you can use the Windows command prompt or PowerShell. Command prompt (cmd.exe) Start or stop the Windows agent: net <start|stop> newrelic-infra Copy Restart the Windows agent: net stop newrelic-infra ; net start newrelic-infra Copy Check the status of the Windows agent: sc query \"newrelic-infra\" | find \"STATE\" Copy PowerShell Start or stop the Windows agent: Stop-Service -Name \"newrelic-infra\" Start-Service -Name \"newrelic-infra\" Copy You can also use net start|stop newrelic-infra Restart the Windows agent: Restart-Service newrelic-infra Copy Check status of Windows agent: (Get-Service newrelic-infra).Status Copy macOS: Start, stop, restart, or check agent status Stop or start the agent: brew services stop newrelic-infra-agent brew services start newrelic-infra-agent Copy Restart the agent: brew services restart newrelic-infra-agent Copy Check status of the agent: brew services list Copy Customize agent logs The infrastructure agent logs to a default location which depends on your platform. You can customize this location with the log_file setting. You can also generate verbose logs for troubleshooting. Determine your init system For Windows Server, the commands in this document use the Windows command prompt. For Linux, the infrastructure agent selects an init system appropriate for your distribution: Distribution SystemD System V Upstart Amazon Linux Amazon Linux 2 CentOS 7 CentOS 8 Debian 7 (\"Wheezy\") Debian 8 (\"Jessie\") Debian 9 (\"Stretch\") RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To manage the infrastructure agent with your config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.38506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "sections": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "&quot;) Debian 8 (&quot;Jessie&quot;) Debian 9 (&quot;Stretch&quot;) RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To <em>manage</em> the <em>infrastructure</em> <em>agent</em> with <em>your</em> config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration"
      },
      "id": "603ec355e7b9d294092a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior": [
    {
      "sections": [
        "Infrastructure agent overhead",
        "Linux single-task host",
        "Linux Docker host",
        "Windows host",
        "Linux ARM64 host",
        "Manage data",
        "Resource utilization"
      ],
      "title": "Infrastructure agent overhead",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "cd4b0d49bf6d11a12ff3a8357b223786b4c3f881",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead/",
      "published_at": "2022-01-08T13:03:25Z",
      "updated_at": "2021-12-25T15:24:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent is a lightweight piece of software, designed to minimize its impact on the performance of your hosts. However, the exact load varies depending on your host's workload, particularly on the number of processes running on the host. This is because the agent collects detailed data from each individual process. As a general guideline, New Relic has collected benchmarks for some common types of hosts: Linux single-task host The agent has very low performance overhead on a classic, single-task host. For example, a server running Apache, Unicorn, or a single Java application. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Operating system: CentOS 7 For this type of classic, single-task host, typical usage is: CPU: about 0.3% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Linux Docker host The agent has very low performance overhead on a host running Docker, with exact usage depending on the number of Docker containers your machine hosts, and whether those processes are long- or short-lived. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Number of containers: 25 containers, about 100 long-lived processes running in containers Operating system: CentOS 7 For this type of Docker host, typical usage is: CPU: about 0.8% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Windows host The agent has very low performance overhead on a typical Windows host serving web apps and running the Windows/IIS stack. Our benchmarks for this type of host are based on an Amazon EC2 t2.small: vCPUs: 1 Memory: 2.0 GB Storage: 30.0 GB Operating system: Windows Server 2012 R2 For this type of Windows host, typical usage is: CPU: 2 to 3% Resident Memory: 30 MB Storage on disk: about 50 MB Linux ARM64 host The agent has similar performance overhead on an ARM64 (Graviton 2) host on EC2 when compared with AMD64 machines. The benchmark is based on Amazon EC2 t3.2xlarge vs. t4g.2xlarge instances. Amazon Linux 2 EC2 instance with infrastructure agent default settings: CPU: about 0.1% on ARM vs 0.13% AMD Virtual memory: about 0.75GB ARM vs 1 GB AMD Resident memory: 20MB ARM vs 22 MB AMD We are always improving the performance of the infrastructure agent. If you see unusually high agent performance overhead, get support at support.newrelic.com. Manage data To learn how to adjust how much data our infrastructure monitoring ingests and reports, see Manage infrastructure data. Resource utilization On Linux systems, infrastructure is installed with default settings for each supported service manager. A memory limit of 1 Gigabyte is enforced. Please consider reviewing and adjusting the default configuration based on your system requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.21625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> overhead",
        "sections": "<em>Infrastructure</em> <em>agent</em> overhead",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> is a lightweight piece of software, designed to minimize its impact on the performance of <em>your</em> hosts. However, the exact load varies depending on <em>your</em> host&#x27;s workload, particularly on the number of processes running on the host. This is because the <em>agent</em> collects detailed"
      },
      "id": "6043fa3464441f329a378f18"
    },
    {
      "sections": [
        "Start, stop, and restart the infrastructure agent",
        "Linux: Start, stop, restart, or check agent status",
        "Windows: Start, stop, restart, or check agent status",
        "Important",
        "Command prompt (cmd.exe)",
        "PowerShell",
        "macOS: Start, stop, restart, or check agent status",
        "Customize agent logs",
        "Determine your init system",
        "Configuration management tools"
      ],
      "title": "Start, stop, and restart the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "81bb8174fac415ee0ec94c51e123c32eba700d6e",
      "image": "https://docs.newrelic.com/static/19e41f9fd7395eb6f1c816aa87182bb9/103b3/otherlinux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T17:29:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent starts automatically after you run the installation script. However, there are situations where you may need to manually restart the agent (for example, after changing your agent configuration). Linux: Start, stop, restart, or check agent status For Linux, ensure you use the correct command for your init system. Select start, stop, restart, or status as appropriate: SystemD (Amazon Linux 2, SLES 12, CentOS 7 or higher, Debian 8 or higher, RHEL 7 or higher, Ubuntu 15.04 or higher): sudo systemctl <start|stop|restart|status> newrelic-infra Copy System V (Debian 7, SLES 11.4, RHEL 5): sudo /etc/init.d/newrelic-infra <start|stop|restart|status> Copy Upstart (Amazon Linux, RHEL 6, Ubuntu 14.04 or lower): sudo initctl <start|stop|restart|status> newrelic-infra Copy Windows: Start, stop, restart, or check agent status Important To start, stop, or restart the agent, you must run cmd.exe or PowerShell as Administrator. For Windows Server, you can use the Windows command prompt or PowerShell. Command prompt (cmd.exe) Start or stop the Windows agent: net <start|stop> newrelic-infra Copy Restart the Windows agent: net stop newrelic-infra ; net start newrelic-infra Copy Check the status of the Windows agent: sc query \"newrelic-infra\" | find \"STATE\" Copy PowerShell Start or stop the Windows agent: Stop-Service -Name \"newrelic-infra\" Start-Service -Name \"newrelic-infra\" Copy You can also use net start|stop newrelic-infra Restart the Windows agent: Restart-Service newrelic-infra Copy Check status of Windows agent: (Get-Service newrelic-infra).Status Copy macOS: Start, stop, restart, or check agent status Stop or start the agent: brew services stop newrelic-infra-agent brew services start newrelic-infra-agent Copy Restart the agent: brew services restart newrelic-infra-agent Copy Check status of the agent: brew services list Copy Customize agent logs The infrastructure agent logs to a default location which depends on your platform. You can customize this location with the log_file setting. You can also generate verbose logs for troubleshooting. Determine your init system For Windows Server, the commands in this document use the Windows command prompt. For Linux, the infrastructure agent selects an init system appropriate for your distribution: Distribution SystemD System V Upstart Amazon Linux Amazon Linux 2 CentOS 7 CentOS 8 Debian 7 (\"Wheezy\") Debian 8 (\"Jessie\") Debian 9 (\"Stretch\") RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To manage the infrastructure agent with your config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.38506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "sections": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "&quot;) Debian 8 (&quot;Jessie&quot;) Debian 9 (&quot;Stretch&quot;) RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To <em>manage</em> the <em>infrastructure</em> <em>agent</em> with <em>your</em> config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration"
      },
      "id": "603ec355e7b9d294092a0818"
    },
    {
      "sections": [
        "Agent message size"
      ],
      "title": "Agent message size",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "a762f70a367322dd6fa16c0825cbf3c3495b6b6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Since infrastructure agent version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 241.60219,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agent</em> message size",
        "sections": "<em>Agent</em> message size",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Since <em>infrastructure</em> <em>agent</em> version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB."
      },
      "id": "603ea87628ccbc6062eba74a"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead": [
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T21:39:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. macOS: Manually install agent versions through HomeBrew. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.40501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Start, stop, and restart the infrastructure agent",
        "Linux: Start, stop, restart, or check agent status",
        "Windows: Start, stop, restart, or check agent status",
        "Important",
        "Command prompt (cmd.exe)",
        "PowerShell",
        "macOS: Start, stop, restart, or check agent status",
        "Customize agent logs",
        "Determine your init system",
        "Configuration management tools"
      ],
      "title": "Start, stop, and restart the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "81bb8174fac415ee0ec94c51e123c32eba700d6e",
      "image": "https://docs.newrelic.com/static/19e41f9fd7395eb6f1c816aa87182bb9/103b3/otherlinux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T17:29:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent starts automatically after you run the installation script. However, there are situations where you may need to manually restart the agent (for example, after changing your agent configuration). Linux: Start, stop, restart, or check agent status For Linux, ensure you use the correct command for your init system. Select start, stop, restart, or status as appropriate: SystemD (Amazon Linux 2, SLES 12, CentOS 7 or higher, Debian 8 or higher, RHEL 7 or higher, Ubuntu 15.04 or higher): sudo systemctl <start|stop|restart|status> newrelic-infra Copy System V (Debian 7, SLES 11.4, RHEL 5): sudo /etc/init.d/newrelic-infra <start|stop|restart|status> Copy Upstart (Amazon Linux, RHEL 6, Ubuntu 14.04 or lower): sudo initctl <start|stop|restart|status> newrelic-infra Copy Windows: Start, stop, restart, or check agent status Important To start, stop, or restart the agent, you must run cmd.exe or PowerShell as Administrator. For Windows Server, you can use the Windows command prompt or PowerShell. Command prompt (cmd.exe) Start or stop the Windows agent: net <start|stop> newrelic-infra Copy Restart the Windows agent: net stop newrelic-infra ; net start newrelic-infra Copy Check the status of the Windows agent: sc query \"newrelic-infra\" | find \"STATE\" Copy PowerShell Start or stop the Windows agent: Stop-Service -Name \"newrelic-infra\" Start-Service -Name \"newrelic-infra\" Copy You can also use net start|stop newrelic-infra Restart the Windows agent: Restart-Service newrelic-infra Copy Check status of Windows agent: (Get-Service newrelic-infra).Status Copy macOS: Start, stop, restart, or check agent status Stop or start the agent: brew services stop newrelic-infra-agent brew services start newrelic-infra-agent Copy Restart the agent: brew services restart newrelic-infra-agent Copy Check status of the agent: brew services list Copy Customize agent logs The infrastructure agent logs to a default location which depends on your platform. You can customize this location with the log_file setting. You can also generate verbose logs for troubleshooting. Determine your init system For Windows Server, the commands in this document use the Windows command prompt. For Linux, the infrastructure agent selects an init system appropriate for your distribution: Distribution SystemD System V Upstart Amazon Linux Amazon Linux 2 CentOS 7 CentOS 8 Debian 7 (\"Wheezy\") Debian 8 (\"Jessie\") Debian 9 (\"Stretch\") RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To manage the infrastructure agent with your config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.38506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "sections": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "&quot;) Debian 8 (&quot;Jessie&quot;) Debian 9 (&quot;Stretch&quot;) RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To <em>manage</em> the <em>infrastructure</em> <em>agent</em> with <em>your</em> config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration"
      },
      "id": "603ec355e7b9d294092a0818"
    },
    {
      "sections": [
        "Agent message size"
      ],
      "title": "Agent message size",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "a762f70a367322dd6fa16c0825cbf3c3495b6b6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Since infrastructure agent version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 241.60219,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agent</em> message size",
        "sections": "<em>Agent</em> message size",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Since <em>infrastructure</em> <em>agent</em> version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB."
      },
      "id": "603ea87628ccbc6062eba74a"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent": [
    {
      "sections": [
        "Infrastructure agent overhead",
        "Linux single-task host",
        "Linux Docker host",
        "Windows host",
        "Linux ARM64 host",
        "Manage data",
        "Resource utilization"
      ],
      "title": "Infrastructure agent overhead",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "cd4b0d49bf6d11a12ff3a8357b223786b4c3f881",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead/",
      "published_at": "2022-01-08T13:03:25Z",
      "updated_at": "2021-12-25T15:24:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent is a lightweight piece of software, designed to minimize its impact on the performance of your hosts. However, the exact load varies depending on your host's workload, particularly on the number of processes running on the host. This is because the agent collects detailed data from each individual process. As a general guideline, New Relic has collected benchmarks for some common types of hosts: Linux single-task host The agent has very low performance overhead on a classic, single-task host. For example, a server running Apache, Unicorn, or a single Java application. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Operating system: CentOS 7 For this type of classic, single-task host, typical usage is: CPU: about 0.3% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Linux Docker host The agent has very low performance overhead on a host running Docker, with exact usage depending on the number of Docker containers your machine hosts, and whether those processes are long- or short-lived. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Number of containers: 25 containers, about 100 long-lived processes running in containers Operating system: CentOS 7 For this type of Docker host, typical usage is: CPU: about 0.8% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Windows host The agent has very low performance overhead on a typical Windows host serving web apps and running the Windows/IIS stack. Our benchmarks for this type of host are based on an Amazon EC2 t2.small: vCPUs: 1 Memory: 2.0 GB Storage: 30.0 GB Operating system: Windows Server 2012 R2 For this type of Windows host, typical usage is: CPU: 2 to 3% Resident Memory: 30 MB Storage on disk: about 50 MB Linux ARM64 host The agent has similar performance overhead on an ARM64 (Graviton 2) host on EC2 when compared with AMD64 machines. The benchmark is based on Amazon EC2 t3.2xlarge vs. t4g.2xlarge instances. Amazon Linux 2 EC2 instance with infrastructure agent default settings: CPU: about 0.1% on ARM vs 0.13% AMD Virtual memory: about 0.75GB ARM vs 1 GB AMD Resident memory: 20MB ARM vs 22 MB AMD We are always improving the performance of the infrastructure agent. If you see unusually high agent performance overhead, get support at support.newrelic.com. Manage data To learn how to adjust how much data our infrastructure monitoring ingests and reports, see Manage infrastructure data. Resource utilization On Linux systems, infrastructure is installed with default settings for each supported service manager. A memory limit of 1 Gigabyte is enforced. Please consider reviewing and adjusting the default configuration based on your system requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.2162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> overhead",
        "sections": "<em>Infrastructure</em> <em>agent</em> overhead",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> is a lightweight piece of software, designed to minimize its impact on the performance of <em>your</em> hosts. However, the exact load varies depending on <em>your</em> host&#x27;s workload, particularly on the number of processes running on the host. This is because the <em>agent</em> collects detailed"
      },
      "id": "6043fa3464441f329a378f18"
    },
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T21:39:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. macOS: Manually install agent versions through HomeBrew. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.40501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Agent message size"
      ],
      "title": "Agent message size",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "a762f70a367322dd6fa16c0825cbf3c3495b6b6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size/",
      "published_at": "2022-01-08T13:01:54Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Since infrastructure agent version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 241.60219,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agent</em> message size",
        "sections": "<em>Agent</em> message size",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Since <em>infrastructure</em> <em>agent</em> version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB."
      },
      "id": "603ea87628ccbc6062eba74a"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/troubleshoot-running-infrastructure-agent": [
    {
      "sections": [
        "Infrastructure agent overhead",
        "Linux single-task host",
        "Linux Docker host",
        "Windows host",
        "Linux ARM64 host",
        "Manage data",
        "Resource utilization"
      ],
      "title": "Infrastructure agent overhead",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "cd4b0d49bf6d11a12ff3a8357b223786b4c3f881",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead/",
      "published_at": "2022-01-08T13:03:25Z",
      "updated_at": "2021-12-25T15:24:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent is a lightweight piece of software, designed to minimize its impact on the performance of your hosts. However, the exact load varies depending on your host's workload, particularly on the number of processes running on the host. This is because the agent collects detailed data from each individual process. As a general guideline, New Relic has collected benchmarks for some common types of hosts: Linux single-task host The agent has very low performance overhead on a classic, single-task host. For example, a server running Apache, Unicorn, or a single Java application. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Operating system: CentOS 7 For this type of classic, single-task host, typical usage is: CPU: about 0.3% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Linux Docker host The agent has very low performance overhead on a host running Docker, with exact usage depending on the number of Docker containers your machine hosts, and whether those processes are long- or short-lived. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Number of containers: 25 containers, about 100 long-lived processes running in containers Operating system: CentOS 7 For this type of Docker host, typical usage is: CPU: about 0.8% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Windows host The agent has very low performance overhead on a typical Windows host serving web apps and running the Windows/IIS stack. Our benchmarks for this type of host are based on an Amazon EC2 t2.small: vCPUs: 1 Memory: 2.0 GB Storage: 30.0 GB Operating system: Windows Server 2012 R2 For this type of Windows host, typical usage is: CPU: 2 to 3% Resident Memory: 30 MB Storage on disk: about 50 MB Linux ARM64 host The agent has similar performance overhead on an ARM64 (Graviton 2) host on EC2 when compared with AMD64 machines. The benchmark is based on Amazon EC2 t3.2xlarge vs. t4g.2xlarge instances. Amazon Linux 2 EC2 instance with infrastructure agent default settings: CPU: about 0.1% on ARM vs 0.13% AMD Virtual memory: about 0.75GB ARM vs 1 GB AMD Resident memory: 20MB ARM vs 22 MB AMD We are always improving the performance of the infrastructure agent. If you see unusually high agent performance overhead, get support at support.newrelic.com. Manage data To learn how to adjust how much data our infrastructure monitoring ingests and reports, see Manage infrastructure data. Resource utilization On Linux systems, infrastructure is installed with default settings for each supported service manager. A memory limit of 1 Gigabyte is enforced. Please consider reviewing and adjusting the default configuration based on your system requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.2162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> overhead",
        "sections": "<em>Infrastructure</em> <em>agent</em> overhead",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> is a lightweight piece of software, designed to minimize its impact on the performance of <em>your</em> hosts. However, the exact load varies depending on <em>your</em> host&#x27;s workload, particularly on the number of processes running on the host. This is because the <em>agent</em> collects detailed"
      },
      "id": "6043fa3464441f329a378f18"
    },
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T21:39:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. macOS: Manually install agent versions through HomeBrew. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.40501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Start, stop, and restart the infrastructure agent",
        "Linux: Start, stop, restart, or check agent status",
        "Windows: Start, stop, restart, or check agent status",
        "Important",
        "Command prompt (cmd.exe)",
        "PowerShell",
        "macOS: Start, stop, restart, or check agent status",
        "Customize agent logs",
        "Determine your init system",
        "Configuration management tools"
      ],
      "title": "Start, stop, and restart the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "81bb8174fac415ee0ec94c51e123c32eba700d6e",
      "image": "https://docs.newrelic.com/static/19e41f9fd7395eb6f1c816aa87182bb9/103b3/otherlinux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent/",
      "published_at": "2022-01-08T13:02:34Z",
      "updated_at": "2021-08-20T17:29:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent starts automatically after you run the installation script. However, there are situations where you may need to manually restart the agent (for example, after changing your agent configuration). Linux: Start, stop, restart, or check agent status For Linux, ensure you use the correct command for your init system. Select start, stop, restart, or status as appropriate: SystemD (Amazon Linux 2, SLES 12, CentOS 7 or higher, Debian 8 or higher, RHEL 7 or higher, Ubuntu 15.04 or higher): sudo systemctl <start|stop|restart|status> newrelic-infra Copy System V (Debian 7, SLES 11.4, RHEL 5): sudo /etc/init.d/newrelic-infra <start|stop|restart|status> Copy Upstart (Amazon Linux, RHEL 6, Ubuntu 14.04 or lower): sudo initctl <start|stop|restart|status> newrelic-infra Copy Windows: Start, stop, restart, or check agent status Important To start, stop, or restart the agent, you must run cmd.exe or PowerShell as Administrator. For Windows Server, you can use the Windows command prompt or PowerShell. Command prompt (cmd.exe) Start or stop the Windows agent: net <start|stop> newrelic-infra Copy Restart the Windows agent: net stop newrelic-infra ; net start newrelic-infra Copy Check the status of the Windows agent: sc query \"newrelic-infra\" | find \"STATE\" Copy PowerShell Start or stop the Windows agent: Stop-Service -Name \"newrelic-infra\" Start-Service -Name \"newrelic-infra\" Copy You can also use net start|stop newrelic-infra Restart the Windows agent: Restart-Service newrelic-infra Copy Check status of Windows agent: (Get-Service newrelic-infra).Status Copy macOS: Start, stop, restart, or check agent status Stop or start the agent: brew services stop newrelic-infra-agent brew services start newrelic-infra-agent Copy Restart the agent: brew services restart newrelic-infra-agent Copy Check status of the agent: brew services list Copy Customize agent logs The infrastructure agent logs to a default location which depends on your platform. You can customize this location with the log_file setting. You can also generate verbose logs for troubleshooting. Determine your init system For Windows Server, the commands in this document use the Windows command prompt. For Linux, the infrastructure agent selects an init system appropriate for your distribution: Distribution SystemD System V Upstart Amazon Linux Amazon Linux 2 CentOS 7 CentOS 8 Debian 7 (\"Wheezy\") Debian 8 (\"Jessie\") Debian 9 (\"Stretch\") RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To manage the infrastructure agent with your config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.38506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "sections": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "&quot;) Debian 8 (&quot;Jessie&quot;) Debian 9 (&quot;Stretch&quot;) RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To <em>manage</em> the <em>infrastructure</em> <em>agent</em> with <em>your</em> config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration"
      },
      "id": "603ec355e7b9d294092a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-agent": [
    {
      "sections": [
        "Uninstall infrastructure integrations",
        "Cloud integrations",
        "AWS",
        "Azure",
        "Google Cloud Platform (GCP)",
        "On-host integrations",
        "Apache",
        "Cassandra",
        "Kubernetes",
        "MySQL",
        "NGINX",
        "Redis",
        "StatsD",
        "Moving away from the integrations package",
        "Uninstall package"
      ],
      "title": "Uninstall infrastructure integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1e9232193cbf71bdbe1a6c6d0374ed0d6b7e7b0f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-integrations/",
      "published_at": "2022-01-08T13:04:05Z",
      "updated_at": "2021-10-30T20:41:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Uninstalling the infrastructure agent does not directly affect any of your infrastructure Integrations: if you uninstall the agent, your integrations will remain. Similarly, if you disable or uninstall your integrations, the infrastructure agent will remain. To uninstall any of your integrations, follow the procedure corresponding to the type of integration. Cloud integrations AWS You can disable infrastructure AWS integrations and still retain the connection between your AWS account and New Relic. We recommend not to disable your EC2 and EBS integrations because those add important metadata to your infrastructure data. If you want to... Do this Disable one or more AWS service integrations To disable services while keeping your AWS account linked to New Relic: From one.newrelic.com > Infrastructure, select AWS > Manage services. From your Edit AWS account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all AWS integrations To disconnect your AWS account completely from New Relic, you need to unlink your AWS account. This disables all New Relic integrations associated with that AWS account. Go to one.newrelic.com > Infrastructure > AWS > Manage services. From your Edit AWS account page, select Unlink this account. Save your changes. Sign in to AWS and select Services > IAM > Roles. Select the checkbox for the role you want to delete, then select Role Actions > Delete Role. Unlinking your AWS account will disable the trust relationship set up via your ARN. Azure If you want to... Do this Disable one or more Azure service integrations To disable services while keeping your Azure account linked to New Relic: Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all Azure integrations To disconnect your Azure account completely from New Relic, you need to unlink your Azure account. This requires being either the user who registered the app or an administrator. This procedure will disable all New Relic integrations associated with that Azure account. Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, select Unlink this account. Save your changes. Sign in to Azure and go into All Services > Identity > App registrations, or go to Azure Active Directory service and select App registrations. Find the registered app (the recommended name is NewRelic-Integrations). To see the full list of available apps, select the dropdown menu beside the search field and select All apps. Select the app and, on the panel that opens, select Delete. Google Cloud Platform (GCP) If you want to... Do this Disable one or more GCP service integrations To disable services while keeping your GCP account linked to New Relic: From one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all GCP integrations To disconnect your GCP account completely from New Relic, you need to unlink your GCP account. This disables all New Relic integrations associated with that GCP account. If you registered the GCP project using a User account, follow these steps. Go to one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, select Unlink this account. Save your changes. If you registered the GCP project using a Service account, follow these steps. If you are deleting a custom role, be aware that this role may be used for other purposes besides New Relic access. Sign in to New Relic and go to Infrastructure > Integrations > Google Cloud Platform. For a standard (non-custom) user role, select Manage Services for the account you want to remove. Copy the value of User and save it. OR For a custom user role, go to IAM > admin > Roles, search for the role, select it, and select DELETE. You are now finished and can skip the remaining steps. Standard (non-custom) user role: Sign in to Google Cloud and select the correct project in the Select a project box. From the navigation menu, select IAM & admin > IAM. Search for and select the user value you saved, then select REMOVE. On-host integrations If you used the integrations package, see the integrations package instructions. Here are some examples: Apache Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-apache yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-apache zypper (SLES) sudo zypper -n remove nri-apache Cassandra Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-cassandra yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-cassandra zypper (SLES) sudo zypper -n remove nri-cassandra Kubernetes Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy If you used Helm to install the Kubernetes integration, just uninstall it using Helm: helm uninstall --namespace NAMESPACE_USED_DURING_INSTALLATION RELEASE_NAME Copy If you installed Kubernetes integration using a manifest, use the same manifest to uninstall it: kubectl delete -f newrelic-infrastructure.yaml Copy MySQL Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-mysql yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-mysql zypper (SLES) sudo zypper -n remove nri-mysql NGINX Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-nginx yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-nginx zypper (SLES) sudo zypper -n remove nri-nginx Redis Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-redis yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-redis zypper (SLES) sudo zypper -n remove nri-redis StatsD cd /path/to/statsd npm uninstall @newrelic/statsd-infra-backend From the StatsD config.js, remove the \"@newrelic/statsd-infra-backend\" entry from the list of backends. Restart StatsD. Moving away from the integrations package While it is still possible to use the integrations package, we recommend removing it completely and working with integrations on an individual basis. The last integration package contains the following versions of the integrations: Apache 1.1.2 Cassandra 2.0.3 MySQL 1.1.5 Nginx 1.0.2 Redis 1.0.1 If you remove the integrations package and want to continue using the related on-host integrations, you will need to install them one by one. To uninstall the package and re-install your integrations: Remove the integrations package by following these instructions. The config files from the old integrations will not be deleted, so you won’t have to configure them again. Uninstall package Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove newrelic-infra-integrations sudo apt-get autoremove yum (Amazon Linux, CentOS, or RHEL) sudo yum remove newrelic-infra-integrations sudo yum autoremove zypper (SLES) sudo zypper -n remove newrelic-infra-integrations --clean-deps Copy Install your integrations one by one following these instructions. To replicate the integrations package, you will need to install all the available integrations again.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 306.81223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "sections": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Uninstalling the <em>infrastructure</em> <em>agent</em> does not directly affect any of your <em>infrastructure</em> Integrations: if you <em>uninstall</em> the <em>agent</em>, your integrations will remain. Similarly, if you disable or <em>uninstall</em> your integrations, the <em>infrastructure</em> <em>agent</em> will remain. To <em>uninstall</em> any of your integrations"
      },
      "id": "603ea831196a67fc6fa83db5"
    },
    {
      "sections": [
        "Update the infrastructure agent",
        "Tip",
        "View the infrastructure agent version",
        "Update the agent for installs using the package manager",
        "Update using apt (Debian, Ubuntu)",
        "Update using yum (Amazon Linux, CentOS, RHEL)",
        "Update using Zypper (SLES)",
        "Update on Windows Server (32 bits)",
        "Update on Windows Server (64 bits)",
        "Update on macOS",
        "Update with config management tools",
        "Update the agent for assisted and manual tarball installs",
        "Important",
        "Update the containerized version of the agent",
        "Identify outdated agent versions from the UI"
      ],
      "title": "Update the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1ddaa00ceaf5936084d25f207f868d89fd0957f6",
      "image": "https://docs.newrelic.com/static/06b72c159cd2d6b502bb7cbab7a98e67/103b3/ebs.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/update-infrastructure-agent/",
      "published_at": "2022-01-08T09:41:32Z",
      "updated_at": "2021-08-20T17:30:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to update the infrastructure agent to the latest version for Linux and Windows servers. Tip To install the infrastructure agent for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To uninstall the infrastructure agent, see Uninstall the Infrastructure agent. View the infrastructure agent version The infrastructure agent does not update itself automatically. Check the infrastructure agent release notes to make sure you have the latest agent version. To view the current infrastructure agent version for a host, use any of these options: Go to one.newrelic.com > Infrastructure > Settings > Agents > Agent version. Go to one.newrelic.com > Infrastructure > Hosts > (select a host). Create a query for SystemSample. Update the agent for installs using the package manager If you used the default installation process, use your package manager to update the program and its dependencies to the latest version. Here are examples for some common systems: Update using apt ( Debian, Ubuntu) To manually update the infrastructure agent with apt-get: sudo apt-get update && sudo apt-get install --only-upgrade newrelic-infra -y Copy Update using yum ( Amazon Linux, CentOS, RHEL) To manually update the infrastructure agent with yum: sudo yum update newrelic-infra -y Copy After updating you may need to start the agent. Update using Zypper ( SLES) To manually update the infrastructure agent with Zypper: sudo zypper -n update newrelic-infra Copy After updating you may need to start the agent. Update on Windows Server (32 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy After updating you may need to start the agent. Update on Windows Server (64 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy After updating you may need to start the agent. Update on macOS To manually update the infrastructure agent on macOS, open the terminal and run the following commands: Stop the service (if already running): brew services stop newrelic-infra-agent Copy Uninstall the agent: brew update newrelic-infrastructure Copy Update with config management tools To update the infrastructure agent using your configuration management tool, see the documentation for your tool: Configure with Ansible Configure with Chef Configure with AWS Elastic Beanstalk Configure with Puppet Update the agent for assisted and manual tarball installs Important Since there are are no automated scripts, old files may remain when you update. Be sure to manually remove outdated files. To update the agent, download the file again and follow the installation procedure for Linux (assisted or manual) or Windows (assisted or manual). This will overwrite your old installation. Update the containerized version of the agent Use the latest label to ensure that our Docker image is automatically updated. Identify outdated agent versions from the UI You can use the Infrastructure monitoring UI to search for outdated agent versions: Go to one.newrelic.com > Infrastructure > Inventory * * * * . In the search bar, type newrelic-infra. Select a group's dropdown to see the agent versions for that group. To manually check the infrastructure agent versions, you can log onto a server and run newrelic-infra --version, or the applicable command for your package manager.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.8933,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Update</em> the <em>infrastructure</em> <em>agent</em>",
        "sections": "<em>Update</em> <em>the</em> <em>agent</em> for <em>installs</em> using <em>the</em> package manager",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to <em>update</em> the <em>infrastructure</em> <em>agent</em> to the latest version for Linux and Windows servers. Tip To <em>install</em> the <em>infrastructure</em> <em>agent</em> for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To <em>uninstall</em> the <em>infrastructure</em> <em>agent</em>"
      },
      "id": "60440e8fe7b9d252a2579a19"
    },
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.27084,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid inaccurate"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-integrations": [
    {
      "sections": [
        "Uninstall the infrastructure agent",
        "Uninstall the Linux infrastructure agent",
        "Uninstall with apt (Debian, Ubuntu)",
        "Uninstall with yum (Amazon Linux, CentOS, RHEL)",
        "Uninstall with Zypper (SLES)",
        "Important",
        "Uninstall the Windows infrastructure agent",
        "Tip",
        "Uninstall the macOS infrastructure agent",
        "Uninstall using config management tools",
        "Optional: Purge remaining files"
      ],
      "title": "Uninstall the infrastructure agent ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "d99fe6244698657f7f2e67bc6f6fcbfc9d8ffbf9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-agent/",
      "published_at": "2022-01-08T13:03:25Z",
      "updated_at": "2021-08-26T14:37:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Your hosts are added automatically when you install the infrastructure agent for your Linux or Windows operating system, or update the agent. Similarly, your hosts disappear automatically when you uninstall the agent. You do not need to manually add or remove your hosts. Uninstalling the agent does not directly affect any of your integrations. To uninstall an integration, see Uninstall integrations. Uninstall the Linux infrastructure agent If you used the default install procedure for your infrastructure agent for Linux environments, use your package management tools to uninstall it. You do not need to stop the service before running the uninstall command. Uninstall with apt ( Debian, Ubuntu) Execute the following command as root: sudo apt-get remove newrelic-infra Copy Uninstall with yum ( Amazon Linux, CentOS, RHEL) Execute the following command as root: sudo yum remove newrelic-infra Copy Uninstall with Zypper ( SLES) Execute the following command as root: sudo zypper -n remove newrelic-infra Copy If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall the Windows infrastructure agent Tip Requires Administrator rights in your Windows admin group. If you used the default install procedure for the infrastructure agent for Windows environments, to uninstall: Stop the infrastructure agent. From the Windows Control Panel, use the Add/Remove Programs and Features tool to uninstall the infrastructure agent. From the Windows Program Files, manually delete the New Relic folder to delete all files associated with the infrastructure agent for Windows. If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall the macOS infrastructure agent If you used the default install procedure for the infrastructure agent for macOS environments, to uninstall: Stop the infrastructure agent: brew services stop newrelic-infra-agent Copy From the terminal, run the uninstall command: brew uninstall newrelic-infra-agent Copy Uninstall using config management tools To uninstall the infrastructure agent if you used a configuration management tool: Config management tools Uninstall New Relic infrastructure agent Ansible Set the 'agent_state' parameter to 'absent' Chef Set the 'agent_action' node to uninstall AWS Elastic Beanstalk Remove newrelic.config from .ebextensions, then deploy. Puppet Set the 'ensure' parameter to 'absent' Optional: Purge remaining files If you use standard package management tools for your selected platform, the uninstall process typically leaves configuration and other miscellaneous files. If you need to completely purge any of the remaining files after uninstalling the New Relic infrastructure agent, follow standard procedures for your operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 289.702,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em> ",
        "sections": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Your hosts are added automatically when you <em>install</em> the <em>infrastructure</em> <em>agent</em> for your Linux or Windows operating system, or <em>update</em> the <em>agent</em>. Similarly, your hosts disappear automatically when you <em>uninstall</em> the <em>agent</em>. You do not need to manually add or remove your hosts. Uninstalling the <em>agent</em> does"
      },
      "id": "603ea0e964441f31114e885c"
    },
    {
      "sections": [
        "Update the infrastructure agent",
        "Tip",
        "View the infrastructure agent version",
        "Update the agent for installs using the package manager",
        "Update using apt (Debian, Ubuntu)",
        "Update using yum (Amazon Linux, CentOS, RHEL)",
        "Update using Zypper (SLES)",
        "Update on Windows Server (32 bits)",
        "Update on Windows Server (64 bits)",
        "Update on macOS",
        "Update with config management tools",
        "Update the agent for assisted and manual tarball installs",
        "Important",
        "Update the containerized version of the agent",
        "Identify outdated agent versions from the UI"
      ],
      "title": "Update the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1ddaa00ceaf5936084d25f207f868d89fd0957f6",
      "image": "https://docs.newrelic.com/static/06b72c159cd2d6b502bb7cbab7a98e67/103b3/ebs.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/update-infrastructure-agent/",
      "published_at": "2022-01-08T09:41:32Z",
      "updated_at": "2021-08-20T17:30:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to update the infrastructure agent to the latest version for Linux and Windows servers. Tip To install the infrastructure agent for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To uninstall the infrastructure agent, see Uninstall the Infrastructure agent. View the infrastructure agent version The infrastructure agent does not update itself automatically. Check the infrastructure agent release notes to make sure you have the latest agent version. To view the current infrastructure agent version for a host, use any of these options: Go to one.newrelic.com > Infrastructure > Settings > Agents > Agent version. Go to one.newrelic.com > Infrastructure > Hosts > (select a host). Create a query for SystemSample. Update the agent for installs using the package manager If you used the default installation process, use your package manager to update the program and its dependencies to the latest version. Here are examples for some common systems: Update using apt ( Debian, Ubuntu) To manually update the infrastructure agent with apt-get: sudo apt-get update && sudo apt-get install --only-upgrade newrelic-infra -y Copy Update using yum ( Amazon Linux, CentOS, RHEL) To manually update the infrastructure agent with yum: sudo yum update newrelic-infra -y Copy After updating you may need to start the agent. Update using Zypper ( SLES) To manually update the infrastructure agent with Zypper: sudo zypper -n update newrelic-infra Copy After updating you may need to start the agent. Update on Windows Server (32 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy After updating you may need to start the agent. Update on Windows Server (64 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy After updating you may need to start the agent. Update on macOS To manually update the infrastructure agent on macOS, open the terminal and run the following commands: Stop the service (if already running): brew services stop newrelic-infra-agent Copy Uninstall the agent: brew update newrelic-infrastructure Copy Update with config management tools To update the infrastructure agent using your configuration management tool, see the documentation for your tool: Configure with Ansible Configure with Chef Configure with AWS Elastic Beanstalk Configure with Puppet Update the agent for assisted and manual tarball installs Important Since there are are no automated scripts, old files may remain when you update. Be sure to manually remove outdated files. To update the agent, download the file again and follow the installation procedure for Linux (assisted or manual) or Windows (assisted or manual). This will overwrite your old installation. Update the containerized version of the agent Use the latest label to ensure that our Docker image is automatically updated. Identify outdated agent versions from the UI You can use the Infrastructure monitoring UI to search for outdated agent versions: Go to one.newrelic.com > Infrastructure > Inventory * * * * . In the search bar, type newrelic-infra. Select a group's dropdown to see the agent versions for that group. To manually check the infrastructure agent versions, you can log onto a server and run newrelic-infra --version, or the applicable command for your package manager.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.8933,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Update</em> the <em>infrastructure</em> <em>agent</em>",
        "sections": "<em>Update</em> <em>the</em> <em>agent</em> for <em>installs</em> using <em>the</em> package manager",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to <em>update</em> the <em>infrastructure</em> <em>agent</em> to the latest version for Linux and Windows servers. Tip To <em>install</em> the <em>infrastructure</em> <em>agent</em> for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To <em>uninstall</em> the <em>infrastructure</em> <em>agent</em>"
      },
      "id": "60440e8fe7b9d252a2579a19"
    },
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.27084,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid inaccurate"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/update-infrastructure-agent": [
    {
      "sections": [
        "Uninstall infrastructure integrations",
        "Cloud integrations",
        "AWS",
        "Azure",
        "Google Cloud Platform (GCP)",
        "On-host integrations",
        "Apache",
        "Cassandra",
        "Kubernetes",
        "MySQL",
        "NGINX",
        "Redis",
        "StatsD",
        "Moving away from the integrations package",
        "Uninstall package"
      ],
      "title": "Uninstall infrastructure integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1e9232193cbf71bdbe1a6c6d0374ed0d6b7e7b0f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-integrations/",
      "published_at": "2022-01-08T13:04:05Z",
      "updated_at": "2021-10-30T20:41:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Uninstalling the infrastructure agent does not directly affect any of your infrastructure Integrations: if you uninstall the agent, your integrations will remain. Similarly, if you disable or uninstall your integrations, the infrastructure agent will remain. To uninstall any of your integrations, follow the procedure corresponding to the type of integration. Cloud integrations AWS You can disable infrastructure AWS integrations and still retain the connection between your AWS account and New Relic. We recommend not to disable your EC2 and EBS integrations because those add important metadata to your infrastructure data. If you want to... Do this Disable one or more AWS service integrations To disable services while keeping your AWS account linked to New Relic: From one.newrelic.com > Infrastructure, select AWS > Manage services. From your Edit AWS account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all AWS integrations To disconnect your AWS account completely from New Relic, you need to unlink your AWS account. This disables all New Relic integrations associated with that AWS account. Go to one.newrelic.com > Infrastructure > AWS > Manage services. From your Edit AWS account page, select Unlink this account. Save your changes. Sign in to AWS and select Services > IAM > Roles. Select the checkbox for the role you want to delete, then select Role Actions > Delete Role. Unlinking your AWS account will disable the trust relationship set up via your ARN. Azure If you want to... Do this Disable one or more Azure service integrations To disable services while keeping your Azure account linked to New Relic: Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all Azure integrations To disconnect your Azure account completely from New Relic, you need to unlink your Azure account. This requires being either the user who registered the app or an administrator. This procedure will disable all New Relic integrations associated with that Azure account. Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, select Unlink this account. Save your changes. Sign in to Azure and go into All Services > Identity > App registrations, or go to Azure Active Directory service and select App registrations. Find the registered app (the recommended name is NewRelic-Integrations). To see the full list of available apps, select the dropdown menu beside the search field and select All apps. Select the app and, on the panel that opens, select Delete. Google Cloud Platform (GCP) If you want to... Do this Disable one or more GCP service integrations To disable services while keeping your GCP account linked to New Relic: From one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all GCP integrations To disconnect your GCP account completely from New Relic, you need to unlink your GCP account. This disables all New Relic integrations associated with that GCP account. If you registered the GCP project using a User account, follow these steps. Go to one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, select Unlink this account. Save your changes. If you registered the GCP project using a Service account, follow these steps. If you are deleting a custom role, be aware that this role may be used for other purposes besides New Relic access. Sign in to New Relic and go to Infrastructure > Integrations > Google Cloud Platform. For a standard (non-custom) user role, select Manage Services for the account you want to remove. Copy the value of User and save it. OR For a custom user role, go to IAM > admin > Roles, search for the role, select it, and select DELETE. You are now finished and can skip the remaining steps. Standard (non-custom) user role: Sign in to Google Cloud and select the correct project in the Select a project box. From the navigation menu, select IAM & admin > IAM. Search for and select the user value you saved, then select REMOVE. On-host integrations If you used the integrations package, see the integrations package instructions. Here are some examples: Apache Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-apache yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-apache zypper (SLES) sudo zypper -n remove nri-apache Cassandra Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-cassandra yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-cassandra zypper (SLES) sudo zypper -n remove nri-cassandra Kubernetes Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy If you used Helm to install the Kubernetes integration, just uninstall it using Helm: helm uninstall --namespace NAMESPACE_USED_DURING_INSTALLATION RELEASE_NAME Copy If you installed Kubernetes integration using a manifest, use the same manifest to uninstall it: kubectl delete -f newrelic-infrastructure.yaml Copy MySQL Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-mysql yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-mysql zypper (SLES) sudo zypper -n remove nri-mysql NGINX Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-nginx yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-nginx zypper (SLES) sudo zypper -n remove nri-nginx Redis Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-redis yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-redis zypper (SLES) sudo zypper -n remove nri-redis StatsD cd /path/to/statsd npm uninstall @newrelic/statsd-infra-backend From the StatsD config.js, remove the \"@newrelic/statsd-infra-backend\" entry from the list of backends. Restart StatsD. Moving away from the integrations package While it is still possible to use the integrations package, we recommend removing it completely and working with integrations on an individual basis. The last integration package contains the following versions of the integrations: Apache 1.1.2 Cassandra 2.0.3 MySQL 1.1.5 Nginx 1.0.2 Redis 1.0.1 If you remove the integrations package and want to continue using the related on-host integrations, you will need to install them one by one. To uninstall the package and re-install your integrations: Remove the integrations package by following these instructions. The config files from the old integrations will not be deleted, so you won’t have to configure them again. Uninstall package Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove newrelic-infra-integrations sudo apt-get autoremove yum (Amazon Linux, CentOS, or RHEL) sudo yum remove newrelic-infra-integrations sudo yum autoremove zypper (SLES) sudo zypper -n remove newrelic-infra-integrations --clean-deps Copy Install your integrations one by one following these instructions. To replicate the integrations package, you will need to install all the available integrations again.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 306.81223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "sections": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Uninstalling the <em>infrastructure</em> <em>agent</em> does not directly affect any of your <em>infrastructure</em> Integrations: if you <em>uninstall</em> the <em>agent</em>, your integrations will remain. Similarly, if you disable or <em>uninstall</em> your integrations, the <em>infrastructure</em> <em>agent</em> will remain. To <em>uninstall</em> any of your integrations"
      },
      "id": "603ea831196a67fc6fa83db5"
    },
    {
      "sections": [
        "Uninstall the infrastructure agent",
        "Uninstall the Linux infrastructure agent",
        "Uninstall with apt (Debian, Ubuntu)",
        "Uninstall with yum (Amazon Linux, CentOS, RHEL)",
        "Uninstall with Zypper (SLES)",
        "Important",
        "Uninstall the Windows infrastructure agent",
        "Tip",
        "Uninstall the macOS infrastructure agent",
        "Uninstall using config management tools",
        "Optional: Purge remaining files"
      ],
      "title": "Uninstall the infrastructure agent ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "d99fe6244698657f7f2e67bc6f6fcbfc9d8ffbf9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-agent/",
      "published_at": "2022-01-08T13:03:25Z",
      "updated_at": "2021-08-26T14:37:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Your hosts are added automatically when you install the infrastructure agent for your Linux or Windows operating system, or update the agent. Similarly, your hosts disappear automatically when you uninstall the agent. You do not need to manually add or remove your hosts. Uninstalling the agent does not directly affect any of your integrations. To uninstall an integration, see Uninstall integrations. Uninstall the Linux infrastructure agent If you used the default install procedure for your infrastructure agent for Linux environments, use your package management tools to uninstall it. You do not need to stop the service before running the uninstall command. Uninstall with apt ( Debian, Ubuntu) Execute the following command as root: sudo apt-get remove newrelic-infra Copy Uninstall with yum ( Amazon Linux, CentOS, RHEL) Execute the following command as root: sudo yum remove newrelic-infra Copy Uninstall with Zypper ( SLES) Execute the following command as root: sudo zypper -n remove newrelic-infra Copy If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall the Windows infrastructure agent Tip Requires Administrator rights in your Windows admin group. If you used the default install procedure for the infrastructure agent for Windows environments, to uninstall: Stop the infrastructure agent. From the Windows Control Panel, use the Add/Remove Programs and Features tool to uninstall the infrastructure agent. From the Windows Program Files, manually delete the New Relic folder to delete all files associated with the infrastructure agent for Windows. If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall the macOS infrastructure agent If you used the default install procedure for the infrastructure agent for macOS environments, to uninstall: Stop the infrastructure agent: brew services stop newrelic-infra-agent Copy From the terminal, run the uninstall command: brew uninstall newrelic-infra-agent Copy Uninstall using config management tools To uninstall the infrastructure agent if you used a configuration management tool: Config management tools Uninstall New Relic infrastructure agent Ansible Set the 'agent_state' parameter to 'absent' Chef Set the 'agent_action' node to uninstall AWS Elastic Beanstalk Remove newrelic.config from .ebextensions, then deploy. Puppet Set the 'ensure' parameter to 'absent' Optional: Purge remaining files If you use standard package management tools for your selected platform, the uninstall process typically leaves configuration and other miscellaneous files. If you need to completely purge any of the remaining files after uninstalling the New Relic infrastructure agent, follow standard procedures for your operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 289.702,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em> ",
        "sections": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Your hosts are added automatically when you <em>install</em> the <em>infrastructure</em> <em>agent</em> for your Linux or Windows operating system, or <em>update</em> the <em>agent</em>. Similarly, your hosts disappear automatically when you <em>uninstall</em> the <em>agent</em>. You do not need to manually add or remove your hosts. Uninstalling the <em>agent</em> does"
      },
      "id": "603ea0e964441f31114e885c"
    },
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.27084,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid inaccurate"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/windows-installation/install-infrastructure-monitoring-agent-windows": [
    {
      "sections": [
        "Zip manual install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Install the service script",
        "Configuration file",
        "Changing the config file's location",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "What's next?"
      ],
      "title": "Zip manual install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "f7c89a92aefa26a400384c4334bcdc876dd07546",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-manual-install-infrastructure-agent-windows/",
      "published_at": "2022-01-08T09:09:09Z",
      "updated_at": "2021-10-13T02:51:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom installation process for the infrastructure agent for Windows allows you to tailor all aspects of the installation. You can place files and folders wherever you want on your filesystem. This method gives you full control of the installation. You are responsible for placing the files in the correct folders, providing the correct configuration values, and ensuring the agent has all the right permissions. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Install the service script. Optionally, you can: Change the location of the configuration file. Configure the plugin directory. Configure the agent directory. Configure the log file. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. For more information, see our documentation about troubleshooting a running infrastructure agent. Install the service script To proceed with the installation, you need to create the service. Check the file provided in the zip file for reference: C:\\Program Files\\New Relic\\newrelic-infra\\installer.ps1 Copy Configuration file The infrastructure agent depends on a configuration file, usually named newrelic-infra.yml, to configure the agent's behavior. This file is placed in the same folder with the agent. You can create a new config file based on the config file template. For more information, see how to configure the agent. Changing the config file's location By default, the configuration file is located in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml. To change the location of the configuration file: Execute the command regedit.exe. Browse to the folder Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\newrelic-infra\\ImagePath. Retrieve the ImagePath key. If the agent binary is on the default path, look for the key at C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe. Use the -config flag to add the new location of the configuration file to the key: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe -config c:\\config.yaml Copy Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named integration-name-config.yml by default. This config file is placed in the predefined location C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is C:\\Program Files\\New Relic\\newrelic-infra\\. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. The agent also uses a different folder, app_data_dir, to store data. By default it points to C:\\ProgramData\\New Relic\\newrelic-infra\\. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 257.58585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>installation</em> process for the <em>infrastructure</em> <em>agent</em> for <em>Windows</em> allows you to tailor all aspects of the <em>installation</em>. You can place files and folders wherever you want on your filesystem. This method gives you full control of the <em>installation</em>. You are responsible for placing the files"
      },
      "id": "603ea57b196a678ad3a83dbf"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2022-01-08T13:04:04Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.38004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>, you can make the changes you need to the <em>installation</em> script we provide so you can adapt it to your environment. Before <em>installation</em>, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.27081,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid inaccurate"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Windows",
        "Install for Windows Server and Windows 10 using our guided install",
        "Install using our step-by-step instructions",
        "PowerShell install",
        "32-bit Windows",
        "64-bit Windows",
        "Step-by-step install",
        "Important",
        "Scripted installation",
        "Install using zip files",
        "Caution",
        "What's next?",
        "Update the agent"
      ],
      "title": "Install the infrastructure monitoring agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "32766f16044664c9c7d66075801930ff53ca5c49",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/install-infrastructure-monitoring-agent-windows/",
      "published_at": "2022-01-08T09:40:48Z",
      "updated_at": "2021-11-06T11:44:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring agent for Windows, you can monitor individual servers and also analyze how your service performs as a whole. The Windows agent can run on your own hardware or in cloud systems such as Amazon EC2 or Windows Azure, and supports Windows Server and Windows 10. You can also install with Chef. Before installation, be sure to review the requirements. Install for Windows Server and Windows 10 using our guided install If you haven't already, create a New Relic account. It's free, forever. To install the infrastructure monitoring agent for Windows, you can use our guided install. If you're in the EU, try our euro guided install. Get an account Guided install EU guided install Install using our step-by-step instructions If guided install doesn't work, use our PowerShell script, or follow the step-by-step instructions: PowerShell install Review the agent requirements and supported operating systems. Open the PowerShell as administrator and run the following command: 32-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy 64-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy For a scripted installation, you can pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Step-by-step install Review the infrastructure monitoring agent requirements and supported operating systems. Download the latest .MSI installer image from: 32-bit Windows https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi​ Copy 64-bit Windows https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Copy Important Do not double-click the installer. This will not fully install the local agent and can result in permissions issues. In an admin account, run the install script using an absolute path. 32-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy 64-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy Scripted installation For a scripted installation, you can also pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Add your New Relic license key to the license_key attribute in newrelic-infra.yml, located in C:\\Program Files\\New Relic\\newrelic-infra\\. When finished, the contents of newrelic-infra.yml should resemble the following: license_key: YOUR_LICENSE_KEY Copy Start the newrelic-infra service. To start from the Windows command prompt, run: net start newrelic-infra Copy Wait a few minutes, then view your server in the Infrastructure UI. If no data appears after waiting a few minutes, follow the troubleshooting steps. Install using zip files For custom setup scenarios, you can install the infrastructure monitoring agent using our zip files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment. Caution Installing the infrastructure monitoring agent using zip files is not supported. What's next? The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services. Update the agent To upgrade to the latest version, follow standard procedures to update the infrastructure monitoring agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 266.54227,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". You can also <em>install</em> with Chef. Before <em>installation</em>, be sure to review the requirements. <em>Install</em> for <em>Windows</em> Server and <em>Windows</em> 10 using our guided <em>install</em> If you haven&#x27;t already, create a New Relic account. It&#x27;s free, forever. To <em>install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>, you can use"
      },
      "id": "6044672464441f9bb6378ed9"
    },
    {
      "sections": [
        "Zip manual install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Install the service script",
        "Configuration file",
        "Changing the config file's location",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "What's next?"
      ],
      "title": "Zip manual install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "f7c89a92aefa26a400384c4334bcdc876dd07546",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-manual-install-infrastructure-agent-windows/",
      "published_at": "2022-01-08T09:09:09Z",
      "updated_at": "2021-10-13T02:51:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom installation process for the infrastructure agent for Windows allows you to tailor all aspects of the installation. You can place files and folders wherever you want on your filesystem. This method gives you full control of the installation. You are responsible for placing the files in the correct folders, providing the correct configuration values, and ensuring the agent has all the right permissions. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Install the service script. Optionally, you can: Change the location of the configuration file. Configure the plugin directory. Configure the agent directory. Configure the log file. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. For more information, see our documentation about troubleshooting a running infrastructure agent. Install the service script To proceed with the installation, you need to create the service. Check the file provided in the zip file for reference: C:\\Program Files\\New Relic\\newrelic-infra\\installer.ps1 Copy Configuration file The infrastructure agent depends on a configuration file, usually named newrelic-infra.yml, to configure the agent's behavior. This file is placed in the same folder with the agent. You can create a new config file based on the config file template. For more information, see how to configure the agent. Changing the config file's location By default, the configuration file is located in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml. To change the location of the configuration file: Execute the command regedit.exe. Browse to the folder Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\newrelic-infra\\ImagePath. Retrieve the ImagePath key. If the agent binary is on the default path, look for the key at C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe. Use the -config flag to add the new location of the configuration file to the key: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe -config c:\\config.yaml Copy Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named integration-name-config.yml by default. This config file is placed in the predefined location C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is C:\\Program Files\\New Relic\\newrelic-infra\\. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. The agent also uses a different folder, app_data_dir, to store data. By default it points to C:\\ProgramData\\New Relic\\newrelic-infra\\. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 257.58585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>installation</em> process for the <em>infrastructure</em> <em>agent</em> for <em>Windows</em> allows you to tailor all aspects of the <em>installation</em>. You can place files and folders wherever you want on your filesystem. This method gives you full control of the <em>installation</em>. You are responsible for placing the files"
      },
      "id": "603ea57b196a678ad3a83dbf"
    },
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.27081,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid inaccurate"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-manual-install-infrastructure-agent-windows": [
    {
      "sections": [
        "Install the infrastructure monitoring agent for Windows",
        "Install for Windows Server and Windows 10 using our guided install",
        "Install using our step-by-step instructions",
        "PowerShell install",
        "32-bit Windows",
        "64-bit Windows",
        "Step-by-step install",
        "Important",
        "Scripted installation",
        "Install using zip files",
        "Caution",
        "What's next?",
        "Update the agent"
      ],
      "title": "Install the infrastructure monitoring agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "32766f16044664c9c7d66075801930ff53ca5c49",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/install-infrastructure-monitoring-agent-windows/",
      "published_at": "2022-01-08T09:40:48Z",
      "updated_at": "2021-11-06T11:44:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring agent for Windows, you can monitor individual servers and also analyze how your service performs as a whole. The Windows agent can run on your own hardware or in cloud systems such as Amazon EC2 or Windows Azure, and supports Windows Server and Windows 10. You can also install with Chef. Before installation, be sure to review the requirements. Install for Windows Server and Windows 10 using our guided install If you haven't already, create a New Relic account. It's free, forever. To install the infrastructure monitoring agent for Windows, you can use our guided install. If you're in the EU, try our euro guided install. Get an account Guided install EU guided install Install using our step-by-step instructions If guided install doesn't work, use our PowerShell script, or follow the step-by-step instructions: PowerShell install Review the agent requirements and supported operating systems. Open the PowerShell as administrator and run the following command: 32-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy 64-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy For a scripted installation, you can pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Step-by-step install Review the infrastructure monitoring agent requirements and supported operating systems. Download the latest .MSI installer image from: 32-bit Windows https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi​ Copy 64-bit Windows https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Copy Important Do not double-click the installer. This will not fully install the local agent and can result in permissions issues. In an admin account, run the install script using an absolute path. 32-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy 64-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy Scripted installation For a scripted installation, you can also pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Add your New Relic license key to the license_key attribute in newrelic-infra.yml, located in C:\\Program Files\\New Relic\\newrelic-infra\\. When finished, the contents of newrelic-infra.yml should resemble the following: license_key: YOUR_LICENSE_KEY Copy Start the newrelic-infra service. To start from the Windows command prompt, run: net start newrelic-infra Copy Wait a few minutes, then view your server in the Infrastructure UI. If no data appears after waiting a few minutes, follow the troubleshooting steps. Install using zip files For custom setup scenarios, you can install the infrastructure monitoring agent using our zip files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment. Caution Installing the infrastructure monitoring agent using zip files is not supported. What's next? The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services. Update the agent To upgrade to the latest version, follow standard procedures to update the infrastructure monitoring agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 266.54227,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". You can also <em>install</em> with Chef. Before <em>installation</em>, be sure to review the requirements. <em>Install</em> for <em>Windows</em> Server and <em>Windows</em> 10 using our guided <em>install</em> If you haven&#x27;t already, create a New Relic account. It&#x27;s free, forever. To <em>install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>, you can use"
      },
      "id": "6044672464441f9bb6378ed9"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2022-01-08T13:04:04Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.38004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>, you can make the changes you need to the <em>installation</em> script we provide so you can adapt it to your environment. Before <em>installation</em>, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2022-01-08T13:01:11Z",
      "updated_at": "2021-12-25T15:23:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). macOS: 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux 2 All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 and their service packs. macOS 10.15 (Catalina), 11 (Big Sur), 12 (Monterey). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.27081,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid inaccurate"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring": [
    {
      "sections": [
        "Dimensional metric equivalents for the agent and on-host integrations",
        "BETA FEATURE"
      ],
      "title": "Dimensional metric equivalents for the agent and on-host integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2022-01-08T07:23:37Z",
      "updated_at": "2021-11-26T06:28:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. In the past, our infrastructure agent and on-host integrations have reported metrics as attributes attached to events, also known as \"sample data.\" We have now made these metrics also available as dimensional metrics, a data format that allows for improved analysis and aggregation over time. The following table presents the equivalent dimensional metric names for our infrastructure agent and for our on-host integrations. For tips on how to query dimensional metrics, see Query dimensional metrics. Integration Dimensional metric name (new) Sample metric name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTim",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.71799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "sections": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "BETA FEATURE This feature is currently in beta. In the past, our <em>infrastructure</em> agent and on-host integrations have reported metrics as attributes attached to events, also known as &quot;sample <em>data</em>.&quot; We have now made these metrics also available as dimensional metrics, a <em>data</em> format that allows"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2022-01-08T09:41:32Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.66985,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host filter sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    },
    {
      "sections": [
        "Manage infrastructure data reporting"
      ],
      "title": "Manage infrastructure data reporting",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "7cd87ba8f7686e9233f4171021607d499bf6bc72",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting/",
      "published_at": "2022-01-08T09:09:08Z",
      "updated_at": "2021-03-16T07:33:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the infrastructure agent or on-host integrations that report data via the infrastructure agent, there are several ways to configure data reporting. Here are two common options for managing data reporting: Enable/disable process metrics Select specific attributes to report For other agent configuration options, see Configuration. For our infrastructure integrations, you can also change the frequency of data reporting: For on-host integrations: use a specific integration's interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic data management in general, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.3926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "sections": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " configuration options, see Configuration. For our <em>infrastructure</em> integrations, you can also change the frequency of <em>data</em> reporting: For on-host integrations: use a specific integration&#x27;s interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic <em>data</em> management in general, see <em>Manage</em> <em>data</em>."
      },
      "id": "603e775a196a67f470a83de4"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data": [
    {
      "sections": [
        "Dimensional metric equivalents for the agent and on-host integrations",
        "BETA FEATURE"
      ],
      "title": "Dimensional metric equivalents for the agent and on-host integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2022-01-08T07:23:37Z",
      "updated_at": "2021-11-26T06:28:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. In the past, our infrastructure agent and on-host integrations have reported metrics as attributes attached to events, also known as \"sample data.\" We have now made these metrics also available as dimensional metrics, a data format that allows for improved analysis and aggregation over time. The following table presents the equivalent dimensional metric names for our infrastructure agent and for our on-host integrations. For tips on how to query dimensional metrics, see Query dimensional metrics. Integration Dimensional metric name (new) Sample metric name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTim",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.71799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "sections": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "BETA FEATURE This feature is currently in beta. In the past, our <em>infrastructure</em> agent and on-host integrations have reported metrics as attributes attached to events, also known as &quot;sample <em>data</em>.&quot; We have now made these metrics also available as dimensional metrics, a <em>data</em> format that allows"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "View logs for your APM and infrastructure data",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-13T14:37:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. View logs for your APM and infrastructure data You can also bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.04555,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "View logs for <em>your</em> APM and <em>infrastructure</em> <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "The integration of APM and <em>infrastructure</em> <em>data</em> lets you see <em>your</em> APM <em>data</em> and <em>infrastructure</em> <em>data</em> side by side so you can find the root cause of problems more quickly. The main ways to find and use APM <em>data</em> in <em>infrastructure</em> monitoring are: View APM charts on <em>Infrastructure</em> monitoring UI pages"
      },
      "id": "603e88b2e7b9d246932a07f6"
    },
    {
      "sections": [
        "Manage infrastructure data reporting"
      ],
      "title": "Manage infrastructure data reporting",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "7cd87ba8f7686e9233f4171021607d499bf6bc72",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting/",
      "published_at": "2022-01-08T09:09:08Z",
      "updated_at": "2021-03-16T07:33:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the infrastructure agent or on-host integrations that report data via the infrastructure agent, there are several ways to configure data reporting. Here are two common options for managing data reporting: Enable/disable process metrics Select specific attributes to report For other agent configuration options, see Configuration. For our infrastructure integrations, you can also change the frequency of data reporting: For on-host integrations: use a specific integration's interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic data management in general, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.3926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "sections": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " configuration options, see Configuration. For our <em>infrastructure</em> integrations, you can also change the frequency of <em>data</em> reporting: For on-host integrations: use a specific integration&#x27;s interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic <em>data</em> management in general, see <em>Manage</em> <em>data</em>."
      },
      "id": "603e775a196a67f470a83de4"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics": [
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "View logs for your APM and infrastructure data",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-13T14:37:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. View logs for your APM and infrastructure data You can also bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.04555,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "View logs for <em>your</em> APM and <em>infrastructure</em> <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "The integration of APM and <em>infrastructure</em> <em>data</em> lets you see <em>your</em> APM <em>data</em> and <em>infrastructure</em> <em>data</em> side by side so you can find the root cause of problems more quickly. The main ways to find and use APM <em>data</em> in <em>infrastructure</em> monitoring are: View APM charts on <em>Infrastructure</em> monitoring UI pages"
      },
      "id": "603e88b2e7b9d246932a07f6"
    },
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2022-01-08T09:41:32Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.66985,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host filter sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    },
    {
      "sections": [
        "Manage infrastructure data reporting"
      ],
      "title": "Manage infrastructure data reporting",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "7cd87ba8f7686e9233f4171021607d499bf6bc72",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting/",
      "published_at": "2022-01-08T09:09:08Z",
      "updated_at": "2021-03-16T07:33:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the infrastructure agent or on-host integrations that report data via the infrastructure agent, there are several ways to configure data reporting. Here are two common options for managing data reporting: Enable/disable process metrics Select specific attributes to report For other agent configuration options, see Configuration. For our infrastructure integrations, you can also change the frequency of data reporting: For on-host integrations: use a specific integration's interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic data management in general, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.3926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "sections": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " configuration options, see Configuration. For our <em>infrastructure</em> integrations, you can also change the frequency of <em>data</em> reporting: For on-host integrations: use a specific integration&#x27;s interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic <em>data</em> management in general, see <em>Manage</em> <em>data</em>."
      },
      "id": "603e775a196a67f470a83de4"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting": [
    {
      "sections": [
        "Dimensional metric equivalents for the agent and on-host integrations",
        "BETA FEATURE"
      ],
      "title": "Dimensional metric equivalents for the agent and on-host integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2022-01-08T07:23:37Z",
      "updated_at": "2021-11-26T06:28:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. In the past, our infrastructure agent and on-host integrations have reported metrics as attributes attached to events, also known as \"sample data.\" We have now made these metrics also available as dimensional metrics, a data format that allows for improved analysis and aggregation over time. The following table presents the equivalent dimensional metric names for our infrastructure agent and for our on-host integrations. For tips on how to query dimensional metrics, see Query dimensional metrics. Integration Dimensional metric name (new) Sample metric name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTim",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.71799,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "sections": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "BETA FEATURE This feature is currently in beta. In the past, our <em>infrastructure</em> agent and on-host integrations have reported metrics as attributes attached to events, also known as &quot;sample <em>data</em>.&quot; We have now made these metrics also available as dimensional metrics, a <em>data</em> format that allows"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "View logs for your APM and infrastructure data",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-13T14:37:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. View logs for your APM and infrastructure data You can also bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.04555,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "View logs for <em>your</em> APM and <em>infrastructure</em> <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "The integration of APM and <em>infrastructure</em> <em>data</em> lets you see <em>your</em> APM <em>data</em> and <em>infrastructure</em> <em>data</em> side by side so you can find the root cause of problems more quickly. The main ways to find and use APM <em>data</em> in <em>infrastructure</em> monitoring are: View APM charts on <em>Infrastructure</em> monitoring UI pages"
      },
      "id": "603e88b2e7b9d246932a07f6"
    },
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2022-01-08T09:41:32Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.66985,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host filter sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88638,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-app-service-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88638,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-application-gateway-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88638,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-containers-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88638,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    },
    {
      "sections": [
        "Azure Virtual Network monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Azure Virtual Network monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "21564ee0f8fc6acad9c0829b83bea6a569a03d2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration/",
      "published_at": "2022-01-08T08:58:46Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Virtual Network that reports data from your Virtual Network service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Network reports metric data about your virtual networks (VNets), like packets dropped per second or bytes forwarded per second. It also collects inventory data about the status and configuration of your account. You can monitor and alert on your Virtual Network data from New Relic, and you can create custom queries and chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Network integration: Polling interval: 1 minute for metrics; 5 minutes for inventory Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualNetworksPublicIpAddressSample event type. For more on how to find and use integration data, see Understand and use data. Metric data Here are the metrics reported by the Azure Virtual Network integration. For more about how data is structured and reported to New Relic, see Understand and use integration data. Metric Description BytesDroppedDDoS Average inbound bytes dropped per second, DDoS. BytesForwardedDDoS Average inbound bytes forwarded per second, DDoS. BytesInDDoS Average inbound bytes per second, DDoS. DDoSTriggerTCPPackets Average inbound TCP packets per second to trigger DDoS mitigation. DDoSTriggerUDPPackets Inbound UDP packets to trigger DDoS mitigation. IfUnderDDoSAttack Count of entities under DDoS attack. PacketsInDDoS Average inbound packets per second, DDoS PacketsDroppedDDoS Average inbound packets dropped per second, DDoS. PacketsForwardedDDoS Average inbound packets forwarded per second, DDoS. TCPBytesInDDoS Average inbound TCP bytes per second, DDoS. TCPBytesDroppedDDoS Average inbound TCP bytes dropped per second, DDoS. TCPBytesForwardedDDoS Average inbound TCP bytes forwarded per second, DDoS. TCPPacketsInDDoS Average inbound TCP packets per second, DDoS. TCPPacketsDroppedDDoS Average inbound TCP packets dropped per second, DDoS. TCPPacketsForwardedDDoS Average inbound TCP packets forwarded per second, DDoS. UDPPacketsInDDoS Average inbound UDP packets per second, DDoS. UDPPacketsDroppedDDoS Average inbound UDP packets dropped per second, DDoS. UDPPacketsForwardedDDoS Average inbound UDP packets forwarded per second, DDoS. UDPBytesInDDoS Average inbound UDP bytes per second, DDoS. UDPBytesDroppedDDoS Average inbound UDP bytes dropped per second, DDoS. UDPBytesForwardedDDoS Average inbound UDP bytes forwarded per second, DDoS. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Network integration reports this inventory data: Inventory category Data reported azure/virtualnetworks/peering This category includes Azure's Virtual Network peering. Data includes: allowVirtualNetworkAccess allowForwardedTraffic allowGatewayTransit useRemoteGateways virtualNetworkId remoteVirtualNetworkId peeringState provisioningState azure/virtualnetworks/subnet This category includes subnet and queue data. Data includes: addressPrefix networkSecurityGroupId routeTableId provisioningState virtualNetworkId fullSubnetName azure/virtualnetworks/virtual-network This category includes subscription data. Data includes: addressSpace dnsServers provisioningState resourceGroupName (deprecates ResourceGroup) azure/virtualnetworks/security-rule/ Security rules allow or deny inbound or outbound network traffic based on source or destination IP address, port, and protocol. Data includes: access description destinationAddressPrefix destinationPortRange direction priority protocol provisioningState regionName resourceGroupName sourceAddressPrefix sourcePortRange azure/virtualnetworks/security-group/ Security groups contain a list of security rules, and are used to limit network traffic to resources in a virtual network. Data includes: name networkInterfaceIds provisioningState regionName resourceGroupName subnetIds type azure/virtualnetworks/route/ This category includes route data. Data includes: destinationAddressPrefix name nextHopIpAddress nextHopType provisioningState regionName resourceGroupName routeTableId azure/virtualnetworks/route-table/ Route tables enable resources connected to any subnet in any virtual network to communicate with each other, and the Internet. Data includes: name provisioningState regionName resourceGroupName type azure/virtualnetworks/public-ip-address/ Public IP addresses are used for communication with the Internet, including Azure public-facing services. Data includes: assignedNetworkInterface idleTimeoutInMinutes ipAllocationMethod name provisionedState regionName resourceGroupName type version assignedLoadBalancer fqdn leafDomainLabel azure/virtualnetworks/network-interface/ A network interface enables an Azure Virtual Machine to communicate with Internet, Azure, and on-premises resources. Data includes: appliedDnsServers dnsServers isAcceleratedNetworkingEnabled isIpForwardingEnabled name networkSecurityGroup privateIp privateIpAllocationMethod provisioningState regionName resourceGroupName type virtualMachineId internalDomainNameSuffix macAddress azure/virtualnetworks/ip-configuration/ This category includes Azure's IP configuration Data includes: isPrimary name networkInterfaceId privateIp privateIpAllocationMethod privateIpVersion regionName resourceGroupName subnet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Virtual Network monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Virtual Network monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em>&#x27;s Virtual Network that reports data from your Virtual Network service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a7196a6713e6f7d95d"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-data-factory-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-database-postgresql-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-event-hub-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-express-route-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-firewalls-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-front-door-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-functions-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-key-vault-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-load-balancer-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-logic-apps-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-machine-learning-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-power-bi-dedicated-capacities-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-redis-cache-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.21371,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-service-bus-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-sql-managed-instances-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-storage-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machines-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure virtual machine scale sets monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Virtual machine scale sets ScaleSet data"
      ],
      "title": "Azure virtual machine scale sets monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "ee5b635c499f8780805c42b14c841b08d0a8cb08",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration/",
      "published_at": "2022-01-08T09:10:41Z",
      "updated_at": "2021-10-23T17:41:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Azure virtual machine scale sets data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure virtual machine scale sets integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. You can query and explore your data using the following event type: Entity Event type Provider ScaleSet AzureVirtualMachineScaleSetSample AzureVirtualMachineScaleSet For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure virtual machine scale sets data for ScaleSet. Virtual machine scale sets ScaleSet data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the Virtual Machine(s) networkInBytes Bytes The number of billable bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutBytes Bytes The number of billable bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic) diskReadBytes Bytes Bytes read from disk during monitoring period diskWriteBytes Bytes Bytes written to disk during monitoring period diskReadOperationsCountPerSecond CountPerSecond Disk Read IOPS diskWriteOperationsCountPerSecond CountPerSecond Disk Write IOPS cpuCreditsRemaining Count Total number of credits available to burst cpuCreditsConsumed Count Total number of credits consumed by the Virtual Machine dataDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period dataDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period dataDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period dataDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period dataDiskQueueDepth Count Data Disk Queue Depth(or Queue Length) osDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period for OS disk osDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period for OS disk osDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period for OS disk osDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period for OS disk osDiskQueueDepth Count OS Disk Queue Depth(or Queue Length) inboundFlows Count Inbound Flows are number of current flows in the inbound direction (traffic going into the VM) outboundFlows Count Outbound Flows are number of current flows in the outbound direction (traffic going out of the VM) inboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of inbound flows (traffic going into the VM) outboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of outbound flows (traffic going out of the VM) premiumDataDiskCacheReadHitPercent Percent Premium Data Disk Cache Read Hit premiumDataDiskCacheReadMissPercent Percent Premium Data Disk Cache Read Miss premiumOSDiskCacheReadHitPercent Percent Premium OS Disk Cache Read Hit premiumOSDiskCacheReadMissPercent Percent Premium OS Disk Cache Read Miss networkInTotalBytes Bytes The number of bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutTotalBytes Bytes The number of bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1255.6587,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": " data. Metric data This <em>integration</em> collects <em>Azure</em> <em>virtual</em> machine <em>scale</em> <em>sets</em> data for <em>ScaleSet</em>. <em>Virtual</em> machine <em>scale</em> <em>sets</em> <em>ScaleSet</em> data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the <em>Virtual</em> Machine(s) networkInBytes Bytes"
      },
      "id": "617daa0ee7b9d26e59c05ca8"
    },
    {
      "sections": [
        "Glossary of New Relic terms",
        "account dropdown",
        "account switcher",
        "administrator",
        "agent",
        "agent API",
        "aggregated metrics",
        "aggregation delay",
        "aggregation function",
        "aggregation method",
        "aggregation timer",
        "aggregation window",
        "alert",
        "alert condition",
        "alert evaluation",
        "alert policy",
        "apdex",
        "apdex_f",
        "apdex_t",
        "API (application programming interface)",
        "APM",
        "application",
        "application ID",
        "application name",
        "Applied Intelligence (AI)",
        "attribute",
        "availability monitoring",
        "browser",
        "Browser monitoring",
        "background external",
        "child account",
        "cloud-based integration",
        "collector",
        "Command line interface (CLI)",
        "compute unit (CU)",
        "condition_id",
        "CPM (calls per minute)",
        "CPU burn",
        "custom attribute",
        "custom dashboard",
        "custom event",
        "custom instrumentation",
        "custom metric",
        "data collector",
        "data explorer",
        "degradation period",
        "dimensional metric",
        "Docker",
        "downtime",
        "entity",
        "event",
        "expected error",
        "exporter",
        "Flex",
        "framework",
        "harvest cycle",
        "health status indicator",
        "host",
        "host ID",
        "ignored error",
        "incident",
        "Infrastructure monitoring",
        "Insights",
        "instance ID",
        "instrumentation",
        "integration",
        "interaction",
        "interaction trace",
        "inventory data",
        "key transaction",
        "launcher",
        "log",
        "Log monitoring",
        "Logs",
        "Logs in context",
        "master account",
        "metric",
        "metric timeslice",
        "metric grouping issue",
        "minion",
        "Mobile monitoring",
        "monitor",
        "NerdGraph",
        "Nerdlet",
        "Nerdpack",
        "New Relic Edge with Infinite Tracing",
        "New Relic One",
        "New Relic One catalog",
        "NRQL (New Relic query language)",
        "non-web transaction",
        "notification",
        "notification channel",
        "on-host integration",
        "owner",
        "page load timing",
        "parameter",
        "parent account",
        "permalink",
        "pinger",
        "polling interval (AWS)",
        "PPM (pages per minute)",
        "private location",
        "recovery period",
        "response time",
        "restricted user",
        "rollup",
        "root span",
        "RPM",
        "RUM (real user monitoring)",
        "runbook",
        "SAML (Security Assertion Markup Language)",
        "Selenium",
        "service",
        "signal",
        "signal filter",
        "span",
        "SSL certificate",
        "SSO (single sign on)",
        "streaming algorithm",
        "sub-accounts",
        "Synthetic monitoring",
        "target",
        "tag",
        "thresholds",
        "throughput",
        "tier",
        "time picker",
        "time range",
        "timeslice data",
        "trace",
        "traffic light",
        "transaction",
        "transaction trace",
        "UI",
        "user",
        "UTC",
        "value function (metrics)",
        "violation",
        "web external",
        "web transaction",
        "WebDriverJS",
        "workload"
      ],
      "title": "Glossary of New Relic terms",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Welcome to New Relic",
        "Get started"
      ],
      "external_id": "8f8fc1ec9f41e6a4d6b4e986e9b0589bc2ca1f86",
      "image": "https://docs.newrelic.com/static/44172b3e07c1f24191825360676b9d99/c1b63/account-dropdown.png",
      "url": "https://docs.newrelic.com/docs/glossary/glossary/",
      "published_at": "2022-01-09T01:45:37Z",
      "updated_at": "2022-01-08T01:45:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you're considering New Relic One or you're already using our capabilities, this glossary of common terminology can help. And if you don't already have a New Relic account, don't hesitate to sign up at newrelic.com/signup. It's free, forever! account dropdown In the upper right of the New Relic UI, the account dropdown gives you access to your account settings. If you're trying to switch between accounts, use the account switcher. account switcher If you have access to more than one account in a multi-account organization, you can use the account switcher to switch between accounts. This is located in the top right of most New Relic UI pages. For more on factors that affect access to accounts, see Factors affecting access. To find account settings, use the account dropdown. administrator A type of user role on a New Relic account. For more information, see Users. agent At New Relic, an agent is a piece of monitoring software that provides integrations with various technologies (for example, web frameworks, host operating systems, or database types). The agents send that data to New Relic, usually on a specific cadence. For more information, see: New Relic Instant Observability Install agents agent API Some New Relic agents have agent APIs that allow you to extend the functionality of an agent. You can use the API to control, customize and extend the functionality of the agent. Here are some agent API docs: APM agents: C SDK API Go agent API Java agent API .NET agent API Node.js agent API PHP agent API Ruby agent API Python agent API Browser agent: Browser agent API Mobile agents: iOS SDK API Android SDK API aggregated metrics Aggregated metric data summarizes calls to specific methods in your application, including how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on the New Relic tool and your subscription level. For more information, see the documentation about data retention. aggregation delay The length of time in seconds to wait for the aggregation window to fill with data. Required when using CADENCE or EVENT_FLOW aggreation_method types. aggregation function You can use NRQL query functions, such as sum(), average(), or latest() to choose how the data points in an aggregation window should be processed into a single data point. The single aggregated data point is what's passed through the alert evaluation process. aggregation method New Relic aggregates data into windows, and needs to determine when the current window ends and the next one begins. The aggregation_method is the logic that tells us when we have all the data for a given aggregation window. Once the window is closed, the data is aggregated into a single point and evaluated against the threshold. This field is optional. One of the following three values can be specified: EVENT_FLOW: (Default) Each aggregation window will wait until it starts to see timestamps arrive that are past its own delay setting. Once this occurs, the data is published. Relies on the timestamps of arriving data, so wall-clock time is no longer relevant. Works best for sources that come in frequently and with low event spread (high througput metrics) CADENCE: Classic New Relic logic where each evaluation window waits exactly as long as the aggregation_delay setting, using the wall-clock time as a timer. aggregation_delay is required when using this option. Data arriving too late will be dropped, which can cause false alerts. EVENT_TIMER: Each aggregation window has a timer on it, set to the aggregation_timer setting. The Timer starts running as soon as the first data point appears for that aggregation window (based on the data point’s timestamp). The aggregation_timer is reset for each new data point that arrives for that window. Once the aggregation_timer reaches 0, the aggregation window is published. Ideal for sparse and batched data, such as cloud integrations and infrequent error logs. aggregation timer The length of time in seconds to wait after each data point received, to ensure the entire batch is processed. Required when using EVENT_TIMER aggregation_method type. aggregation window Streaming alerts gathers data together into specific amounts of time. These windows of time are customizable. Data points are collected together based their timestamps and reported as a batch. The customizable aggregation window provides greater flexibility and fewer false violations when alerting on irregular or less frequent data points. alert An alert communicates an event or incident that designated personnel can track through Alerts. For an explanation of how basic alerts concepts are related, see Concepts and workflow. alert condition An alert condition (or condition), identified by its unique numeric condition_id, contains the criteria for creating a violation. The condition includes the threshold that is set for a metric timeslice or a custom metric over time on a chosen target. For an explanation of how a condition relates to other basic alerts concepts, see Concepts and workflow. alert evaluation Streaming data is assessed on a set of aggregation windows to determine if an alert condition is violating or recovering. The aggregation window time is how long we'll collect data before running the NRQL query condition. The offset evaluation time is how long you want us to wait for late data before assessing it. If a window doesn't have any data points, it's treated as a gap for loss of signal. alert policy A collection of one or more conditions, one or more notification channels, and an Incident preference setting. If a condition contained within the policy opens a violation, an incident may be opened depending on the Incident preference setting. Notifications will then be sent to all channels attached to the policy. For an explanation of how a policy relates to other basic alerts concepts, see Concepts and workflow. apdex Apdex is an industry-standard way to measure users' satisfaction with the response time of an application or service. New Relic rates each response as Satisfied, Tolerated, or Frustrated, and uses these ratings to calculate an overall user satisfaction score. For more information, see Apdex: Measure user satisfaction. apdex_f The response time above which a transaction are rated frustrating. Defaults to four times apdex_t. Requests that complete in less than apdex_t are rated satisfied. Requests that take longer than apdex_t, but less than four times apdex_t (apdex_f), are tolerated. Any requests that take longer than apdex_f are rated frustrating. For more information, see Apdex: Measure user satisfaction. apdex_t The response time above which a transaction is considered tolerable. The default value is 0.5 seconds, but you can change this in your Apdex settings. Requests that complete in less than apdex_t are rated satisfied. Requests that take more than apdex_t, but less than apdex_f, are tolerated. Any requests that take longer than apdex_f are rated frustrating. For more information, see Apdex: Measure user satisfaction. API (application programming interface) New Relic offers a variety of APIs and SDKs. For more information, see the introduction to New Relic's APIs. APM New Relic's APM (application performance monitoring) provides monitoring of your web or non-web application's performance. APM supports apps using several programming languages. application For New Relic purposes, any program instrumented by New Relic. application ID Some New Relic solutions assign a monitored application a unique application ID, often shortened to app ID. When present, this ID is available in the UI. It is also reported as an attribute and can be queried. For how to determine this, see Find app ID. application name The name that New Relic combines with your license key to uniquely identify a particular app. For more information, see Name your application. Applied Intelligence (AI) Applied Intelligence (AI) helps you find, troubleshoot, and resolve problems more quickly. Specifically, it’s a hybrid machine learning engine that reduces alert noise, correlates incidents, and automatically detects anomalies. Applied Intelligence includes Alerts, Incident Intelligence, and Proactive Detection. attribute Attributes are key-value pairs attached to data objects reported to New Relic. Attributes add detail, and they're similar to tags or labels in other SaaS software. You can explore this data by querying or searching via the UI or by using the data dictionary. Examples: APM reports a Transaction event. This includes timing data for the transaction in a duration attribute, which might have a value of .002. Our Infrastructure Monitoring reports a ProcessSample event. This includes a variety of CPU usage attributes, including a cpuSystemPercent attribute, which might have a value of .01. Our Telemetry SDK reports a Metric data type for storing metrics, with attached attributes like metricName and newrelic.source. Some New Relic tools allow you to report custom attributes to enhance your monitoring. For more information about attributes in APM, see Agent attributes. availability monitoring See Types of Synthetics monitors. browser The New Relic UI supports most browsers. For more information, see Supported browsers. For our end-user browser monitoring tool, see Browser Monitoring. Browser monitoring A Real User Monitoring (RUM) solution that measures the speed and performance of your end users as they navigate to your site from different web browsers, devices, operating systems, and networks. background external See web external. child account See parent account. cloud-based integration New Relic offers cloud-based integrations with providers such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform. collector The component that collects data from New Relic agents running on an app server, mobile device, or end-user browser. While the agent is installed on a user's app server, the collectors are centrally located in New Relic's data center. In order to contact the collector, the agent must be able to reach New Relic's domains and IP addresses. (The exact domain or IP depends on the New Relic monitoring tool.) The collector receives and interprets this data, and stores it in a database. The data is then retrieved and presented in the New Relic UI and by our various REST APIs. Command line interface (CLI) Our command line interface (CLI) is a tool you can use to build a New Relic application. This is the same tool our own engineers use. Go here for quick start instructions. Go to our Developer site for sample apps and guides. compute unit (CU) A unit of measurement that determines your pricing for some New Relic products governed by our original product-based pricing model. For more information, see Compute unit pricing. condition_id See alert condition. CPM (calls per minute) The number of calls your application receives each minute. This usually corresponds to the number of page views or external connections, and is usually the same as RPM (requests per minute). CPU burn The time consumed by code minus the wait time for a transaction. This is the time actually spent processing the transaction. It appears in the New Relic UI at the top of the transaction view for the agents that provide it (Ruby and PHP only). custom attribute A key-value pair added to a transaction or event in order to gain additional information about it. For more information, see custom attributes. custom dashboard A customizable dashboard with charts and tables that includes data from multiple New Relic data sources. For more information, see dashboards. custom event An event, in New Relic terms, is a data object with attached attributes. New Relic reports default event types, like Transaction and TransactionError. You can also create your own events. Events can be queried, and are used in some other features. You can generate custom events with APM agents, the browser monitoring agent, the mobile monitoring agents, and via the Event API. Alternatively, you can add custom attributes to some existing default New Relic events. custom instrumentation Custom instrumentation allows you to extend New Relic's monitoring to instrument code elements New Relic doesn't automatically instrument. Custom instrumentation is useful when your framework is not supported by New Relic, or when New Relic fails to pick up some element of your program. You can also use custom instrumentation to block a transaction from being reported entirely. For more information, see Custom instrumentation. custom metric Metric timeslice data that is manually recorded via an API call. Custom metrics allow you to record arbitrary metrics; for example, timing or computer resource data. All custom metric names must be prefixed with Custom/. For more information, see Custom metrics. Not to be confused with custom instrumentation data. data collector See collector. data explorer Use the data explorer to access, query and customize your data, create visualizations, and make connections between your services in a consistent and curated experience. For more on using the data explorer, see Introduction to the data explorer. degradation period When a data source enters a violating state, a degradation period of time begins. The degradation period is set in the condition's threshold. A violation will open if the source stays in a violating state for the entire degradation period. In addition: If the data source enters a non-violating state before the entire time has elapsed, the degradation period countdown is reset, and a violation does not open. If your alert condition threshold is configured as at least once in, the degradation period always lasts a single minute. dimensional metric A dimensional metric is a metric that has multiple attributes, also known as dimensions. At New Relic, we report dimensional metrics using the Metric data type. For more on other metric data types, see Metric data. Docker An open platform for distributed applications, which allows you to assemble multi-container portable apps. Infrastructure Monitoring includes integrated Docker monitoring. For more information about Docker, see the Docker website. downtime The period of time when customers cannot access your site and your app is not reporting to New Relic. For more information, see Synthetic Monitoring and Types of synthetic monitors. entity In New Relic, an entity is anything we can identify that has data you can monitor. An entity can be something you monitor directly, like applications and microservices, or indirectly, like data centers. You can identify one or more entities to be targets for alert conditions. In the Alerts API, the entity being monitored is identified with an entity_id. For more on this, see What are entities? event The word event is a general term that can have many meanings. At New Relic, event can have several meanings: At New Relic, event data is one of our core data types. Event data represents a record of a single event at a particular moment in time. Events can vary by type (for example, Transaction or Mobile, and will have associated attributes (for example, timestamp or transactionName). For more details, see Event data. For our infrastructure monitoring, the word event can be used to refer to important system and host activity. For example, a configuration change for a monitored host would be registered on Infrastructure's Events UI page. For alerts, the Events UI page displays a list of alerts-related incidents for your monitored entities. Events are reported for a violation opening and for closing. In some contexts, event can refer to any NRQL-queryable data type. For example, when you run a NRQL query, you will see a count of inspected events: this refers to a count of all data types queried. expected error An expected error is a common error that you don't want to affect your Apdex score or error rate. For more information, see Manage errors in APM. exporter At New Relic, an exporter is a type of integration that reports telemetry data to New Relic from a third-party (non-New Relic) telemetry tool. For examples, see Exporters, or search our integration quickstarts in New Relic I/O. Flex New Relic Flex is an application-agnostic, all-in-one infrastructure integration. With it, you can build your own integration that collects metric data from a wide variety of services, and that can instrument any app that exposes metrics over a standard protocol (HTTP, file, shell) in a standard format (for example, JSON or plain text) to the terminal. It's a recommended way to create a custom integration, because it doesn't require coding skills. framework A framework is a structured collection of pre-defined functions, into which an application builder inserts their own code to build their application. A framework is not the same as a library. While a library is a collection of functions you can call as needed, a framework is a skeleton for your application. The functions in that framework then call your functions. For more about the distinction between a framework and a library, see What is the difference between a framework and a library?. New Relic automatically instruments many common frameworks. For more about the frameworks New Relic supports, see the agent-specific documentation: C SDK supported frameworks Go supported frameworks Java supported frameworks .NET supported frameworks Node.js supported frameworks PHP supported frameworks Python supported frameworks Ruby supported frameworks harvest cycle The period of time between each connection from a New Relic agent to the collector. Between harvest cycles, an agent collects and caches data. At the end of the cycle an agent reports those data to the collector, then begins a new harvest cycle. health status indicator Some New Relic UI pages have a health status indicator appearing next to an index of monitored entities. This is a colored bar (generally green, yellow, red, or gray) indicating the status of your app or other entity monitored by New Relic. It also indicates whether the entity has any alert policies assigned to it and whether there are any policy violations. In general, the colored bar will be green, yellow, red, or gray to indicate the health status. Exceptions: Our REST API (v2) uses orange instead of yellow for the application's health and reporting status. Service maps use different criteria for reporting the health of a connection between an app and an external service not monitored by New Relic (for example, a third party API). host At New Relic, a host means one of the following: A physical machine is a hardware-based device with dedicated physical resources, including memory, processing, and storage. Each machine has its own OS which applications run on. A virtual machine (VM) is the software implementation of a physical machine that executes programs like a physical machine. One or more virtual machines can run on a physical machine. Each virtual machine has its own OS and allocated virtual machine resources such as RAM and CPU. A cloud instance is a type of virtual machine that is run in the public cloud. In this context, virtual machines and cloud instances are different from Java Virtual Machines (JVMs) and containers. host ID Each host identified by APM is assigned a host ID. This ID is used to uniquely identify it, and to retrieve data about that host via the REST API. For more information, see List host ID. ignored error An error that you have told the APM agent not to report to the collector. For more information, see Manage errors in APM. incident An incident is a collection of one or more violations of the conditions defined in an alert policy. An incident record includes all of the open and close time stamps for each violation, as well as chart snapshots of the data being evaluated around the time of each violation. You can view detailed information from the Incidents pages in the user interface. You can also select your preference for how we roll up violations into the incident. For an explanation of how an incident relates to other basic alerts concepts, see Concepts and workflow. Infrastructure monitoring By connecting changes in host performance to changes in your configuration, infrastructure monitoring provides real-time metrics and powerful analytics that reduce your mean-time-to-resolution (MTTR). Infrastructure is specifically designed for complex environments that need flexible, dynamic server monitoring, from a physical datacenter to thousands of Amazon Elastic Compute Cloud (Amazon EC2) instances and other types of integrations. Insights Insights was the name for the New Relic product that previously governed the reporting of custom events, and the ability to query and chart your New Relic data. These features are now a fundamental part of the New Relic One platform and are no longer governed by the Insights product or name. To learn more about these features: Event API for reporting custom events Query and chart data For historical reasons, the word \"Insights\" is still used in some places. For example: For New Relic organizations on our original pricing model, Insights Pro is still the product name governing custom event data ingest and retention. Some APM agents still have Insights language in their codebase. For example, the Java agent custom_insights_events configuration. There is an API key called the Insights insert key. instance ID Each instance identified by New Relic is assigned a unique instance ID. Instance IDs are most commonly found for JVMs (Java Virtual Machines), but can exist for each agent. This ID is used to uniquely identify it, and to retrieve data about that instance via the REST API. For more information, see List instance IDs. instrumentation The collection of data from an application or host. When New Relic instruments a framework, it detects the methods and calls used by that framework, and intelligently groups them together. integration At New Relic, an integration refers to a solution that integrates with a specific technology (like a web framework or a type of database). All our integrations can be found as quickstarts in New Relic Instant Observability. interaction In our mobile monitoring, an interaction is a specific code path initiated by a user interaction (usually a button press). An interaction is the mobile equivalent of a transaction, and like a transaction an interaction can be traced and monitored. You can see much of the data included in an interaction in the BrowserInteraction event. interaction trace An interaction trace is a complete picture of a single interaction. With interaction traces, New Relic gives you much deeper visibility into a single slow interaction, which can help you understand a broader problem. Interaction traces are the mobile equivalent of a transaction trace. For more information, see Creating interactions (iOS) and Creating interactions (Android). inventory data Inventory data is information about the status or configuration of a service or host. Examples of inventory data include: Configuration settings Name of the host the service is on Amazon AWS region Port being used For more information, see Understand and use data. key transaction A web transaction that the user has marked as particularly important; for example, key business events (such as signups or purchase confirmations), or transactions with a high performance impact (such as searches). Key transactions have their own pages in the UI and other customized values. For more information, see Key transactions. launcher A launcher is a specific piece of code you can include when you create a New Relic One app. It creates the tile on the homepage that you click to launch the app. For more information, see the documentation about core UI components. log A log is a message about a system used to understand the activity of the system and to diagnose problems. For more information on how we use log data, see Log management. Log monitoring Our log management and monitoring features give you the tools to collect, process, explore, visualize, and alert on your log data using your existing log forwarder. With all of your log data in one place, you'll be able to make better decisions, detect and resolve problems more quickly, and see your logs in context to troubleshoot faster. Logs Our Logs feature is a scalable log management platform that allows you to connect your log data with the rest of your telemetry data. Pre-built plugins with some of the most common open-source logging tools make it simple to send your data from anywhere to New Relic. Logs in context Logs in context makes it easy to link to your log data with related data across the rest of our platform. Bringing all of this data together in a single tool allows you to quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. master account See parent account. metric A metric is a numeric measurement. Metric data is a broad category because there are several ways to make and report measurements. For more about how metrics are reported at New Relic, see New Relic data types. metric timeslice New Relic reports metrics in several ways. One variety of metric data is called metric timeslice data; this is the type of data used to generate many of the charts in APM, mobile monitoring, and browser monitoring (for more details, see metric timeslice data). Over time, metric timeslice data is aggregated into longer timeslice data records for more efficient storage. For more about how we aggregate this type of data, see Data aggregation. For how to query this type of data, see Query metric timeslice data. metric grouping issue A metric grouping issue occurs when an account sends too many differently named metric timeslice data points to New Relic, and those individual web transactions are not properly aggregated. For example, rather than a single /user/controlpanel/ metric name, you might see /user/controlpanel/alice, /user/controlpanel/bob, and /user/controlpanel/carol. For more information, see Metric grouping issues. minion The software that accepts monitor jobs from a private location. A minion is a packaged virtual appliance that runs in your hypervisor. For more information, see Private locations overview and install and configure private minions. Mobile monitoring Mobile monitoring allows you to monitor and manage the performance of your mobile apps on Android, iOS, tvOS, and other systems. Mobile monitoring provides end-to-end details, including crashes, throughput, HTTP requests, error traces, and more. Not to be confused with New Relic's own mobile apps for Android, iPhone, and iPad. monitor For our Synthetic Monitoring, a monitor ensures your website or API endpoint is available. For more information, see Adding and editing monitors. NerdGraph NerdGraph is our GraphQL API, an efficient and flexible query language that lets you request exactly the data you need, without over-fetching or under-fetching. NerdGraph calls get all the data you need in a single request. NerdGraph also makes it easier to evolve APIs over time and enables powerful developer tools. You can use our NerdGraph GraphiQL explorer to explore the schema and find definitions. With valid New Relic API key, you can try it out yourself at api.newrelic.com/graphiql. Nerdlet A Nerdlet is a component of a New Relic One application. It's a specific UI view, represented by a React JavaScript package. For more information, see Nerdpack file structure. Nerdpack A Nerdpack is a component of a New Relic One application. It's the package containing all the files needed by that application. For more information, see Nerdpack file structure. New Relic Edge with Infinite Tracing New Relic Edge with Infinite Tracing is a fully managed, distributed tracing service that observes 100% of your application traces, then provides actionable data so you can solve issues faster. For more information, see /docs/understand-dependencies/distributed-tracing/get-started/how-new-relic-distributed-tracing-works. New Relic One For more information, see Introduction to New Relic One. New Relic One catalog Our catalog is a collection of applications built on the New Relic One platform. The catalog includes custom apps we've built, public open source apps, and any apps that you buid. You can browse the catalog on New Relic One. NRQL (New Relic query language) NRQL is a query language, similar in form to SQL, that allows you to query the data stored in your New Relic account. non-web transaction APM identifies transactions as either web or non-web. When New Relic does not detect a transaction was initiated by a web request, this is called a non-web transaction. For more information, see Background processes and other non-web transactions. notification The message sent when an incident opens, is acknowledged, or closes. The type of notification is defined by the alert policy's notification channel. For an explanation of how notifications relate to other basic alerts concepts, see Concepts and workflow. notification channel Where we send a notification when an incident opens, is acknowledged, or closes. Available channels include email, mobile push notifications, webhooks, and more. on-host integration On-host integrations refer to integrations that reside on your own servers or hosts and that communicate with our infrastructure agent. For more information, see Introduction to on-host integrations. owner For accounts on our original pricing model, this is a type of user role: the user who initially created the account. For more information, see Users. page load timing With page load timing, New Relic monitors the full load time for end-user browsers. New Relic's application agents dynamically inject JavaScript into the page, then capture the following key load points: Navigation start: The user initiates the transaction. First byte: The browser receives the requested page. DOM ready: The browser has finished parsing DOM. Page ready: Page loading is complete. Page load timing is sometimes referred to as RUM, or real user monitoring. Unlike standard RUM, page load timing also captures JavaScript errors and AJAX requests. For more information, see Page load timing process. parameter Deprecated term; see attribute. parent account New Relic organizations can have a parent/child account structure. This structure was much more important for organizations on our original user model, but is still used for some features for organizations on the New Relic One user model. Learn more about account structure. Parent accounts were previously referred to as \"master accounts\", and child accounts were previously referred to as \"sub-accounts\". permalink A unique URL that links to a view of your application at a specific point in time. Permalinks are useful for troubleshooting and for sharing interesting time windows with colleagues. pinger The component of New Relic that connects to your website to verify your website is accessible. New Relic has pingers in Europe, Asia, and the United States. Each pinger attempts to contact your website at least once every two minutes. If enough pingers are unable to reach your website, your application will be considered down. For in-depth scriptable testing, including real browser tests and tests of API endpoints, see Synthetic Monitoring. Synthetic Monitoring includes free ping monitoring, which allows you to monitor your website from locations around the world. For more information, see Types of Synthetic monitors. polling interval (AWS) Our Amazon integrations query your AWS services according to a polling interval, which varies depending on the integration. Each polling interval occurs for every AWS entity. For example, if you have thirteen Elastic Load Balancers (ELB), each one will be polled every five minutes. Depending on the AWS integration, there may be delays in the timing between the API request and the metric data returned. If you notice unusual delays, follow the integration troubleshooting procedures. PPM (pages per minute) The number of pages per minute your application serves. private location A Synthetic monitor feature that allows you to run Synthetic monitors from within your own systems by creating private minions. Private locations allow you to extend your Synthetic coverage to new geographical locations, and to monitor websites behind your firewall such as an intranet site. For more information, see Private locations overview. recovery period A recovery period of time begins when a data source enters a non-violating state after being in a violating state. The recovery period is set in the condition's threshold. A violation will close when a source remains in a non-violating state and the recovery period time has elapsed. If the data source enters a violating state before the time has elapsed, the recovery period clock will reset and the violation won't close. response time The duration of time between a request for service and a response. For more information, see Response time. restricted user A type of user role on a New Relic account. For more information, see Users. rollup Using the same application name for multiple applications. This allows you to combine data in APM, either from multiple applications, or from multiple instances of an application. For more information, see Rolling up app data. root span For distributed tracing, the root span is the first span in a trace. In many cases, the root span duration will represent the duration of the entire trace, or be very close to it. However, for more complex, modern systems that use a lot of asynchronous, non-blocking processes, this will not be true. For those systems, the root span’s duration may be significantly less than the duration of the trace. RPM The term RPM usually refers to the number of requests per minute your application receives from users. This is usually the same as CPM (calls per minute). Historically, some New Relic monitoring solutions, like APM and Browser Monitoring, used to contain RPM in the URL; for example, https://rpm.newrelic.com. This language use originally referred to Rails performance management because the first iteration of our product monitored Ruby on Rails applications. We monitor many more languages and systems than Ruby now. RUM (real user monitoring) See page load timing. runbook A runbook contains standard procedures and operations typically used by system administrators, network operations staff, and other personnel to handle outages, alert incidents, and other situations. If your organization stores runbook instructions as URLs, you can link this information to an alerts policy so your personnel has easy access to this information when an incident violates the defined policy thresholds. SAML (Security Assertion Markup Language) SAML is an XML-based data format for sharing authentication data between two parties. New Relic accounts must obtain a SAML certificate in order to enable Single Sign On for their users. For more information, see SAML service providers. Selenium Selenium is an open-source browser testing suite. Synthetics uses Selenium to test monitored websites with real browsers. For more information, see monitor types. service A service is a cluster of runtime server processes that accomplish a particular task, usually service requests. Unlike an application, a service is not usually invoked by a human. New Relic offers a variety of integrations that allow you to report data from your services. signal The stream of telemetry data that's watched and alerted on. You use NRQL queries to define a signal. signal filter When we receive data and it's routed to the streaming alerts platform, your NRQL WHERE clause will filter the data coming in. The filtered streaming data is what's evaluated for loss of signal violations, for example. span In a distributed trace, a span is a \"named, timed operation representing a contiguous segment of work in that trace\" (from OpenTracing.io definition). For distributed tracing, spans are displayed in the distributed tracing UI, and the data type Span is available to be queried. See also root span. SSL certificate SSL certificates encrypt data that is being transmitted. While New Relic refers to security certificates as SSL because it is a more commonly used term, all certificates adhere to industry standards for secure encryption in transit. SSO (single sign on) SSO (single sign on) allows you to manage user authentication in New Relic using an external SSO provider. For more information, see Setting up SSO. streaming algorithm This is what determines when the data in an aggregation window is processed. The streaming algorithm uses your server's clock time and the aggregation window size to trigger the alert evaluation process. sub-accounts See master account. Synthetic monitoring Synthetic monitoring allows you to monitor your website or API endpoint via automated, scriptable tools. Use free ping monitor to ensure your website is accessible, or expand your monitoring with browser monitors, which test your website with real browsers. Go further with scripting, to script browsers or API monitors for sophisticated testing. target A target is a resource or component monitored by a New Relic monitoring tool that has been identified in an alert condition. When the data source for that target crosses the defined critical threshold, we will open a violation. Depending on your policy's Incident preference setting, Alerts may create an incident record and send notifications through the defined channels. See also entity. tag Tags are key:value metadata added to monitored apps, hosts, dashboards, and other entities to help you organize your data at a high level. For details, see Tags. thresholds Thresholds are alert condition settings that define a violation. Threshold values include the value a data source must pass to trigger a violation and the time-related settings that define a violation; for example: Passing a certain value for at least x minutes Passing a certain value only once in x minutes While the data source passes a certain value, a degradation period starts. Likewise, when that data source stops passing a certain value, a recovery period starts. The durations of these two time periods are defined in the alert condition threshold settings. Thresholds have a required critical (red) threshold and an optional warning (yellow) threshold. In the UI, the entity's health status indicator will change to yellow or red when a threshold has been crossed and a violation will open. For more information, see Define thresholds. For an explanation of how thresholds relate to other basic Alerts concepts, see Concepts and workflow. throughput Throughput is a measurement of user activity for a monitored application. APM throughput and Browser Monitoring throughput are measured in different ways: APM: requests per minute (RPM) Browser: page views per minute (PPM) tier A tier can refer to how New Relic categorizes or visualizes the various agent language ecosystems that we support. For example: In APM, the color-coded categories that appear on your app's main Overview chart show response time spent in various functions, processes, or agents as tiers; for example, request queuing, garbage collection, Middleware, JVMs, etc. In New Relic labels, TIER can be used to define or classify the client-server architecture; for example, front-end and back-end tiers. \"Tier\" may sometimes be used to refer to our pricing editions. time picker By default the New Relic UI shows data for the past 30 minutes, ending now. To change the time window, use the time picker. time range A time range can refer to a length of time selected in the New Relic UI. New Relic displays a time range depending on the range you select using the time picker. timeslice data See metric timeslice data. trace A trace is a description of how a request travels through a system. Trace data helps you understand the performance of your system and diagnose problems. For more information on how we use trace data, see New Relic data types. traffic light See health status. transaction A transaction is defined as one logical unit of work in an application. This term primarily refers to server-side transactions monitored by APM. For more information, see documentation about web transactions and non-web transactions. The term transaction is also sometimes used in Browser Monitoring. In that case, it primarily refers to activity beginning with a browser-side web request and ending with a complete page load. transaction trace A transaction trace is a complete picture of a single transaction, down to the database queries and exact invocation patterns. With transaction traces, New Relic gives you much deeper visibility into a single slow transaction, which can help you understand a broader problem. For more information, see Transaction traces. UI The New Relic user interface. For more information, see Standard page functions. user A user can refer to a specific user role in a New Relic account. For more information, see Users. UTC Universal Time Coordinated (UTC), or Coordinated Universal Time, is a standard timestamp for synchronizing time around the world. value function (metrics) The numeric value obtained from metric timeslice data; for example, an average, minimum, maximum, total, sample size, etc. violation A violation occurs when the entity monitored by an alert condition reports a value that crosses the thresholds defined in that condition. For an explanation of how violations relate to other basic alerts concepts, see Concepts and workflow. You can view a summary of the violations for a selected incident's page. You can also view the violations for a specific entity from the product's UI. web external Web external is the term applied to the portion of time spent in transactions to external applications from within the code of the application you are monitoring. That time can be a call to a third party company (a payment provider, for example) or it could be a call to another microservice within your own company. Web external demonstrates how performance is impacted by your code executing outside the application you are measuring. web transaction A transaction is defined as one logical unit of work in an application. This term primarily refers to server-side transactions monitored by APM. Web transactions are initiated with an HTTP request. For most organizations, these represent customer-centric interactions and thus are the most important transactions to monitor. For more information, see Web transactions and Non-web transactions. WebDriverJS WebDriver is a Selenium component, used to control Synthetics scripted browsers. Specifically, Synthetics uses WebDriverJS, a Node.js-based flavor of Selenium. For more information, see Writing scripted browsers and Scripted browser examples. workload A workload represents a group of entities that work together to provide a digital service. For more information, see Workloads.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.41348,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "availability <em>monitoring</em>",
        "body": ", devices, operating systems, and networks. background external See web external. child account See parent account. cloud-based <em>integration</em> New Relic offers cloud-based integrations with providers such as Amazon Web Services (AWS), Microsoft <em>Azure</em>, and Google Cloud Platform. collector The component"
      },
      "id": "61b40189196a672dd0a5aa8c"
    },
    {
      "sections": [
        "VMware vSphere monitoring integration",
        "Why it matters",
        "Compatibility and requirements",
        "Important",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "Enable and configure performance metrics (Beta)",
        "Caution",
        "Tip",
        "Collect vSphere events (Beta)",
        "Collect snapshots data (Beta)",
        "Collect vSphere tags (Beta)",
        "Filter resources by tags (Beta)",
        "Example configuration",
        "Update your integration",
        "View and use data",
        "Metric data",
        "VSphereHostSample",
        "VSphereVmSample",
        "VSphereDatastoreSample",
        "VSphereDatacenterSample",
        "VSphereResourcePoolSample",
        "VSphereClusterSample",
        "VSphereSnapshotVmSample"
      ],
      "title": "VMware vSphere monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "ab80d0e38fcc2851aa2c18034e0d191c6d77910e",
      "image": "https://docs.newrelic.com/static/a5b511bbd58392771e7ee8472579bc01/8c557/infrastructure-ohi-vmware-vsphere_0.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/vmware-vsphere-monitoring-integration/",
      "published_at": "2022-01-08T12:28:56Z",
      "updated_at": "2021-12-20T10:25:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's VMware vSphere integration helps you understand the health and performance of your vSphere environment. You can: Query data to get insights on the performance on your hypervisors, virtual machines, and more. Go from high level views down to the most granular data. vSphere data visualized in a New Relic dashboard includes operating systems, status, average CPU and memory consumption, and more. Our integration uses the vSphere API to collect metrics and events generated by all vSphere's components, and forwards the data to our platform via the infrastructure agent. Why it matters With our vSphere integration you can: Instrument and monitor multiple vSphere instances using the same account. Collect data on snapshots, VMs, hosts, resource pools, clusters, and datastores, including tags. Monitor the health of your hypervisors and VMs using our charts and dashboards. Use the data retrieved to monitor key performance and key capacity scaling indicators. Set alerts based on any metrics collected from vCenter. Create workloads to group resources and focus on key data. You can create workloads using data collected via the vSphere integration. Compatibility and requirements Our integration is compatible with VMware vSphere 6.5 or higher. Before installing the integration, make sure that you meet the following requirements: Infrastructure agent installed on a host vCenter service account having at least read-only global permissions with the propagate to children option checked Important Large environments: In environments with more than 800 virtual machines, the integration cannot report all data and may fail. We offer a workaround that will preserve all metrics and events, but it will disable entity registration. To apply the workaround, add the following environment variable to the configuration file: EVENTS: true METRICS: true Copy Install and activate To install the vSphere integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-vsphere. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp vsphere-config.yml.sample vsphere-config.yml Copy Edit the vsphere-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-vsphere MSI installer image from: download.newrelic.com/infrastructure_agent/windows/integrations/nri-vsphere/nri-vsphere-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-vsphere-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp vsphere-config.yml.sample vsphere-config.yml Copy Edit the vsphere-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. To configure the vSphere integration, you must define the URL of the vSphere API endpoint, and your vSphere username and password. For configuration examples, see the sample configuration files. Some vSphere integration features are optional and can be enabled via configuration settings. In addition, with secrets management, you can configure on-host integrations with New Relic's infrastructure monitoring agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. Enable and configure performance metrics (Beta) Performance metrics provide a better understanding of the current status of VMware resources and can be collected in addition to the metrics collected by default;and included in the samples;described at the bottom of the page. All metrics collected are included in the corresponding sample with the perf. prefix attached to the name. For example, net.packetsRx.summation is collected and sent as perf.net.packetsRx.summation. To collect vSphere performance metrics, use the ENABLE_VSPHERE_PERF_METRICS environment variable. Data is collected according to the settings in the vsphere-performance.metrics configuration file. You can override the location of the performance metrics config file using PERF_METRIC_FILE environment variable. Notice that the integration follows VMware's data collection levels (1 to 4). When ENABLE_VSPHERE_PERF_METRICS is set, all level 1 metrics are collected. The data collection level of the performance metrics collected can be modified using PERF_LEVEL. Each metric in the config file can be commented out and new ones can be added if needed. Caution Collection of performance data can increase the load in vCenter and the time needed by to collect data. We recommended to only include the metrics you need in the configuration file. To fine-tune data collection, the number of entities and metrics retrieved per request can be modified using BATCH_SIZE_PERF_ENTITIES and BATCH_SIZE_PERF_METRICS. Tip For more information on vSphere performance metrics, see the VMware documentation. Collect vSphere events (Beta) To collect vSphere events, use the ENABLE_VSPHERE_EVENTS environment variable. The integration collects events between the current time and the last fetched event for each datacenter. It stores the information regarding the last fetched event in a cache that is updated after each execution. Events are only available if the integration is connected to a vCenter and not directly to an ESXi host. The number of events collected per request can be tuned by modifying EVENTS_PAGE_SIZE, which is set to 100 by default. Events are available in the Events page and can be queried via NRQL as InfrastructureEvent under vSphereEvent. Here is an example of vSphere events data: \"summary\": \"User dcui@127.0.0.1 logged out (login time: Tuesday, 14 July, 2020 08:32:09 AM, number of API invocations: 0, user agent: VMware-client/6.5.0)\", \"vSphereEvent.computeResource\": \"cluster1\", \"vSphereEvent.datacenter\": \"Prod Datacenter\", \"vSphereEvent.date\": \"Tue, 14 Jul 2020 09:03:51 UTC\", \"vSphereEvent.host\": \"192.168.0.230\", \"vSphereEvent.userName\": \"dcui\" Copy Collect snapshots data (Beta) To collect snapshot data, use the ENABLE_VSPHERE_SNAPSHOTS environment variable. Snapshot data can be found in VSphereSnapshotVmSample. Collected data covers total and unique space occupied by disk and memory files, snapshot tree, and creation time. You can use this information to create NRQL queries, dashboards, and alerts, since it's linked to the corresponding virtual machine entity. Collect vSphere tags (Beta) To collect vSphere tags, use the ENABLE_VSPHERE_TAGS environment variable. Tags are available as attributes in the corresponding entity sample as label.tagCategory:tagName. If two tags of the same category are assigned to a resource, they are added to a unique attribute separated by a pipe character. For example: label.tagCategory:tagName|tagName2. Tags can be used to run NRQL queries, filter entities in the New Relic Explorer, and to create dashboards and alerts. Filter resources by tags (Beta) Resource filtering allows you to specify which resources you want to monitor by declaring a set of tags that resources must have in order to be monitored. Resources require a match on any (one or more) of the filter tags in order to be included. If none of the resource tags match any of the filter tags, no information about that resource is sent to New Relic. To use filtering resources by tag you need to have the ENABLE_VSPHERE_TAGS environment variable enabled. A tag filter expression is a space-separated list of pairs of strings with the format category=name. For example, to only retrieve resources with a tag category region and include regions us and eu use a filter expression like: region=us region=eu INCLUDE_TAGS: > region=us region=eu Copy To enable resource filtering by tag, edit your integration configuration file and add the option INCLUDE_TAGS with the filter expression you want. Caution Note that datacenter resources acting as the root of the resource tree MUST have tags attached AND match the filter expression in order for other child resources to be fetched. Important If you connect the integration directly to the ESXi host, vCenter data is not available (for example, events, tags, or datacenter metadata). Example configuration Here are examples of the vSphere integration configuration, including performance metrics: vsphere-config.yml.sample (Linux) vsphere-win-config.yml.sample (Windows) vsphere-performance.metrics (Performance metrics) For more information, see our documentation about the general structure of on-host integration configurations. Important The configuration option inventory_source is not compatible with this integration. Update your integration On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. View and use data Data from this service is reported to an integration dashboard. You can query this data for troubleshooting purposes or to create charts and dashboards. vSphere data is attached to these event types: VSphereHostSample VSphereClusterSample VSphereVmSample VSphereDatastoreSample VSphereDatacenterSample VSphereResourcePoolSample VSphereSnapshotVmSample Performance data is enabled and configured separately (see Enable and configure performance metrics). For more on how to view and use your data, see Understand integration data. Metric data The vSphere integration provides metric data attached to the following New Relic events: VSphereHostSample VSphereVmSample VSphereDatastoreSample VSphereDatacenterSample VSphereResourcePoolSample VSphereClusterSample VSphereSnapshotVmSample VSphereHostSample Name Description cpu.totalMHz Sum of the MHz for all the individual cores on the host cpu.coreMHz Speed of the CPU cores cpu.available Amount of free CPU MHz in the host cpu.overallUsage CPU usage across all cores on the host in MHz cpu.percent Percentage of CPU utilization in the host cpu.cores Number of physical CPU cores on the host. Physical CPU cores are the processors contained by a CPU package cpu.threads Number of physical CPU threads on the host disk.totalMiB Total capacity of disks mounted in host, in MiB mem.free Amount of available memory in the host, in MiB mem.usage Amount of used memory in the host, in MiB mem.size Total memory capacity of the host, in MiB vmCount Number of virtual machines in the host hypervisorHostname Name of the host uuid The hardware BIOS identification datacenterName Name of the datacenter related to the host clusterName Name of the cluster related to the host resourcePoolNameList List of names of the resource pools related to the host datastoreNameList List of names of datastores related to the host datacenterLocation Datacenter location networkNameList List of names of networks related to the host overallStatus gray: Status is unknown green: Entity is OK yellow: Entity might have a problem red: Entity definitely has a problem connectionState The host connection state: connected: Connected to the server. For ESX Server, this is the default setting. disconnected: The user has explicitly taken the host down. VirtualCenter does not expect to receive heartbeats from the host. The next time a heartbeat is received, the host is moved to the connected state again and an event is logged. notResponding: VirtualCenter is not receiving heartbeats from the server. The state automatically changes to connected once heartbeats are received again. This state is typically used to trigger an alarm on the host. inMaintenanceMode The flag to indicate whether or not the host is in maintenance mode. This flag is set when the host has entered the maintenance mode. It is not set during the entering phase of maintenance mode. inQuarantineMode The flag to indicate whether or not the host is in quarantine mode. InfraUpdateHa will recommend to set this flag based on the HealthUpdates received by the HealthUpdateProviders configured for the cluster. A host that is reported as degraded will be recommended to enter quarantine mode, while a host that is reported as healthy will be recommended to exit quarantine mode. Execution of these recommended actions will set this flag. Hosts in quarantine mode will be avoided by vSphere DRS as long as the increased consolidation in the cluster does not negatively affect VM performance. powerState The host power state: poweredOff: The host was specifically powered off by the user through VirtualCenter. This state is not a cetain state, because after VirtualCenter issues the command to power off the host, the host might crash, or kill all the processes but fail to power off. poweredOn: The host is powered on. A host that is entering standby mode entering is also in this state. standBy: The host was specifically put in standby mode, either explicitly by the user or automatically by DPM. This state is not a certain state, because after VirtualCenter issues the command to put the host in standby state, the host might crash, or kill all the processes but fail to power off. A host that is exiting standby mode s also in this state. unknown: If the host is disconnected or notResponding, we know its power state, so the host is marked as unknown. standbyMode The host’s standby mode. The property is only populated by vCenter server. If queried directly from the ESX host, the property is unset. entering: The host is entering standby mode. exiting: The host is exiting standby mode. in: The host is in standby mode. none: The host is not in standby mode, and it is not in the process of entering or exiting standby mode. cryptoState Encryption state of the host. Valid values are enumerated by the CryptoState type: incapable: The host is not safe for receiving sensitive material. prepared: The host is prepared for receiving sensitive material but does not have a host key set yet. safe: The host is crypto safe and has a host key set. bootTime The time when the host was booted. VSphereVmSample Name Description mem.size Memory size of the virtual machine, in MiB mem.usage Guest memory utilization statistics, in MiB. This is also known as active guest memory. The value can range between 0 and the configured memory size of the virtual machine. Valid while the virtual machine is running. mem.free Guest memory available, in MiB. The value can range between 0 and the configured memory size of the virtual machine. Valid while the virtual machine is running. mem.ballooned The size of the balloon driver in the virtual machine, in MiB. The host will inflate the balloon driver to reclaim physical memory from the virtual machine. This is a sign that there is memory pressure on the host. mem.swapped The portion of memory, in MiB, that is granted to this virtual machine from the host's swap space. This is a sign that there is memory pressure on the host. mem.swappedSsd The amount of memory swapped to fast disk device such as SSD, in MiB cpu.allocationLimit Resource limits for CPU, in MHz. If set to -1, there is no fixed allocation limit. cpu.overallUsage Basic CPU performance statistics, in MHz. Valid while the virtual machine is running. cpu.hostUsagePercent Percent of the host CPU used by the virtual machine. In case a limit is configured, the percentage is calculated by taking the limit as the total. cpu.cores Number of processors in the virtual machine disk.totalMiB Total storage space, committed to this virtual machine across all datastores, in MiB ipAddress Primary guest IP address, if available ipAddresses List of IPs associated with the VM (except ipAddress). A pipe or vertical bar character (|) is used as a separator. connectionState Indicates whether or not the virtual machine is available for management: connected: Server has access to the virtual machine. disconnected: Server is currently disconnected from the virtual machine, since its host is disconnected. inaccessible: One or more of the virtual machine configuration files are inaccessible. invalid: The virtual machine configuration format is invalid. orphaned: The virtual machine is no longer registered on its associated host. powerState The current power state of the virtual machine: poweredOff, poweredOn, or suspended. guestHeartbeatStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. operatingSystem Operating system of the virtual machine guestFullName Guest operating system full name, if available from guest tools hypervisorHostname Name of the host where the virtual machine is running instanceUuid Unique identification of the virtual machine datacenterName Name of the datacenter clusterName Name of the cluster resourcePoolNameList List of names of the resource pools datastoreNameList List of names of datastores networkNameList List of names of networks datacenterLocation Datacenter location overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. disk.suspendMemory Size of the snapshot file (bytes). disk.suspendMemoryUnique Size of the snapshot file, unique blocks (bytes). disk.totalUncommittedMiB Additional storage space potentially used by this virtual machine on all datastores. Essentially an aggregate of the property uncommitted across all datastores that this virtual machine is located on (Mebibytes). disk.totalUnsharedMiB Total storage space occupied by the virtual machine across all datastores, that is not shared with any other virtual machine (Mebibytes). mem.hostUsage Host memory usage (Mebibytes). resourcePoolName Resource Pool Name. vmConfigName Vm Config Name. vmHostname Vm Hostname. VSphereDatastoreSample Name Description capacity Maximum capacity of this datastore, in GiB, if accessible is true freeSpace Available space of this datastore, in GiB, if accessible is true uncommitted Total additional storage space, potentially used by all virtual machines on this datastore, in GiB, if accessible is true vmCount Number of virtual machines attached to the datastore datacenterLocation Datacenter location datacenterName Datacenter name hostCount Number of hosts attached to the datastore overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. accessible Connectivity status of the datastore. If this is set to false, the datastore is not accessible. url Unique locator for the datastore, if accessible is true fileSystemType Type of file system volume, such as VMFS or NFS name Name of the datastore nas.remoteHost Host that runs the NFS/CIFS server nas.remotePath Remote path of NFS/CIFS mount point VSphereDatacenterSample Name Description datastore.totalUsedGiB Total used space in the datastores, in GiB datastore.totalFreeGiB Total free space in the datastores, in GiB datastore.totalGiB Total size of the datastores, in GiB cpu.cores Total CPU count per datacenter cpu.overallUsagePercentage Total CPU usage, in percentage cpu.overallUsage Total CPU usage, in MHz cpu.totalMHz Total CPU capacity, in MHz mem.usage Total memory usage, in MiB mem.size Total memory, in MiB mem.usagePercentage Total memory usage as percentage clusters Total cluster count per datacenter resourcePools Total resource pools per datacenter datastores Total datastores per datacenter networks Total network adapter count per datacenter overallStatus gray: Status is unknown green: Entity is OK yellow: Entity might have a problem red: Entity definitely has a problem hostCount Total host system count per datacenter vmCount Total virtual machines count per datacenter VSphereResourcePoolSample Name Description cpu.TotalMHz Resource pool CPU total capacity, in MHz cpu.overallUsage Resource pool CPU usage, in MHz mem.size Resource pool total memory reserved, in MiB mem.usage Resource pool memory usage, in MiB mem.free Resource pool memory available, in MiB mem.ballooned Size of the balloon driver in the resource pool, in MiB mem.swapped Portion of memory, in MiB, that is granted to this resource pool from the host's swap space vmCount Number of virtual machines in the resource pool overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. resourcePoolName Name of the resource pool datacenterLocation Datacenter location datacenterName Name of the datacenter clusterName Name of the cluster VSphereClusterSample Name Description cpu.totalEffectiveMHz Effective CPU resources, in MHz, available to virtual machines. This is the aggregated effective resource level from all running hosts. Hosts that are in maintenance mode or are unresponsive are not counted. Resources used by the VMware Service Console are not included in the aggregate. This value represents the amount of resources available for the root resource pool for running virtual machines. cpu.totalMHz Aggregated CPU resources of all hosts, in MHz. It does not filter out cpu used by system or related to hosts under maintenance. cpu.cores Number of physical CPU cores. Physical CPU cores are the processors contained by a CPU package. cpu.threads Aggregated number of CPU threads. mem.size Aggregated memory resources of all hosts, in MiB. It does not filter out memory used by system or related to hosts under maintenance. mem.effectiveSize Effective memory resources, in MiB, available to run virtual machines. This is the aggregated effective resource level from all running hosts. Hosts that are in maintenance mode or are unresponsive are not counted. Resources used by the VMware Service Console are not included in the aggregate. This value represents the amount of resources available for the root resource pool for running virtual machines. effectiveHosts Total number of effective hosts. This number exclude hosts under maintenance. hosts Total number of hosts overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. datastoreList List of datastore used by the cluster. A pipe or vertical bar character (|) is used as a separator. hostList List of hosts belonging to the cluster. A pipe or vertical bar character (|) is used as a separator. networkList List of networks attached to the cluster. A pipe or vertical bar character (|) is used as a separator. drsConfig.vmotionRate Threshold for generated ClusterRecommendations. DRS generates only those recommendations that are above the specified vmotionRate. Ratings vary from 1 to 5. This setting applies to manual, partiallyAutomated, and fullyAutomated DRS clusters. dasConfig.restartPriorityTimeout Maximum time the lower priority VMs should wait for the higher priority VMs to be ready (Seconds). datacenterName Datacenter name. datacenterLocation Datacenter location. drsConfig.enabled Flag indicating whether or not the service is enabled. drsConfig.enableVmBehaviorOverrides Flag that dictates whether DRS Behavior overrides for individual virtual machines (ClusterDrsVmConfigInfo) are enabled. drsConfig.defaultVmBehavior Specifies the cluster-wide default DRS behavior for virtual machines. You can override the default behavior for a virtual machine by using the ClusterDrsVmConfigInfo object. dasConfig.enabled Flag to indicate whether or not vSphere HA feature is enabled. dasConfig.admissionControlEnabled Flag that determines whether strict admission control is enabled dasConfig.isolationResponse Indicates whether or not the virtual machine should be powered off if a host determines that it is isolated from the rest of the compute resource. dasConfig.restartPriority Restart priority for a virtual machine. dasConfig.hostMonitoring Determines whether HA restarts virtual machines after a host fails. dasConfig.vmMonitoring Level of HA Virtual Machine Health Monitoring Service. dasConfig.vmComponentProtecting This property indicates if vSphere HA VM Component Protection service is enabled. dasConfig.hbDatastoreCandidatePolicy The policy on what datastores will be used by vCenter Server to choose heartbeat datastores: allFeasibleDs, allFeasibleDsWithUserPreference, userSelectedDs VSphereSnapshotVmSample Name Description snapshotTreeInfo Tree info for the snapshot. Es: Cluster:Vm:Snapshot1:Snapshot2 name Snapshot name creationTime Snapshot creation time powerState The power state of the virtual machine when this snapshot was taken snapshotId The unique identifier that distinguishes this snapshot from other snapshots of the virtual machine quiesced Flag to indicate whether or not the snapshot was created with the \"quiesce\" option, ensuring a consistent state of the file system backupManifest The relative path from the snapshotDirectory pointing to the backup manifest. Available for certain quiesced snapshots only description Description of the snapshot replaySupported Flag to indicate whether this snapshot is associated with a recording session on the virtual machine that can be replayed totalMemoryInDisk Total size of memory in disk. totalUniqueMemoryInDisk Total size of the file corresponding to the file blocks that were allocated uniquely to store memory. In other words, if the underlying storage supports sharing of file blocks across disk files, the property corresponds to the size of the file blocks that were allocated only in context of this file. It does not include shared blocks that were allocated in other files. This property will be unset if the underlying implementation is unable to compute this information. totalDisk Total size of snapshot files in disk totalUniqueDisk Total size of the file corresponding to the file blocks that were allocated uniquely to store snapshot data in disk. In other words, if the underlying storage supports sharing of file blocks across disk files, the property corresponds to the size of the file blocks that were allocated only in context of this file. It does not include shared blocks that were allocated in other files. This property will be unset if the underlying implementation is unable to compute this information. datastorePathDisk Disk file path in the datastore datastorePathMemory Memory file path in the datastore",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.63478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware vSphere <em>monitoring</em> <em>integration</em>",
        "sections": "VMware vSphere <em>monitoring</em> <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": "New Relic&#x27;s VMware vSphere <em>integration</em> helps you understand the health and performance of your vSphere environment. You can: Query data to get insights on the performance on your hypervisors, <em>virtual</em> <em>machines</em>, and more. Go from high level views down to the most granular data. vSphere data"
      },
      "id": "617dacb6196a676aaef7dcf3"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VPN Gateway monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPN Gateways VPNGateway data"
      ],
      "title": "Azure VPN Gateway monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "7c7e90887a336450dfdef6962b238f29085532cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Azure VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our Azure VPN Gateway integration collects bandwidth and packet drop data from your Azure VPN Gateway service. You can monitor and alert on your Azure VPN Gateway data from New Relic Infrastructure, and you can create custom queries and custom chart dashboards. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure VPN Gateways integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event Type Provider VPNGateway AzureVpnGatewaysVPNGatewaySample AzureVpnGatewaysVPNGateway For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure VPN Gateways data for VPNGateway. VPN Gateways VPNGateway data Metric Unit Description averageBandwidthBytesPerSecond BytesPerSecond Average site-to-site bandwidth of a gateway in bytes per second p2SBandwidthBytesPerSecond BytesPerSecond Average point-to-site bandwidth of a gateway in bytes per second p2SConnectionCount Count Point-to-site connection count of a gateway tunnelAverageBandwidthBytesPerSecond BytesPerSecond Average bandwidth of a tunnel in bytes per second tunnelEgressBytes Bytes Outgoing bytes of a tunnel tunnelIngressBytes Bytes Incoming bytes of a tunnel tunnelEgressPackets Count Outgoing packet count of a tunnel tunnelIngressPackets Count Incoming packet count of a tunnel tunnelEgressPacketDropTSMismatch Count Outgoing packet drop count from traffic selector mismatch of a tunnel tunnelIngressPacketDropTSMismatch Count Incoming packet drop count from traffic selector mismatch of a tunnel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VPN Gateway monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Azure</em> VPN Gateway data to our products. This document explains how to activate this integration and describes the data that can be reported. Features Our <em>Azure</em> VPN Gateway integration collects bandwidth and packet drop data from"
      },
      "id": "617d54a6196a674fc0f7dce5"
    },
    {
      "sections": [
        "Azure Virtual Network monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Azure Virtual Network monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "21564ee0f8fc6acad9c0829b83bea6a569a03d2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration/",
      "published_at": "2022-01-08T08:58:46Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Virtual Network that reports data from your Virtual Network service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Network reports metric data about your virtual networks (VNets), like packets dropped per second or bytes forwarded per second. It also collects inventory data about the status and configuration of your account. You can monitor and alert on your Virtual Network data from New Relic, and you can create custom queries and chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Network integration: Polling interval: 1 minute for metrics; 5 minutes for inventory Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualNetworksPublicIpAddressSample event type. For more on how to find and use integration data, see Understand and use data. Metric data Here are the metrics reported by the Azure Virtual Network integration. For more about how data is structured and reported to New Relic, see Understand and use integration data. Metric Description BytesDroppedDDoS Average inbound bytes dropped per second, DDoS. BytesForwardedDDoS Average inbound bytes forwarded per second, DDoS. BytesInDDoS Average inbound bytes per second, DDoS. DDoSTriggerTCPPackets Average inbound TCP packets per second to trigger DDoS mitigation. DDoSTriggerUDPPackets Inbound UDP packets to trigger DDoS mitigation. IfUnderDDoSAttack Count of entities under DDoS attack. PacketsInDDoS Average inbound packets per second, DDoS PacketsDroppedDDoS Average inbound packets dropped per second, DDoS. PacketsForwardedDDoS Average inbound packets forwarded per second, DDoS. TCPBytesInDDoS Average inbound TCP bytes per second, DDoS. TCPBytesDroppedDDoS Average inbound TCP bytes dropped per second, DDoS. TCPBytesForwardedDDoS Average inbound TCP bytes forwarded per second, DDoS. TCPPacketsInDDoS Average inbound TCP packets per second, DDoS. TCPPacketsDroppedDDoS Average inbound TCP packets dropped per second, DDoS. TCPPacketsForwardedDDoS Average inbound TCP packets forwarded per second, DDoS. UDPPacketsInDDoS Average inbound UDP packets per second, DDoS. UDPPacketsDroppedDDoS Average inbound UDP packets dropped per second, DDoS. UDPPacketsForwardedDDoS Average inbound UDP packets forwarded per second, DDoS. UDPBytesInDDoS Average inbound UDP bytes per second, DDoS. UDPBytesDroppedDDoS Average inbound UDP bytes dropped per second, DDoS. UDPBytesForwardedDDoS Average inbound UDP bytes forwarded per second, DDoS. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Network integration reports this inventory data: Inventory category Data reported azure/virtualnetworks/peering This category includes Azure's Virtual Network peering. Data includes: allowVirtualNetworkAccess allowForwardedTraffic allowGatewayTransit useRemoteGateways virtualNetworkId remoteVirtualNetworkId peeringState provisioningState azure/virtualnetworks/subnet This category includes subnet and queue data. Data includes: addressPrefix networkSecurityGroupId routeTableId provisioningState virtualNetworkId fullSubnetName azure/virtualnetworks/virtual-network This category includes subscription data. Data includes: addressSpace dnsServers provisioningState resourceGroupName (deprecates ResourceGroup) azure/virtualnetworks/security-rule/ Security rules allow or deny inbound or outbound network traffic based on source or destination IP address, port, and protocol. Data includes: access description destinationAddressPrefix destinationPortRange direction priority protocol provisioningState regionName resourceGroupName sourceAddressPrefix sourcePortRange azure/virtualnetworks/security-group/ Security groups contain a list of security rules, and are used to limit network traffic to resources in a virtual network. Data includes: name networkInterfaceIds provisioningState regionName resourceGroupName subnetIds type azure/virtualnetworks/route/ This category includes route data. Data includes: destinationAddressPrefix name nextHopIpAddress nextHopType provisioningState regionName resourceGroupName routeTableId azure/virtualnetworks/route-table/ Route tables enable resources connected to any subnet in any virtual network to communicate with each other, and the Internet. Data includes: name provisioningState regionName resourceGroupName type azure/virtualnetworks/public-ip-address/ Public IP addresses are used for communication with the Internet, including Azure public-facing services. Data includes: assignedNetworkInterface idleTimeoutInMinutes ipAllocationMethod name provisionedState regionName resourceGroupName type version assignedLoadBalancer fqdn leafDomainLabel azure/virtualnetworks/network-interface/ A network interface enables an Azure Virtual Machine to communicate with Internet, Azure, and on-premises resources. Data includes: appliedDnsServers dnsServers isAcceleratedNetworkingEnabled isIpForwardingEnabled name networkSecurityGroup privateIp privateIpAllocationMethod provisioningState regionName resourceGroupName type virtualMachineId internalDomainNameSuffix macAddress azure/virtualnetworks/ip-configuration/ This category includes Azure's IP configuration Data includes: isPrimary name networkInterfaceId privateIp privateIpAllocationMethod privateIpVersion regionName resourceGroupName subnet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Virtual Network monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Virtual Network monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em>&#x27;s Virtual Network that reports data from your Virtual Network service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a7196a6713e6f7d95d"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration": [
    {
      "sections": [
        "Azure Cosmos DB (Document DB) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and query data",
        "Metric data",
        "Important",
        "Account Data",
        "DataBase Data",
        "Collection Data",
        "Inventory data"
      ],
      "title": "Azure Cosmos DB (Document DB) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "e4bb0ee9204d3af8c336f3bccd58052df2451116",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-07T05:48:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Cosmos DB service that reports your Cosmos DB metrics and inventory data to New Relic. This document explains how to activate the Cosmos DB integration and describes the data that can be captured. Features New Relic gathers both database data and collection billing data from your Azure Cosmos DB service. You can monitor and alert on your Azure Cosmos DB data from New Relic, and you can create custom queries and custom chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. The Cosmos DB integration requires you to create an additional role and permission to fetch database and collection data: Go to the Azure Portal and open a shell by selecting the terminal icon. Add the following command: az role definition create --role-definition '{ \"Name\": \"NewRelic Integrations\", \"Actions\": [ \"*/read\", \"Microsoft.DocumentDB/databaseAccounts/listKeys/action\" ], \"NotActions\": [], \"AssignableScopes\": [ \"/subscriptions/YOUR_INSERT_SUBSCRIPTION_ID\" ], \"Description\": \"Read Only for NewRelic Integrations\", \"IsCustom\": \"true\" }' Copy From Services > Subscriptions, select the subscription, go to Access control (IAM), and then select Add. In the Role search box, add the name of the newly created role definition (for example, NewRelic Integrations). In the Select search box, add the name of the New Relic integration application, and select it. Ensure that the application is added to the Selected members list, then Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Cosmos DB integration: Polling interval: 5 minutes Resolution: 1 minute or 5 minutes, varies by metric. For resolution information on a specific metric, see Microsoft Azure's documentation about support metrics. View and query data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select the Cosmos DB Integration. You can query and explore your data using the following event types: Entity Event Type Provider Account AzureCosmosDbAccountSample AzureCosmosDbAccount Database AzureCosmosDbDatabaseSample AzureCosmosDbDatabase Collection AzureCosmosDbCollectionSample AzureCosmosDbCollection For more on how to find and use data, see Understand and use integration data. Metric data Important For information on deprecated Cosmos DB events or metrics, see Azure Cosmos DB integration (deprecated). We strongly recommend migrating to the supported events and metrics in this document. To view metrics reported by the Cosmos DB integration, query the Entities below. Use the metadata associated with each metric to filter and facet the data being reported. For detailed metric information, see the Azure supported metrics documentation. Account Data Metric Description Metadata totalRequests Total number of requests. account kind region offerType statusCode resourceGroup metadataRequests Count of metadata requests. account kind region offerType statusCode resourceGroup mongoRequests Count of Mongo requests made. account kind region commandName offerType errorCode resourceGroup mongoRequestCharge Total number of Mongo request units consumed. account kind region commandName offerType errorCode resourceGroup totalRequestUnits Total number of request units consumed. account kind region offerType statusCode resourceGroup provisionedThroughput Throughput provisioned for the database or collection. account offerType kind resourceGroup availableStorageBytes Total available storage, in bytes. account kind offerType region resourceGroup dataUsageBytes Total data usage reported, in bytes. account kind offerType region resourceGroup indexUsageBytes Total index usage reported, in bytes. account kind offerType region resourceGroup documentQuotaBytes Total storage quota reported, in bytes. account kind offerType region resourceGroup documentCount Total document count reported. account kind offerType region resourceGroup ReplicationLatency P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account kind sourceRegion offerType targetRegion resourceGroup ServiceAvailability Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraRequestCharges Total number of request units consumed for Cassandra requests. account kind errorCode offerType opperationType region resourceType resourceGroup cassandraConnectionClosures Total number of Cassandra connections that were closed. account kind closureReason offerType region resourceGroup DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region DataBase Data Metric Description Metadata totalRequests Total number of requests. account databaseName region statusCode metadataRequests Count of metadata requests. account databaseName region statusCode mongoRequests Count of Mongo requests made. account databaseName region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account databaseName region commandName errorCode totalRequestUnits Total number of request units consumed. account databaseName region statusCode provisionedThroughput Throughput provisioned for the database or collection. account databaseName availableStorageBytes Total available storage, in bytes. account databaseName region dataUsageBytes Total data usage reported, in bytes. account databaseName region indexUsageBytes Total index usage reported, in bytes. account databaseName region documentQuotaBytes Total storage quota reported, in bytes. account databaseName region documentCount Total document count reported. account databaseName region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account databaseName errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account databaseName errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account closureReason region Collection Data Metric Description Metadata totalRequests Total number of requests. account collectionName database region statusCode metadataRequests Count of metadata requests. account collectionName database region statusCode mongoRequests Count of Mongo requests made. account collectionName database region commandName errorCode mongoRequestCharge Total number of Mongo request units consumed. account collectionName database region commandName errorCode totalRequestUnits Total number of request units consumed. account collectionName database region statusCode provisionedThroughput Throughput provisioned for the database or collection. account collectionName database availableStorageBytes Total available storage, in bytes. account collectionName database region dataUsageBytes Total data usage reported, in bytes. account collectionName database region indexUsageBytes Total index usage reported, in bytes. account collectionName database region documentQuotaBytes Total storage quota reported, in bytes. account collectionName database region documentCount Total document count reported. account collectionName database region replicationLatencyMilliseconds P99 replication latency across source and target regions for geo-enabled account, in milliseconds. account collectionName sourceRegion targetRegion serviceAvailabilityPercent Account requests availability percentage in hour, day, or month granularity. No specific metadata. cassandraRequests Count of Cassandra requests made. account collectionName database errorCode opperationType region resourceType cassandraRequestCharges Total number of request units consumed for Cassandra requests. account collectionName database errorCode opperationType region resourceType cassandraConnectionClosures Total number of Cassandra connections that were closed. account collectionName closureReason region Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Cosmos DB integration reports the inventory data for the entity type azure/cosmosdb/account/ using the following metadata: documentEndpoint: The document end point. databaseAccountOfferType: The database account offer type. consistencyPolicy: The consistency policy for the Cosmos DB database account. defaultConsistencyLevel: The default consistency level for the Cosmos DB database account. kind: The type of database account set at database account creation. resourceGroupName: The Azure resource group name that the Cosmos DB database account belong to. regionName: The region name in which the Azure DocumentDB database account is deployed. type: The azure resource type, which is Microsoft.DocumentDB/databaseAccounts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.88635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cosmos DB (Document DB) monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "-definition &#x27;{ &quot;Name&quot;: &quot;NewRelic <em>Integrations</em>&quot;, &quot;Actions&quot;: [ &quot;*&#x2F;read&quot;, &quot;<em>Microsoft</em>.DocumentDB&#x2F;databaseAccounts&#x2F;<em>list</em>Keys&#x2F;action&quot; ], &quot;NotActions&quot;: [], &quot;AssignableScopes&quot;: [ &quot;&#x2F;subscriptions&#x2F;YOUR_INSERT_SUBSCRIPTION_ID&quot; ], &quot;Description&quot;: &quot;Read Only for NewRelic <em>Integrations</em>&quot;, &quot;IsCustom&quot;: &quot;true&quot; }&#x27; Copy From"
      },
      "id": "617dc763e7b9d2d3dac0580e"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "426d04f6684d7e3beed48dbb2f71d0024db6a2a7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2022-01-08T08:19:07Z",
      "updated_at": "2021-10-23T17:42:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a6196a672080f7dcff"
    },
    {
      "sections": [
        "Azure Virtual Network monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Azure Virtual Network monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "21564ee0f8fc6acad9c0829b83bea6a569a03d2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration/",
      "published_at": "2022-01-08T08:58:46Z",
      "updated_at": "2021-10-23T17:42:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure's Virtual Network that reports data from your Virtual Network service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Network reports metric data about your virtual networks (VNets), like packets dropped per second or bytes forwarded per second. It also collects inventory data about the status and configuration of your account. You can monitor and alert on your Virtual Network data from New Relic, and you can create custom queries and chart dashboards. Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Network integration: Polling interval: 1 minute for metrics; 5 minutes for inventory Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualNetworksPublicIpAddressSample event type. For more on how to find and use integration data, see Understand and use data. Metric data Here are the metrics reported by the Azure Virtual Network integration. For more about how data is structured and reported to New Relic, see Understand and use integration data. Metric Description BytesDroppedDDoS Average inbound bytes dropped per second, DDoS. BytesForwardedDDoS Average inbound bytes forwarded per second, DDoS. BytesInDDoS Average inbound bytes per second, DDoS. DDoSTriggerTCPPackets Average inbound TCP packets per second to trigger DDoS mitigation. DDoSTriggerUDPPackets Inbound UDP packets to trigger DDoS mitigation. IfUnderDDoSAttack Count of entities under DDoS attack. PacketsInDDoS Average inbound packets per second, DDoS PacketsDroppedDDoS Average inbound packets dropped per second, DDoS. PacketsForwardedDDoS Average inbound packets forwarded per second, DDoS. TCPBytesInDDoS Average inbound TCP bytes per second, DDoS. TCPBytesDroppedDDoS Average inbound TCP bytes dropped per second, DDoS. TCPBytesForwardedDDoS Average inbound TCP bytes forwarded per second, DDoS. TCPPacketsInDDoS Average inbound TCP packets per second, DDoS. TCPPacketsDroppedDDoS Average inbound TCP packets dropped per second, DDoS. TCPPacketsForwardedDDoS Average inbound TCP packets forwarded per second, DDoS. UDPPacketsInDDoS Average inbound UDP packets per second, DDoS. UDPPacketsDroppedDDoS Average inbound UDP packets dropped per second, DDoS. UDPPacketsForwardedDDoS Average inbound UDP packets forwarded per second, DDoS. UDPBytesInDDoS Average inbound UDP bytes per second, DDoS. UDPBytesDroppedDDoS Average inbound UDP bytes dropped per second, DDoS. UDPBytesForwardedDDoS Average inbound UDP bytes forwarded per second, DDoS. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Network integration reports this inventory data: Inventory category Data reported azure/virtualnetworks/peering This category includes Azure's Virtual Network peering. Data includes: allowVirtualNetworkAccess allowForwardedTraffic allowGatewayTransit useRemoteGateways virtualNetworkId remoteVirtualNetworkId peeringState provisioningState azure/virtualnetworks/subnet This category includes subnet and queue data. Data includes: addressPrefix networkSecurityGroupId routeTableId provisioningState virtualNetworkId fullSubnetName azure/virtualnetworks/virtual-network This category includes subscription data. Data includes: addressSpace dnsServers provisioningState resourceGroupName (deprecates ResourceGroup) azure/virtualnetworks/security-rule/ Security rules allow or deny inbound or outbound network traffic based on source or destination IP address, port, and protocol. Data includes: access description destinationAddressPrefix destinationPortRange direction priority protocol provisioningState regionName resourceGroupName sourceAddressPrefix sourcePortRange azure/virtualnetworks/security-group/ Security groups contain a list of security rules, and are used to limit network traffic to resources in a virtual network. Data includes: name networkInterfaceIds provisioningState regionName resourceGroupName subnetIds type azure/virtualnetworks/route/ This category includes route data. Data includes: destinationAddressPrefix name nextHopIpAddress nextHopType provisioningState regionName resourceGroupName routeTableId azure/virtualnetworks/route-table/ Route tables enable resources connected to any subnet in any virtual network to communicate with each other, and the Internet. Data includes: name provisioningState regionName resourceGroupName type azure/virtualnetworks/public-ip-address/ Public IP addresses are used for communication with the Internet, including Azure public-facing services. Data includes: assignedNetworkInterface idleTimeoutInMinutes ipAllocationMethod name provisionedState regionName resourceGroupName type version assignedLoadBalancer fqdn leafDomainLabel azure/virtualnetworks/network-interface/ A network interface enables an Azure Virtual Machine to communicate with Internet, Azure, and on-premises resources. Data includes: appliedDnsServers dnsServers isAcceleratedNetworkingEnabled isIpForwardingEnabled name networkSecurityGroup privateIp privateIpAllocationMethod provisioningState regionName resourceGroupName type virtualMachineId internalDomainNameSuffix macAddress azure/virtualnetworks/ip-configuration/ This category includes Azure's IP configuration Data includes: isPrimary name networkInterfaceId privateIp privateIpAllocationMethod privateIpVersion regionName resourceGroupName subnet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.2137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Virtual Network monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Virtual Network monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em>&#x27;s Virtual Network that reports data from your Virtual Network service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "617d54a7196a6713e6f7d95d"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/get-started/activate-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features",
        "Cost considerations"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "7ce4149eca2602fa1a29c921fa8876ad96abd254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2022-01-08T08:19:58Z",
      "updated_at": "2021-12-25T13:06:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data. Cost considerations When evaluating the cost of the Microsoft Azure integrations with New Relic, consider Azure's Monitor Pricing. Refer to the \"Metric queries\" cost item in the Azure pricing documentation. Pricing is based on the number of API calls per month. An estimation of the API calls peformed by New Relic to the different Azure services can be seen in the UI, under Infrastructure > Azure > Azure Status Dashboard.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.297,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "617d53f828ccbcd2fd7ffaf2"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f22bdeae93bc45373cbcdc6f1a19783af5efca3e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2022-01-09T01:49:43Z",
      "updated_at": "2021-10-24T02:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.26021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "617d515228ccbc7acd7fecd6"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f69f3cdd65fc513f97d77b8dc875ac3c0e5560a4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2022-01-08T07:19:56Z",
      "updated_at": "2021-10-24T02:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.01419,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "617dc187e7b9d21ac3c04204"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/get-started/azure-integration-metrics": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features",
        "Cost considerations"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "7ce4149eca2602fa1a29c921fa8876ad96abd254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2022-01-08T08:19:58Z",
      "updated_at": "2021-12-25T13:06:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data. Cost considerations When evaluating the cost of the Microsoft Azure integrations with New Relic, consider Azure's Monitor Pricing. Refer to the \"Metric queries\" cost item in the Azure pricing documentation. Pricing is based on the number of API calls per month. An estimation of the API calls peformed by New Relic to the different Azure services can be seen in the UI, under Infrastructure > Azure > Azure Status Dashboard.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.297,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "617d53f828ccbcd2fd7ffaf2"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "08a33c782bb862ade57c480a77dbd737f9da1003",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-24T02:35:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.4647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "617dae41196a6723f6f7e430"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f22bdeae93bc45373cbcdc6f1a19783af5efca3e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2022-01-09T01:49:43Z",
      "updated_at": "2021-10-24T02:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.26021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "617d515228ccbc7acd7fecd6"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations": [
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "08a33c782bb862ade57c480a77dbd737f9da1003",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-24T02:35:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.4647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "617dae41196a6723f6f7e430"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f22bdeae93bc45373cbcdc6f1a19783af5efca3e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2022-01-09T01:49:43Z",
      "updated_at": "2021-10-24T02:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.26021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "617d515228ccbc7acd7fecd6"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f69f3cdd65fc513f97d77b8dc875ac3c0e5560a4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2022-01-08T07:19:56Z",
      "updated_at": "2021-10-24T02:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.01418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "617dc187e7b9d21ac3c04204"
    }
  ],
  "/docs/infrastructure/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features",
        "Cost considerations"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "7ce4149eca2602fa1a29c921fa8876ad96abd254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2022-01-08T08:19:58Z",
      "updated_at": "2021-12-25T13:06:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data. Cost considerations When evaluating the cost of the Microsoft Azure integrations with New Relic, consider Azure's Monitor Pricing. Refer to the \"Metric queries\" cost item in the Azure pricing documentation. Pricing is based on the number of API calls per month. An estimation of the API calls peformed by New Relic to the different Azure services can be seen in the UI, under Infrastructure > Azure > Azure Status Dashboard.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.29697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "617d53f828ccbcd2fd7ffaf2"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "08a33c782bb862ade57c480a77dbd737f9da1003",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2022-01-08T08:19:35Z",
      "updated_at": "2021-10-24T02:35:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.4647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "617dae41196a6723f6f7e430"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f69f3cdd65fc513f97d77b8dc875ac3c0e5560a4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2022-01-08T07:19:56Z",
      "updated_at": "2021-10-24T02:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.01418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "617dc187e7b9d21ac3c04204"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/filter-group/filter-sets-organize-your-infrastructure-hosts": [
    {
      "sections": [
        "Group infrastructure results by specific attributes",
        "Group charts by specific attributes",
        "Combine filter sets and grouping",
        "Increased CPU usage on a single host"
      ],
      "title": "Group infrastructure results by specific attributes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Filter and group"
      ],
      "external_id": "8436b1d0391cd7caa2a79c4080528697ff7a012d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/filter-group/group-infrastructure-results-specific-attributes/",
      "published_at": "2022-01-08T08:21:22Z",
      "updated_at": "2021-03-11T10:49:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In our infrastructure monitoring tool, you can use the Group by feature to group chart results by specific attributes. For example, on the Hosts page, you might display the AWS regions with the highest CPU usage grouped by awsRegion. Group by is available near the top of some infrastructure monitoring UI pages. Group charts by specific attributes On some infrastructure pages you can use Group by to group page results and charts by a specific attribute, such as host name, entity ID, or AWS region. The attributes available to group by will depend on your system setup. These may include: Default infrastructure attributes Custom attributes APM-related attributes To group infrastructure results by a specific attribute: On pages that have this feature, select Group by (located beside the time picker). From the dropdown, select an attribute to group by. Combine filter sets and grouping Grouping applies to any filter sets you have selected. By combining filter sets with Group by, you can find detailed system information quickly. Increased CPU usage on a single host On the Filter sets sidebar, you see alert threshold violations as Critical icon or Warning icon on one of your filter sets. To view only the hosts related to the filter set on your Hosts page, click the filter set name. To determine which of the hosts is causing the problem, select Group by, then select the hostname attribute. Review the charts which now show the hosts, by name, with the highest CPU usage.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.74147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Group</em> <em>infrastructure</em> results by specific attributes",
        "sections": "<em>Group</em> <em>infrastructure</em> results by specific attributes",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " monitoring UI pages. <em>Group</em> charts by specific attributes On some <em>infrastructure</em> pages you can use <em>Group</em> by to <em>group</em> page results and charts by a specific attribute, such as host name, entity ID, or AWS region. The attributes available to <em>group</em> by will depend on <em>your</em> system setup. These may include: Default"
      },
      "id": "6043edcd64441fd920378ecf"
    },
    {
      "sections": [
        "Dimensional metric equivalents for the agent and on-host integrations",
        "BETA FEATURE"
      ],
      "title": "Dimensional metric equivalents for the agent and on-host integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2022-01-08T07:23:37Z",
      "updated_at": "2021-11-26T06:28:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. In the past, our infrastructure agent and on-host integrations have reported metrics as attributes attached to events, also known as \"sample data.\" We have now made these metrics also available as dimensional metrics, a data format that allows for improved analysis and aggregation over time. The following table presents the equivalent dimensional metric names for our infrastructure agent and for our on-host integrations. For tips on how to query dimensional metrics, see Query dimensional metrics. Integration Dimensional metric name (new) Sample metric name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTim",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 99.17028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "sections": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "BETA FEATURE This feature is currently in beta. In the past, our <em>infrastructure</em> agent and on-host integrations have reported metrics as attributes attached to events, also known as &quot;sample <em>data</em>.&quot; We have now made these metrics also available as dimensional metrics, a <em>data</em> format that allows"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "View logs for your APM and infrastructure data",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-13T14:37:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. View logs for your APM and infrastructure data You can also bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 95.78937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "View logs for <em>your</em> APM and <em>infrastructure</em> <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " <em>Filter</em> hosts by application <em>data</em> Switch between <em>Infrastructure</em> and APM Examine APM <em>data</em> in Inventory and Events pages <em>Infrastructure</em> <em>data</em> appears in APM in the host table on the APM Summary page. View logs for <em>your</em> APM and <em>infrastructure</em> <em>data</em> You can also bring <em>your</em> logs and application&#x27;s <em>data</em> together"
      },
      "id": "603e88b2e7b9d246932a07f6"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/filter-group/group-infrastructure-results-specific-attributes": [
    {
      "sections": [
        "Filter sets: Organize your hosts",
        "Benefits of filter sets",
        "Create filter sets",
        "Edit filter sets",
        "Delete filter sets",
        "Combine filter sets with grouping",
        "Copy filters from filter set to alerts",
        "Important",
        "Filter set logic",
        "Inclusion and exclusion",
        "Recommended: Select values by matching a string",
        "Tip",
        "Select values individually",
        "And/Or"
      ],
      "title": "Filter sets: Organize your hosts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Filter and group"
      ],
      "external_id": "ae70ce239865f3cb006e2ed47fc9bf3fc0598d81",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/filter-group/filter-sets-organize-your-infrastructure-hosts/",
      "published_at": "2022-01-08T08:20:49Z",
      "updated_at": "2021-03-11T08:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic infrastructure monitoring, you can combine filters into a filter set to organize hosts based on criteria that matter the most to you. Read on to learn about the benefits, use, and logic of filter sets. Benefits of filter sets You can create filter sets using available attributes or tags. For example, you can organize your infrastructure into categories such as: Regions Operating system versions Hosts associated with Docker containers Test environments You can share filter sets with other people in your account, and you can quickly identify infrastructure problems by checking the color-coded health status of each host in the filter set. Create filter sets The default infrastructure filter set is All hosts, and it serves as a template for you to create filter sets. To create a filter set: Go to one.newrelic.com > Infrastructure and click Hosts, Inventory, or Events. If All hosts is not displayed in the left sidebar, open that filter set by selecting Saved filter sets > All hosts. In All hosts, click Filter hosts. In the list, click an item to see a list of values. Click Include or Exclude (see Filter set logic). Click values individually or enter text to match multiple values. Continue adding filters until you have the filter set you want. To name your filter, click the icon, type a name, and click Save. Edit filter sets To change an existing filter set: Go to one.newrelic.com > Infrastructure and click Hosts, Inventory, or Events. In the sidebar, click Saved filter sets to open a list. Locate the filter set by scrolling or by entering a search term. Click the filter set to open it. In the sidebar, click an option to update your filter set, and then save. Delete filter sets You can delete any saved filter set except the default All hosts. Go to one.newrelic.com > Infrastructure > Settings > Filter sets. Click to delete the filter set. Combine filter sets with grouping On some pages you can use Group by to group chart results by specific attributes. For example, on the Hosts page, you can group by awsRegion to display the AWS regions with the highest CPU usage. Grouping applies to the selected filter sets. By combining filter sets with grouping, you can find detailed system information quickly. For an example of using these tools to troubleshoot a problem, see Combining filter sets and grouping. Copy filters from filter set to alerts When you create an alert condition, you can build filters individually, or you can copy all the filters from a filter set into a new alert condition. This is a quick shortcut to populate a new alert condition with some filters. Important When you copy filter set filters to a new alert condition, these filters are no longer tied to the filter set. If you make changes to the filter set, the alert filters are not affected. To copy filter set filters to a new alert condition: Go to one.newrelic.com > Infrastructure and click Hosts, Inventory, or Events. In the sidebar, click Saved filter sets to open a list. Locate the filter set by scrolling or by entering a search term. Click the filter set to open it. Mouse over any chart and click > Create alert. Enter an alert condition name. Make adjustments to filters as necessary. Complete the remaining alert fields (see Create alert conditions). Filter set logic When you create a filter set, you generate a list of attributes and/or tags that narrow the results. This section explains how filter sets apply various rules to the list. Inclusion and exclusion As part of building a filter set, you designate whether a filter should include or exclude entities that match certain values. The way the inclusion or exclusion works depends on how you select values: Recommended: Select values by matching a string You can generate a list of values by entering a string that you want values to match. This is useful for matching multiple values. Tip String matching efficiently generates a list of values, and this approach scales as you add new entities. Here is the logic filter sets use with string matching: Comparator Logic Include If you click Include and then enter a string that you want values to match, the filter uses the comparator LIKE, which means the results include any entities that are like the string. For example, you could filter by the term East-, and all entities that contain that term are returned. Exclude If you click Exclude and then enter a string that you want values to match, the filter uses the comparator NOT LIKE, which means the results exclude any entities that are like the string. For example, you could filter by the term West-, and all entities that do not contain that term are returned. Select values individually You can click through the list of attributes/tags to identify individual values. Tip This approach does not scale well if you add new entities. Here is the logic filter sets use with individual value selection: Comparator Logic Include If you click Include and then click specific values, the filter uses the comparator IN, which means the filter looks for entities that exactly match one or more values in your list of selections. Exclude If you click Exclude and then click specific values, the filter set uses the comparator NOT IN, which means the filter returns all entities that do not exactly match one or more values in your list of selections. And/Or Filter sets use the logical operators AND and OR behind the scenes to join the data. Here are the rules for AND and OR: When you click values from multiple attributes or tags, they are joined by AND. When you click values from within an attribute or tags, they are joined by OR. The filter results display hosts for which both of the following are true: Hosts containing any one of the selected infrastructure agent versions Hosts in any one of the selected AWS availability zones",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.73984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Filter</em> sets: Organize <em>your</em> hosts",
        "sections": "<em>Filter</em> sets: Organize <em>your</em> hosts",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " or tags. For example, you can organize <em>your</em> <em>infrastructure</em> into categories such as: Regions Operating system versions Hosts associated with Docker containers Test environments You can share <em>filter</em> sets with other people in <em>your</em> account, and you can quickly identify <em>infrastructure</em> problems by checking"
      },
      "id": "6043ed8ee7b9d289955799cb"
    },
    {
      "sections": [
        "Dimensional metric equivalents for the agent and on-host integrations",
        "BETA FEATURE"
      ],
      "title": "Dimensional metric equivalents for the agent and on-host integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2022-01-08T07:23:37Z",
      "updated_at": "2021-11-26T06:28:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. In the past, our infrastructure agent and on-host integrations have reported metrics as attributes attached to events, also known as \"sample data.\" We have now made these metrics also available as dimensional metrics, a data format that allows for improved analysis and aggregation over time. The following table presents the equivalent dimensional metric names for our infrastructure agent and for our on-host integrations. For tips on how to query dimensional metrics, see Query dimensional metrics. Integration Dimensional metric name (new) Sample metric name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTim",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 99.17027,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "sections": "Dimensional metric equivalents for the agent <em>and</em> on-host integrations",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "BETA FEATURE This feature is currently in beta. In the past, our <em>infrastructure</em> agent and on-host integrations have reported metrics as attributes attached to events, also known as &quot;sample <em>data</em>.&quot; We have now made these metrics also available as dimensional metrics, a <em>data</em> format that allows"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "View logs for your APM and infrastructure data",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2022-01-08T13:04:47Z",
      "updated_at": "2021-11-13T14:37:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. View logs for your APM and infrastructure data You can also bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 95.78937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "View logs for <em>your</em> APM and <em>infrastructure</em> <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " <em>Filter</em> hosts by application <em>data</em> Switch between <em>Infrastructure</em> and APM Examine APM <em>data</em> in Inventory and Events pages <em>Infrastructure</em> <em>data</em> appears in APM in the host table on the APM Summary page. View logs for <em>your</em> APM and <em>infrastructure</em> <em>data</em> You can also bring <em>your</em> logs and application&#x27;s <em>data</em> together"
      },
      "id": "603e88b2e7b9d246932a07f6"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes": [
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Caution",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2022-01-08T12:32:05Z",
      "updated_at": "2021-09-20T19:26:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. Caution If you have filtered your Host Not Reporting condition using tags or labels and then remove a critical tag or label from a targeted host, the system will open a Host Not Reporting violation, since it will characterize that host as having lost its connection. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Alternatively, you can add the hostStatus: shutdown tag to your host along with checking the option mentioned above. This will stop all Host Not Reporting violations from opening for that host, as long as that tag is on it, regardless of agent version or OS. Removing the tag will allow the system to open Host Not Reporting violations for that host again. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.05692,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    },
    {
      "sections": [
        "Alerts for infrastructure: Add, edit, or view host alert information",
        "Create alert conditions for infrastructure",
        "Important",
        "Other infrastructure alert condition methods",
        "Use the Alerts UI",
        "Use the Infrastructure UI",
        "Use infrastructure settings for integrations",
        "Tip",
        "View host alert events",
        "Update or delete host alert information",
        "Use New Relic Alerts to monitor your entire infrastructure",
        "Add a description",
        "Add or edit a runbook URL",
        "Violation time limit for violations",
        "Alert conditions that generate too-long NRQL queries"
      ],
      "title": "Alerts for infrastructure: Add, edit, or view host alert information",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "00207a1020aa29ea6d5d5bbb8e806a50a5966f80",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerts-add-edit-or-view-host-alert-information/",
      "published_at": "2022-01-08T08:22:32Z",
      "updated_at": "2021-08-02T12:47:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring, you can create alert conditions directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic Alerts. Instead, you can immediately select your filter set and tailor the alert condition directly from the chart you are viewing. This helps you proactively manage and monitor the alerting system for your environment. Any alert violations will be created per entity within the filter set. Create alert conditions for infrastructure Alert conditions apply to alert policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to use other types of notification channels, create a new policy from within the Alerts UI. Important The Infrastructure REST API has a limit of 3,700 alert conditions, including both active and disabled conditions. The API, whether used directly or via the UI, will reject all requests to add any additional alert conditions beyond the 3,700 alert condition limit. To add an infrastructure alert condition to an alerts policy: Go to one.newrelic.com > Infrastructure, then select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Type a meaningful condition name. Select the Alert type, or refer to the examples to decide which type to select. Create individual filters, or copy all the filters from a filter set to identify the hosts that you want the alert condition to use. Important For more information about the rules behind filters, see Filter set logic. Define the Critical (required) and Warning (optional, if available) thresholds for triggering the alert notification. Optional: To create the condition criteria proactively but not receive alert notifications at this time, turn off the Enabled checkbox option. Select an existing policy for the new condition. OR Select the option to create a new policy and identify the email for alert notifications. Optional: Add a runbook url. Optional: Set Violation time limit for violations (this defaults to 24 hours). Select Create. Important If New Relic hasn't received a cloud integration service's attribute in the past 60 minutes, we refer to this as a \"silent attribute,\" and it won't be available to use as an alert condition in the UI. In this situation, you can use the API to create alert conditions for silent attributes. Other infrastructure alert condition methods You can also use these other methods to create an infrastructure alert condition: Use the Alerts UI Go to one.newrelic.com > Alerts & AI > Alerts > Alert policies > New alert policy > Create new condition, then select Infrastructure as the product. Use the Infrastructure UI Go to one.newrelic.com > Infrastructure. Select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Use infrastructure settings for integrations Tip Use this method to create an alert condition for infrastructure integrations. Go to one.newrelic.com > Infrastructure > Settings > Alerts, and then click Create alert condition. Name and describe the alert condition. Click the Integrations alert type, and then select the integration data source you'd like to use. Use the Filter entities dropdown to limit your condition to specific attributes. Use the Define thresholds dropdowns to define your condition's thresholds, and then click Create. The configuration settings are optional. You can always update them later. View host alert events Anyone included in the policy's notification channels receive alert notifications directly. In addition, anyone with permissions for your New Relic account can view Infrastructure alert incidents and individual violations through the user interface. Go to one.newrelic.com > Infrastructure > Events. To change the hosts or time frame, use the search window, Filter set, or Time functions. From the Events list, select the alert violation. To view detailed information in Alerts about the selected violation, select the link. Update or delete host alert information To edit, disable (or re-enable), or delete host alert information: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Use the search window or Select all checkbox to locate one or more alert conditions. Select any of the available functions to edit, disable, enable, or delete the selected conditions. Use New Relic Alerts to monitor your entire infrastructure New Relic Alerts provides a single, coordinated alerting tool across all of your New Relic products. This allows you to manage alert policies and conditions that focus on the metrics for entities that you care about the most, such as Docker containers, JVMs, and more. Alert features Features in Infrastructure Alert conditions Create: Use the Infrastructure UI. View, change, disable (or re-enable), or delete: Use the Infrastructure Settings > Alerts UI. Information on alerts View summary information about events: Use the Infrastructure Events UI. View detailed information about alert incidents or individual violations: Use the Alerts UI or the notification channel integrated with the associated policy. Alert policies View, add, change, disable, or delete: For policies with a variety of notification channels: Use the Alerts UI. For policies only needing email notifications: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Create a new policy, and add one or more email addresses as needed. Add host conditions to an existing policy: Use the Infrastructure UI. Notification channels To view, add, change, or delete available notification options: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Search for the condition or policy name. From the list of conditions, select the policy link to view notification channel information in the Alerts UI. Add a description The use of the Description field is available for these alert condition types: NRQL conditions: add a description using the NerdGraph API. Infrastructure conditions: add a description using the UI or the REST API. The text you place in an alert condition's Description field is passed downstream to associated violations and notifications. A description can be used for several purposes, including: Capturing the reason for the alert condition. Defining the signal being monitored. Defining next steps. Add metadata to downstream systems. You can use template substitution to insert values from the attributes in the associated violation event. The template format is {{attributeName}}. For the attributes you can use when creating a description, see Violation event attributes. One available attribute is the special {{tag.*}} attribute. This attribute prefix is used to access any of the tag values that are included with the target signal, or any of the entity tags that are associated with the target signal. If there are entity tags associated with your violation, then they can be accessed using the entity tag name. An example of this would be {{tag.aws.awsRegion}}. When entity tags are available to use, you see them included with the violation, and displayed when you view the violations in an incident. This field has a maximum character size of 4,000. Add or edit a runbook URL The alert condition creation process includes an option for setting a URL for runbook instructions. This lets you link to information or standard procedures for handling a violation. Before adding or updating the link, make sure you use a valid URL. To add, update, or delete an alert condition's runbook URL: Select an alert condition, and make changes to the Runbook URL link. Save the condition. In order to be saved, the URL must be a valid URL. Violation time limit for violations The violation time limit allows you to define a time period after which violations will be force-closed. By default, violation time limit is 24 hours. To add or update an alert condition's violation time limit: Select an alert condition, and make changes to the violation time limit. Save the condition. Alert conditions that generate too-long NRQL queries Alert conditions created for infrastructure rely on behind-the-scenes NRQL queries, and NRQL queries have a 4096-character limit. This means that if your condition generates a very complex NRQL query that filters on many elements (for example, including many hosts or many tags), it will exceed this limit and display an error message saying that the condition failed. To solve this problem, reduce the number of elements you are using in your alert condition. For example: Problem Solution Hosts If you entered a large number of hosts that caused the condition to fail, reduce the number of hosts. Use substrings to target hosts. For example, instead of targeting prod-host-01, prod-host-02, and prod-host-03, just target all hosts with prod-host-0 in the name. Entities Edit your alert condition to target specific attributes that apply to the entities you're trying to target. Create custom attributes for the entities you want to target, and use those attributes in your alert condition. For more information, see Best practices for filtering in infrastructure alerts in New Relic's Explorers Hub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.49585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> for <em>infrastructure</em>: Add, edit, or view host <em>alert</em> information",
        "sections": "Create <em>alert</em> <em>conditions</em> for <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "With New Relic&#x27;s <em>infrastructure</em> monitoring, you can create <em>alert</em> <em>conditions</em> directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic <em>Alerts</em>"
      },
      "id": "6043fa3428ccbc401d2c60b9"
    },
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2022-01-08T08:23:04Z",
      "updated_at": "2021-07-27T14:15:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. The no_trigger_on field is optional. When set to [\"shutdown\"] this enables the Don't trigger alerts for hosts that perform a clean shutdown infrastructure condition option. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12, \"no_trigger_on\": [\"shutdown\"] } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. For new conditions, if a value is not provided, the following default values are used: All conditions: 24 hours When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.12086,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerting-examples": [
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Caution",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2022-01-08T12:32:05Z",
      "updated_at": "2021-09-20T19:26:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. Caution If you have filtered your Host Not Reporting condition using tags or labels and then remove a critical tag or label from a targeted host, the system will open a Host Not Reporting violation, since it will characterize that host as having lost its connection. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Alternatively, you can add the hostStatus: shutdown tag to your host along with checking the option mentioned above. This will stop all Host Not Reporting violations from opening for that host, as long as that tag is on it, regardless of agent version or OS. Removing the tag will allow the system to open Host Not Reporting violations for that host again. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.05692,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    },
    {
      "sections": [
        "Alerts for infrastructure: Add, edit, or view host alert information",
        "Create alert conditions for infrastructure",
        "Important",
        "Other infrastructure alert condition methods",
        "Use the Alerts UI",
        "Use the Infrastructure UI",
        "Use infrastructure settings for integrations",
        "Tip",
        "View host alert events",
        "Update or delete host alert information",
        "Use New Relic Alerts to monitor your entire infrastructure",
        "Add a description",
        "Add or edit a runbook URL",
        "Violation time limit for violations",
        "Alert conditions that generate too-long NRQL queries"
      ],
      "title": "Alerts for infrastructure: Add, edit, or view host alert information",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "00207a1020aa29ea6d5d5bbb8e806a50a5966f80",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerts-add-edit-or-view-host-alert-information/",
      "published_at": "2022-01-08T08:22:32Z",
      "updated_at": "2021-08-02T12:47:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring, you can create alert conditions directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic Alerts. Instead, you can immediately select your filter set and tailor the alert condition directly from the chart you are viewing. This helps you proactively manage and monitor the alerting system for your environment. Any alert violations will be created per entity within the filter set. Create alert conditions for infrastructure Alert conditions apply to alert policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to use other types of notification channels, create a new policy from within the Alerts UI. Important The Infrastructure REST API has a limit of 3,700 alert conditions, including both active and disabled conditions. The API, whether used directly or via the UI, will reject all requests to add any additional alert conditions beyond the 3,700 alert condition limit. To add an infrastructure alert condition to an alerts policy: Go to one.newrelic.com > Infrastructure, then select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Type a meaningful condition name. Select the Alert type, or refer to the examples to decide which type to select. Create individual filters, or copy all the filters from a filter set to identify the hosts that you want the alert condition to use. Important For more information about the rules behind filters, see Filter set logic. Define the Critical (required) and Warning (optional, if available) thresholds for triggering the alert notification. Optional: To create the condition criteria proactively but not receive alert notifications at this time, turn off the Enabled checkbox option. Select an existing policy for the new condition. OR Select the option to create a new policy and identify the email for alert notifications. Optional: Add a runbook url. Optional: Set Violation time limit for violations (this defaults to 24 hours). Select Create. Important If New Relic hasn't received a cloud integration service's attribute in the past 60 minutes, we refer to this as a \"silent attribute,\" and it won't be available to use as an alert condition in the UI. In this situation, you can use the API to create alert conditions for silent attributes. Other infrastructure alert condition methods You can also use these other methods to create an infrastructure alert condition: Use the Alerts UI Go to one.newrelic.com > Alerts & AI > Alerts > Alert policies > New alert policy > Create new condition, then select Infrastructure as the product. Use the Infrastructure UI Go to one.newrelic.com > Infrastructure. Select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Use infrastructure settings for integrations Tip Use this method to create an alert condition for infrastructure integrations. Go to one.newrelic.com > Infrastructure > Settings > Alerts, and then click Create alert condition. Name and describe the alert condition. Click the Integrations alert type, and then select the integration data source you'd like to use. Use the Filter entities dropdown to limit your condition to specific attributes. Use the Define thresholds dropdowns to define your condition's thresholds, and then click Create. The configuration settings are optional. You can always update them later. View host alert events Anyone included in the policy's notification channels receive alert notifications directly. In addition, anyone with permissions for your New Relic account can view Infrastructure alert incidents and individual violations through the user interface. Go to one.newrelic.com > Infrastructure > Events. To change the hosts or time frame, use the search window, Filter set, or Time functions. From the Events list, select the alert violation. To view detailed information in Alerts about the selected violation, select the link. Update or delete host alert information To edit, disable (or re-enable), or delete host alert information: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Use the search window or Select all checkbox to locate one or more alert conditions. Select any of the available functions to edit, disable, enable, or delete the selected conditions. Use New Relic Alerts to monitor your entire infrastructure New Relic Alerts provides a single, coordinated alerting tool across all of your New Relic products. This allows you to manage alert policies and conditions that focus on the metrics for entities that you care about the most, such as Docker containers, JVMs, and more. Alert features Features in Infrastructure Alert conditions Create: Use the Infrastructure UI. View, change, disable (or re-enable), or delete: Use the Infrastructure Settings > Alerts UI. Information on alerts View summary information about events: Use the Infrastructure Events UI. View detailed information about alert incidents or individual violations: Use the Alerts UI or the notification channel integrated with the associated policy. Alert policies View, add, change, disable, or delete: For policies with a variety of notification channels: Use the Alerts UI. For policies only needing email notifications: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Create a new policy, and add one or more email addresses as needed. Add host conditions to an existing policy: Use the Infrastructure UI. Notification channels To view, add, change, or delete available notification options: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Search for the condition or policy name. From the list of conditions, select the policy link to view notification channel information in the Alerts UI. Add a description The use of the Description field is available for these alert condition types: NRQL conditions: add a description using the NerdGraph API. Infrastructure conditions: add a description using the UI or the REST API. The text you place in an alert condition's Description field is passed downstream to associated violations and notifications. A description can be used for several purposes, including: Capturing the reason for the alert condition. Defining the signal being monitored. Defining next steps. Add metadata to downstream systems. You can use template substitution to insert values from the attributes in the associated violation event. The template format is {{attributeName}}. For the attributes you can use when creating a description, see Violation event attributes. One available attribute is the special {{tag.*}} attribute. This attribute prefix is used to access any of the tag values that are included with the target signal, or any of the entity tags that are associated with the target signal. If there are entity tags associated with your violation, then they can be accessed using the entity tag name. An example of this would be {{tag.aws.awsRegion}}. When entity tags are available to use, you see them included with the violation, and displayed when you view the violations in an incident. This field has a maximum character size of 4,000. Add or edit a runbook URL The alert condition creation process includes an option for setting a URL for runbook instructions. This lets you link to information or standard procedures for handling a violation. Before adding or updating the link, make sure you use a valid URL. To add, update, or delete an alert condition's runbook URL: Select an alert condition, and make changes to the Runbook URL link. Save the condition. In order to be saved, the URL must be a valid URL. Violation time limit for violations The violation time limit allows you to define a time period after which violations will be force-closed. By default, violation time limit is 24 hours. To add or update an alert condition's violation time limit: Select an alert condition, and make changes to the violation time limit. Save the condition. Alert conditions that generate too-long NRQL queries Alert conditions created for infrastructure rely on behind-the-scenes NRQL queries, and NRQL queries have a 4096-character limit. This means that if your condition generates a very complex NRQL query that filters on many elements (for example, including many hosts or many tags), it will exceed this limit and display an error message saying that the condition failed. To solve this problem, reduce the number of elements you are using in your alert condition. For example: Problem Solution Hosts If you entered a large number of hosts that caused the condition to fail, reduce the number of hosts. Use substrings to target hosts. For example, instead of targeting prod-host-01, prod-host-02, and prod-host-03, just target all hosts with prod-host-0 in the name. Entities Edit your alert condition to target specific attributes that apply to the entities you're trying to target. Create custom attributes for the entities you want to target, and use those attributes in your alert condition. For more information, see Best practices for filtering in infrastructure alerts in New Relic's Explorers Hub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.49585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> for <em>infrastructure</em>: Add, edit, or view host <em>alert</em> information",
        "sections": "Create <em>alert</em> <em>conditions</em> for <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "With New Relic&#x27;s <em>infrastructure</em> monitoring, you can create <em>alert</em> <em>conditions</em> directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic <em>Alerts</em>"
      },
      "id": "6043fa3428ccbc401d2c60b9"
    },
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2022-01-08T08:23:04Z",
      "updated_at": "2021-07-27T14:15:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. The no_trigger_on field is optional. When set to [\"shutdown\"] this enables the Don't trigger alerts for hosts that perform a clean shutdown infrastructure condition option. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12, \"no_trigger_on\": [\"shutdown\"] } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. For new conditions, if a value is not provided, the following default values are used: All conditions: 24 hours When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.12086,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerts-add-edit-or-view-host-alert-information": [
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Caution",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2022-01-08T12:32:05Z",
      "updated_at": "2021-09-20T19:26:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. Caution If you have filtered your Host Not Reporting condition using tags or labels and then remove a critical tag or label from a targeted host, the system will open a Host Not Reporting violation, since it will characterize that host as having lost its connection. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Alternatively, you can add the hostStatus: shutdown tag to your host along with checking the option mentioned above. This will stop all Host Not Reporting violations from opening for that host, as long as that tag is on it, regardless of agent version or OS. Removing the tag will allow the system to open Host Not Reporting violations for that host again. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.05692,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    },
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2022-01-08T08:23:04Z",
      "updated_at": "2021-07-27T14:15:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. The no_trigger_on field is optional. When set to [\"shutdown\"] this enables the Don't trigger alerts for hosts that perform a clean shutdown infrastructure condition option. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12, \"no_trigger_on\": [\"shutdown\"] } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. For new conditions, if a value is not provided, the following default values are used: All conditions: 24 hours When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.12086,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    },
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2022-01-08T08:21:21Z",
      "updated_at": "2021-07-27T13:58:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process is running too many instances on one host This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.12016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process is running too many instances"
      },
      "id": "603eb49128ccbca939eba74a"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts": [
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Caution",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2022-01-08T12:32:05Z",
      "updated_at": "2021-09-20T19:26:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. Caution If you have filtered your Host Not Reporting condition using tags or labels and then remove a critical tag or label from a targeted host, the system will open a Host Not Reporting violation, since it will characterize that host as having lost its connection. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Alternatively, you can add the hostStatus: shutdown tag to your host along with checking the option mentioned above. This will stop all Host Not Reporting violations from opening for that host, as long as that tag is on it, regardless of agent version or OS. Removing the tag will allow the system to open Host Not Reporting violations for that host again. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.05692,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    },
    {
      "sections": [
        "Alerts for infrastructure: Add, edit, or view host alert information",
        "Create alert conditions for infrastructure",
        "Important",
        "Other infrastructure alert condition methods",
        "Use the Alerts UI",
        "Use the Infrastructure UI",
        "Use infrastructure settings for integrations",
        "Tip",
        "View host alert events",
        "Update or delete host alert information",
        "Use New Relic Alerts to monitor your entire infrastructure",
        "Add a description",
        "Add or edit a runbook URL",
        "Violation time limit for violations",
        "Alert conditions that generate too-long NRQL queries"
      ],
      "title": "Alerts for infrastructure: Add, edit, or view host alert information",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "00207a1020aa29ea6d5d5bbb8e806a50a5966f80",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerts-add-edit-or-view-host-alert-information/",
      "published_at": "2022-01-08T08:22:32Z",
      "updated_at": "2021-08-02T12:47:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring, you can create alert conditions directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic Alerts. Instead, you can immediately select your filter set and tailor the alert condition directly from the chart you are viewing. This helps you proactively manage and monitor the alerting system for your environment. Any alert violations will be created per entity within the filter set. Create alert conditions for infrastructure Alert conditions apply to alert policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to use other types of notification channels, create a new policy from within the Alerts UI. Important The Infrastructure REST API has a limit of 3,700 alert conditions, including both active and disabled conditions. The API, whether used directly or via the UI, will reject all requests to add any additional alert conditions beyond the 3,700 alert condition limit. To add an infrastructure alert condition to an alerts policy: Go to one.newrelic.com > Infrastructure, then select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Type a meaningful condition name. Select the Alert type, or refer to the examples to decide which type to select. Create individual filters, or copy all the filters from a filter set to identify the hosts that you want the alert condition to use. Important For more information about the rules behind filters, see Filter set logic. Define the Critical (required) and Warning (optional, if available) thresholds for triggering the alert notification. Optional: To create the condition criteria proactively but not receive alert notifications at this time, turn off the Enabled checkbox option. Select an existing policy for the new condition. OR Select the option to create a new policy and identify the email for alert notifications. Optional: Add a runbook url. Optional: Set Violation time limit for violations (this defaults to 24 hours). Select Create. Important If New Relic hasn't received a cloud integration service's attribute in the past 60 minutes, we refer to this as a \"silent attribute,\" and it won't be available to use as an alert condition in the UI. In this situation, you can use the API to create alert conditions for silent attributes. Other infrastructure alert condition methods You can also use these other methods to create an infrastructure alert condition: Use the Alerts UI Go to one.newrelic.com > Alerts & AI > Alerts > Alert policies > New alert policy > Create new condition, then select Infrastructure as the product. Use the Infrastructure UI Go to one.newrelic.com > Infrastructure. Select any of these Infrastructure monitoring pages: Hosts, Processes, Network, or Storage. Mouse over the chart you want to alert on, select the ellipses icon, and then select Create alert. Use infrastructure settings for integrations Tip Use this method to create an alert condition for infrastructure integrations. Go to one.newrelic.com > Infrastructure > Settings > Alerts, and then click Create alert condition. Name and describe the alert condition. Click the Integrations alert type, and then select the integration data source you'd like to use. Use the Filter entities dropdown to limit your condition to specific attributes. Use the Define thresholds dropdowns to define your condition's thresholds, and then click Create. The configuration settings are optional. You can always update them later. View host alert events Anyone included in the policy's notification channels receive alert notifications directly. In addition, anyone with permissions for your New Relic account can view Infrastructure alert incidents and individual violations through the user interface. Go to one.newrelic.com > Infrastructure > Events. To change the hosts or time frame, use the search window, Filter set, or Time functions. From the Events list, select the alert violation. To view detailed information in Alerts about the selected violation, select the link. Update or delete host alert information To edit, disable (or re-enable), or delete host alert information: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Use the search window or Select all checkbox to locate one or more alert conditions. Select any of the available functions to edit, disable, enable, or delete the selected conditions. Use New Relic Alerts to monitor your entire infrastructure New Relic Alerts provides a single, coordinated alerting tool across all of your New Relic products. This allows you to manage alert policies and conditions that focus on the metrics for entities that you care about the most, such as Docker containers, JVMs, and more. Alert features Features in Infrastructure Alert conditions Create: Use the Infrastructure UI. View, change, disable (or re-enable), or delete: Use the Infrastructure Settings > Alerts UI. Information on alerts View summary information about events: Use the Infrastructure Events UI. View detailed information about alert incidents or individual violations: Use the Alerts UI or the notification channel integrated with the associated policy. Alert policies View, add, change, disable, or delete: For policies with a variety of notification channels: Use the Alerts UI. For policies only needing email notifications: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Create a new policy, and add one or more email addresses as needed. Add host conditions to an existing policy: Use the Infrastructure UI. Notification channels To view, add, change, or delete available notification options: Go to one.newrelic.com > Infrastructure > Settings > Alerts. Optional: Search for the condition or policy name. From the list of conditions, select the policy link to view notification channel information in the Alerts UI. Add a description The use of the Description field is available for these alert condition types: NRQL conditions: add a description using the NerdGraph API. Infrastructure conditions: add a description using the UI or the REST API. The text you place in an alert condition's Description field is passed downstream to associated violations and notifications. A description can be used for several purposes, including: Capturing the reason for the alert condition. Defining the signal being monitored. Defining next steps. Add metadata to downstream systems. You can use template substitution to insert values from the attributes in the associated violation event. The template format is {{attributeName}}. For the attributes you can use when creating a description, see Violation event attributes. One available attribute is the special {{tag.*}} attribute. This attribute prefix is used to access any of the tag values that are included with the target signal, or any of the entity tags that are associated with the target signal. If there are entity tags associated with your violation, then they can be accessed using the entity tag name. An example of this would be {{tag.aws.awsRegion}}. When entity tags are available to use, you see them included with the violation, and displayed when you view the violations in an incident. This field has a maximum character size of 4,000. Add or edit a runbook URL The alert condition creation process includes an option for setting a URL for runbook instructions. This lets you link to information or standard procedures for handling a violation. Before adding or updating the link, make sure you use a valid URL. To add, update, or delete an alert condition's runbook URL: Select an alert condition, and make changes to the Runbook URL link. Save the condition. In order to be saved, the URL must be a valid URL. Violation time limit for violations The violation time limit allows you to define a time period after which violations will be force-closed. By default, violation time limit is 24 hours. To add or update an alert condition's violation time limit: Select an alert condition, and make changes to the violation time limit. Save the condition. Alert conditions that generate too-long NRQL queries Alert conditions created for infrastructure rely on behind-the-scenes NRQL queries, and NRQL queries have a 4096-character limit. This means that if your condition generates a very complex NRQL query that filters on many elements (for example, including many hosts or many tags), it will exceed this limit and display an error message saying that the condition failed. To solve this problem, reduce the number of elements you are using in your alert condition. For example: Problem Solution Hosts If you entered a large number of hosts that caused the condition to fail, reduce the number of hosts. Use substrings to target hosts. For example, instead of targeting prod-host-01, prod-host-02, and prod-host-03, just target all hosts with prod-host-0 in the name. Entities Edit your alert condition to target specific attributes that apply to the entities you're trying to target. Create custom attributes for the entities you want to target, and use those attributes in your alert condition. For more information, see Best practices for filtering in infrastructure alerts in New Relic's Explorers Hub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.49585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> for <em>infrastructure</em>: Add, edit, or view host <em>alert</em> information",
        "sections": "Create <em>alert</em> <em>conditions</em> for <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "With New Relic&#x27;s <em>infrastructure</em> monitoring, you can create <em>alert</em> <em>conditions</em> directly within the context of what you are currently monitoring with New Relic. For example, if you are monitoring a filter set and notice a problem, you do not need to recreate those criteria from New Relic <em>Alerts</em>"
      },
      "id": "6043fa3428ccbc401d2c60b9"
    },
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2022-01-08T08:21:21Z",
      "updated_at": "2021-07-27T13:58:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process is running too many instances on one host This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.12016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process is running too many instances"
      },
      "id": "603eb49128ccbca939eba74a"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/agent-not-starting-there-are-no-logs": [
    {
      "sections": [
        "Infrastructure agent logging behavior",
        "Logging severity levels",
        "Important",
        "Log formatting",
        "Log rotation",
        "Logrotate config file sample",
        "Tip",
        "Smart verbose mode",
        "Logging before Infrastructure agent v1.4.9",
        "Integration log management",
        "Integration STDERR expected format"
      ],
      "title": "Infrastructure agent logging behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "0dc6570e893e47c4d5b5c4232283432926c6476a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/infrastructure-agent-logging-behavior/",
      "published_at": "2022-01-08T08:52:19Z",
      "updated_at": "2021-11-13T08:23:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure agent gathers its own data as well as integrations's logs and consolidates them in a single source. By default, logs appear in standard-output and are added to a log file. To disable logs in standard output, see the agent's config options. Logging severity levels Infrastructure uses a subset of the standard Syslog severity levels: ERROR: Error conditions met WARN: Warning conditions met INFO: Informational messages DEBUG: Contains debug-level messages (useful when troubleshooting) Important DEBUG level is only shown when the verbose mode is enabled. Log formatting For infrastructure agent v1.4.9 or higher, log messages are inlined with context values. This offers better grouping and filtering; for example: containerized agent found in container containerID: VALUE Copy By default, Infrastructure logs are formatted as text: In foreground mode, log output is colored, without a timestamp: DEBUG Sending deltas divided in blocks component=PatchSender mentityKey=ohaimaci mnumberOfBlocks=1 Copy In background mode, logs are timestamped output, used when running as a service or dumping logs to a file: time=\"2019-07-12T09:54:15+02:00\" level=info msg=\"Agent service manager shutdown completed successfully.\" component=AgentService service=newrelic-infra Copy Alternatively, logs can be formatted as a JSON file: {\"context\":{},\"level\":\"info\",\"msg\":\"upstart_interval_sec: 0\",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} {\"context\":{},\"level\":\"info\",\"msg\":\"plugin_dir: \",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} Copy To change the log format, see the agent configuration settings. Log rotation The infrastructure agent does not provide any native log rotation or compression mechanism. Instead, we encourage you to use consolidated log rotation tools, such as the Linux logrotate tool, which is usually installed by default in most Linux distributions. Logrotate can be configured as an entry in /etc/logrotate.conf, or as a file in the /etc/logrotate.d directory. Logrotate config file sample A sample logrotate config file looks like this: /var/log/newrelic-infra/newrelic-infra.log { copytruncate compress daily dateext maxage 7 } Copy Where: /var/log/newrelic-infra/newrelic-infra.log: The Infrastructure agent log file. It must match the log_file configuration parameter in the /etc/newrelic-infra.yml file. copytruncate: Indicates that the log file is truncated but not deleted when it is rotated. This configuration option is mandatory, otherwise the log file will be deleted and won’t be recreated. compress: Compresses (usually in Gzip format) the rotated log files. daily: The agent rotates logs daily. dateext: Appends a date (by default, in the format YYYYMMDD) to the rotated log file (for example, newrelic-infra.log-20190708.gz). maxage 7: Makes logrotate remove rotated files after 7 days. Tip For a complete description of the logrotate configuration options, see the Linux Logrotate documentation. Since logrotate is usually executed automatically as a cron job, verify that there is a logrotate entry in cron (for example, /etc/cron.daily/logrotate) similar to: #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 Copy Smart verbose mode For infrastructure agent versions 1.9.0 or higher, you can enable smart verbose mode for logs. Smart verbose mode prevents debug messages being logged until an error message is detected. Once an error is detected, debug messages are written to the log file before the error. Note that only the most recent number of configured debug messages are logged. For example, if you have a configured limit of 10, when an error is detected, the 10 most recent debug messages before the error was detected are logged. For more information on how to enable smart verbose mode and the debug message limit, see Infrastructure configuration settings. Logging before Infrastructure agent v1.4.9 Here is a comparison of functionality for Infrastructure agent versions before and after v1.4.9: Agent v1.4.9 and higher Before v1.4.9 Foreground mode logged. The agent couldn't log some entries in foreground mode because the logging service wasn't able to write data until the agent was completely configured. Logs in text and JSON formats. Logs in text only. Logs displayed as inline text. Logs displayed as static literals in a single, decontextualized line. Integration log management Integrations write JSON payloads into STDOUT and plain-text (JSON structured in the future) logs into STDERR. The infrastructure agent handles integration STDERR lines and forward this output into the agent one, usually the service log. Agent handles each STDERR line as follows: when agent runs in verbose mode: it just forwards the full STDERR line as a DEBUG agent log entry placing integration line contexts within the ` msg ` field. otherwise: it parses the line against the expected format (see below) and only logs as agent ERROR level, entries produced by integrations with ` fatal ` or ` error ` severity levels. In this case fields are extracted and forwarded in structured manner (therefore if JSON output is enabled for the agent fields become queryable. Integration STDERR expected format A line is expected to be a list of key-value pairs separated by an equal character. Keys can contain any character, whereas values can have three different formats: string: < quote>any character including escaped quotes \\ \" < quote> map: & { any character} word: any character except spaces Internally agent used this regex to extract the fields: ([^\\s]*?)=(\".*?[^\\\\]\"|&{.*?}|[^\\s]*) Copy For instance, this line: time=\"2015-03-26T01:27:38-04:00\" level=error msg=\"Foo bar baz\" foo=bar Copy Will generate a structured agent log line with these fields: - \"time\": \"2015-03-26T01:27:38-04:00\" - \"level\": \"error\" - \"msg\": \"Foo bar baz\" - \"foo\": \"bar\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.44495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "sections": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "New Relic&#x27;s <em>infrastructure</em> agent gathers its own data as well as integrations&#x27;s <em>logs</em> and consolidates them in a single source. By default, <em>logs</em> appear in standard-output and are added to a <em>log</em> file. To disable <em>logs</em> in standard output, see the agent&#x27;s config options. Logging severity levels"
      },
      "id": "603eb3a228ccbc6badeba7a5"
    },
    {
      "sections": [
        "Incorrect data reported",
        "Problem",
        "Troubleshooting",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows"
      ],
      "title": "Incorrect data reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "84b37d403b5c2b8c8c9d8d9220254d77852c49ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-12-30T06:51:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows unexpected data for some of the events, metrics or attributes collected from the infrastructure agent. Troubleshooting Infrastructure supports trace-level logging that can be enabled on-demand to help troubleshooting complex scenarios. The following trace flags can be configured in order to print all events and metrics send to Telemetry Data Platform. This setting generates a lot of data very quickly, we recommend only enabling it for troubleshooting purposes. Edit the newrelic-infra.yml configuration file and add required flags. For example: verbose: 1 log_file: /path/myfile.log trace: # v3.submission enables detailed logging for events, examples: SystemSample, NetworkSample, etc. - v3.submission # dm.submission - dm.submission Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10, or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Identify the new trace log lines to confirm the data being sent to the Telemetry Data Platform. Log example when v3.submission is enabled: time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Sending events to metrics-ingest.\" component=MetricsIngestSender key=... numEvents=3 postCount=1 timestamps=\"[2021-01-01 09:27:28 +0000 UTC]\" time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Preparing metrics post.\" component=MetricsIngestSender postCount=1 time=\"2021-12-28T09:27:28Z\" level=trace msg=\"[{\\\"EntityID\\\":111,\\\"IsAgent\\\":true,\\\"Events\\\":[{\\\"eventType\\\":\\\"SystemSample\\\",\\\"timestamp\\\":1640683648,\\\"entityKey\\\":\\\"...\\\",\\\"cpuPercent\\\":0.2004008016032026, ...}]\" feature=v3.submission time=\"2021-12-28T09:27:29Z\" level=debug msg=\"Metrics post succeeded.\" component=MetricsIngestSender postCount=1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.29501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows unexpected data for some of the events, metrics or attributes collected from the <em>infrastructure</em> agent. <em>Troubleshooting</em> <em>Infrastructure</em> supports trace-level logging that can be enabled on-demand to help <em>troubleshooting</em> complex"
      },
      "id": "61cd56e3e7b9d25b3e7f54c3"
    },
    {
      "sections": [
        "Generate logs for troubleshooting the infrastructure agent",
        "Problem",
        "Important",
        "Solution",
        "Smart verbose mode",
        "Forward the agent logs to New Relic Logs",
        "Notes for specific systems",
        "Containerized agent on CoreOS"
      ],
      "title": "Generate logs for troubleshooting the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "a0c2ca22e3fca2b3add8c94d211adffce686661c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/generate-logs-troubleshooting-infrastructure/",
      "published_at": "2022-01-08T08:52:19Z",
      "updated_at": "2021-03-16T06:35:54Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting your infrastructure agent, generate verbose logs for a few minutes to find and investigate errors. This can be useful for your own troubleshooting or when working with New Relic Support. Important Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. If you have New Relic infrastructure agent 1.4.0 or higher, you can automate this process by using the newrelic-infra-ctl command. For more information, see the troubleshooting binary documentation. Solution Generating verbose log files requires editing your configuration file. For a sample config file that includes all applicable settings, see the example template. To generate detailed logs: Step Procedures Edit your newrelic-infra.yml file: Enable verbose logging: verbose: 1. (If you use a containerized infrastructure agent on CoreOS, see system-specific notes.) Set log_file to a convenient log file location. Restart the agent so the agent notices the new settings. Let your host run at normal load for about three minutes to generate sufficient logging data. Return your settings to default: Disable verbose logging by setting verbose: 0 in newrelic-infra.yml. Optional: Disable logging to a custom file by removing the log_file line from newrelic-infra.yml. Restart the agent so the agent notices the new settings. Examine the log file for errors. If you need to send your log file to New Relic Support: Include the line in your log file that contains the agent version: New Relic infrastructure agent version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your newrelic-infra.yml. Smart verbose mode Sometimes errors don't occur until after quite some time has passed. This makes debugging difficult, because typically verbose logs are only enabled for a short period time; otherwise there will be many debug logs. For example, if an error occurs one hour after the infrastructure agent has started, getting debug logs around the time of the error can be tricky or impractical. As of infrastructure agent v1.9.0 or higher, you can use smart verbose mode for logs. Smart verbose mode only logs the most recent debug messages after an error has been logged. This allows you to leave smart verbose mode running until an error occurs, without logging lots of irrelevant debug messages, and only logging the most recent debug messages. (The number of messages is determined by your configuration.) For more information on smart verbose mode, see the Infrastructure agent logging behavior docs, and use the Infrastructure configuration settings documentation for details on how to enable smart verbose mode. Forward the agent logs to New Relic Logs The Infrastructure agent can be configured to send its own logs to New Relic Logs. This can be useful for troubleshooting issues with log forwarding, the Infrastructure agent, or when contacting support. For details on how to enable log forwarding for the Infrastructure agent, see Troubleshoot log forwarding. Notes for specific systems These are some additional notes and requirements for specific systems, used to supplement the general logging instructions: Containerized agent on CoreOS If you are using a containerized infrastructure agent on CoreOS: Choose one of these options to change the log level to verbose: Recommended: Set the environment variable NRIA_VERBOSE to 1. Running this on the command line would look like: -e NRIA_VERBOSE=1 Copy OR Edit the config file to set verbose: 1. (Editing the config file in a container is not recommended, because it requires rebuilding the image twice: once to add verbose logging and once to remove it.) Use journalctl to collect the logs: journalctl -u newrelic-infra > newrelic-infra.log Copy Set the verbose logging level back to 0 after collecting logs for a few minutes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.09708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "sections": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " verbose mode. Forward the agent <em>logs</em> to New Relic <em>Logs</em> The <em>Infrastructure</em> agent can be configured to send its own <em>logs</em> to New Relic <em>Logs</em>. This can be useful for <em>troubleshooting</em> issues with <em>log</em> forwarding, the <em>Infrastructure</em> agent, or when contacting support. For details on how to enable <em>log</em> forwarding"
      },
      "id": "603e910028ccbc6304eba76d"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/agent-not-starting-windows": [
    {
      "sections": [
        "Incorrect data reported",
        "Problem",
        "Troubleshooting",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows"
      ],
      "title": "Incorrect data reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "84b37d403b5c2b8c8c9d8d9220254d77852c49ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-12-30T06:51:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows unexpected data for some of the events, metrics or attributes collected from the infrastructure agent. Troubleshooting Infrastructure supports trace-level logging that can be enabled on-demand to help troubleshooting complex scenarios. The following trace flags can be configured in order to print all events and metrics send to Telemetry Data Platform. This setting generates a lot of data very quickly, we recommend only enabling it for troubleshooting purposes. Edit the newrelic-infra.yml configuration file and add required flags. For example: verbose: 1 log_file: /path/myfile.log trace: # v3.submission enables detailed logging for events, examples: SystemSample, NetworkSample, etc. - v3.submission # dm.submission - dm.submission Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10, or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Identify the new trace log lines to confirm the data being sent to the Telemetry Data Platform. Log example when v3.submission is enabled: time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Sending events to metrics-ingest.\" component=MetricsIngestSender key=... numEvents=3 postCount=1 timestamps=\"[2021-01-01 09:27:28 +0000 UTC]\" time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Preparing metrics post.\" component=MetricsIngestSender postCount=1 time=\"2021-12-28T09:27:28Z\" level=trace msg=\"[{\\\"EntityID\\\":111,\\\"IsAgent\\\":true,\\\"Events\\\":[{\\\"eventType\\\":\\\"SystemSample\\\",\\\"timestamp\\\":1640683648,\\\"entityKey\\\":\\\"...\\\",\\\"cpuPercent\\\":0.2004008016032026, ...}]\" feature=v3.submission time=\"2021-12-28T09:27:29Z\" level=debug msg=\"Metrics post succeeded.\" component=MetricsIngestSender postCount=1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.6075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows unexpected data for some of the events, metrics or attributes collected from the <em>infrastructure</em> agent. <em>Troubleshooting</em> <em>Infrastructure</em> supports trace-level logging that can be enabled on-demand to help <em>troubleshooting</em> complex"
      },
      "id": "61cd56e3e7b9d25b3e7f54c3"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-08T13:01:10Z",
      "updated_at": "2021-11-24T17:52:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.21191,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2022-01-08T13:06:49Z",
      "updated_at": "2021-11-13T18:53:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause In Linux and macOS, the New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent. In Windows, it resolves the domain name using internal tools.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure": [
    {
      "sections": [
        "Incorrect data reported",
        "Problem",
        "Troubleshooting",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows"
      ],
      "title": "Incorrect data reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "84b37d403b5c2b8c8c9d8d9220254d77852c49ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-12-30T06:51:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows unexpected data for some of the events, metrics or attributes collected from the infrastructure agent. Troubleshooting Infrastructure supports trace-level logging that can be enabled on-demand to help troubleshooting complex scenarios. The following trace flags can be configured in order to print all events and metrics send to Telemetry Data Platform. This setting generates a lot of data very quickly, we recommend only enabling it for troubleshooting purposes. Edit the newrelic-infra.yml configuration file and add required flags. For example: verbose: 1 log_file: /path/myfile.log trace: # v3.submission enables detailed logging for events, examples: SystemSample, NetworkSample, etc. - v3.submission # dm.submission - dm.submission Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10, or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Identify the new trace log lines to confirm the data being sent to the Telemetry Data Platform. Log example when v3.submission is enabled: time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Sending events to metrics-ingest.\" component=MetricsIngestSender key=... numEvents=3 postCount=1 timestamps=\"[2021-01-01 09:27:28 +0000 UTC]\" time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Preparing metrics post.\" component=MetricsIngestSender postCount=1 time=\"2021-12-28T09:27:28Z\" level=trace msg=\"[{\\\"EntityID\\\":111,\\\"IsAgent\\\":true,\\\"Events\\\":[{\\\"eventType\\\":\\\"SystemSample\\\",\\\"timestamp\\\":1640683648,\\\"entityKey\\\":\\\"...\\\",\\\"cpuPercent\\\":0.2004008016032026, ...}]\" feature=v3.submission time=\"2021-12-28T09:27:29Z\" level=debug msg=\"Metrics post succeeded.\" component=MetricsIngestSender postCount=1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.6075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows unexpected data for some of the events, metrics or attributes collected from the <em>infrastructure</em> agent. <em>Troubleshooting</em> <em>Infrastructure</em> supports trace-level logging that can be enabled on-demand to help <em>troubleshooting</em> complex"
      },
      "id": "61cd56e3e7b9d25b3e7f54c3"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-08T13:01:10Z",
      "updated_at": "2021-11-24T17:52:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.21191,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2022-01-08T13:06:49Z",
      "updated_at": "2021-11-13T18:53:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause In Linux and macOS, the New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent. In Windows, it resolves the domain name using internal tools.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/https-proxy-configuration-missing": [
    {
      "sections": [
        "Incorrect data reported",
        "Problem",
        "Troubleshooting",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows"
      ],
      "title": "Incorrect data reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "84b37d403b5c2b8c8c9d8d9220254d77852c49ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-12-30T06:51:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows unexpected data for some of the events, metrics or attributes collected from the infrastructure agent. Troubleshooting Infrastructure supports trace-level logging that can be enabled on-demand to help troubleshooting complex scenarios. The following trace flags can be configured in order to print all events and metrics send to Telemetry Data Platform. This setting generates a lot of data very quickly, we recommend only enabling it for troubleshooting purposes. Edit the newrelic-infra.yml configuration file and add required flags. For example: verbose: 1 log_file: /path/myfile.log trace: # v3.submission enables detailed logging for events, examples: SystemSample, NetworkSample, etc. - v3.submission # dm.submission - dm.submission Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10, or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Identify the new trace log lines to confirm the data being sent to the Telemetry Data Platform. Log example when v3.submission is enabled: time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Sending events to metrics-ingest.\" component=MetricsIngestSender key=... numEvents=3 postCount=1 timestamps=\"[2021-01-01 09:27:28 +0000 UTC]\" time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Preparing metrics post.\" component=MetricsIngestSender postCount=1 time=\"2021-12-28T09:27:28Z\" level=trace msg=\"[{\\\"EntityID\\\":111,\\\"IsAgent\\\":true,\\\"Events\\\":[{\\\"eventType\\\":\\\"SystemSample\\\",\\\"timestamp\\\":1640683648,\\\"entityKey\\\":\\\"...\\\",\\\"cpuPercent\\\":0.2004008016032026, ...}]\" feature=v3.submission time=\"2021-12-28T09:27:29Z\" level=debug msg=\"Metrics post succeeded.\" component=MetricsIngestSender postCount=1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.6075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows unexpected data for some of the events, metrics or attributes collected from the <em>infrastructure</em> agent. <em>Troubleshooting</em> <em>Infrastructure</em> supports trace-level logging that can be enabled on-demand to help <em>troubleshooting</em> complex"
      },
      "id": "61cd56e3e7b9d25b3e7f54c3"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-08T13:01:10Z",
      "updated_at": "2021-11-24T17:52:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.21191,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2022-01-08T13:06:49Z",
      "updated_at": "2021-11-13T18:53:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause In Linux and macOS, the New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent. In Windows, it resolves the domain name using internal tools.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported": [
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-08T13:01:10Z",
      "updated_at": "2021-11-24T17:52:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.21191,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2022-01-08T13:06:49Z",
      "updated_at": "2021-11-13T18:53:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause In Linux and macOS, the New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent. In Windows, it resolves the domain name using internal tools.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-09-14T07:22:29Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in infrastructure, and vice versa. If you do not see this APM-infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see APM data in infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.69029,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>infrastructure</em>, and vice versa. If you do not see this APM-<em>infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported": [
    {
      "sections": [
        "Incorrect data reported",
        "Problem",
        "Troubleshooting",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows"
      ],
      "title": "Incorrect data reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "84b37d403b5c2b8c8c9d8d9220254d77852c49ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-12-30T06:51:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows unexpected data for some of the events, metrics or attributes collected from the infrastructure agent. Troubleshooting Infrastructure supports trace-level logging that can be enabled on-demand to help troubleshooting complex scenarios. The following trace flags can be configured in order to print all events and metrics send to Telemetry Data Platform. This setting generates a lot of data very quickly, we recommend only enabling it for troubleshooting purposes. Edit the newrelic-infra.yml configuration file and add required flags. For example: verbose: 1 log_file: /path/myfile.log trace: # v3.submission enables detailed logging for events, examples: SystemSample, NetworkSample, etc. - v3.submission # dm.submission - dm.submission Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10, or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Identify the new trace log lines to confirm the data being sent to the Telemetry Data Platform. Log example when v3.submission is enabled: time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Sending events to metrics-ingest.\" component=MetricsIngestSender key=... numEvents=3 postCount=1 timestamps=\"[2021-01-01 09:27:28 +0000 UTC]\" time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Preparing metrics post.\" component=MetricsIngestSender postCount=1 time=\"2021-12-28T09:27:28Z\" level=trace msg=\"[{\\\"EntityID\\\":111,\\\"IsAgent\\\":true,\\\"Events\\\":[{\\\"eventType\\\":\\\"SystemSample\\\",\\\"timestamp\\\":1640683648,\\\"entityKey\\\":\\\"...\\\",\\\"cpuPercent\\\":0.2004008016032026, ...}]\" feature=v3.submission time=\"2021-12-28T09:27:29Z\" level=debug msg=\"Metrics post succeeded.\" component=MetricsIngestSender postCount=1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.60744,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows unexpected data for some of the events, metrics or attributes collected from the <em>infrastructure</em> agent. <em>Troubleshooting</em> <em>Infrastructure</em> supports trace-level logging that can be enabled on-demand to help <em>troubleshooting</em> complex"
      },
      "id": "61cd56e3e7b9d25b3e7f54c3"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-08T13:01:10Z",
      "updated_at": "2021-11-24T17:52:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.21191,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-09-14T07:22:29Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in infrastructure, and vice versa. If you do not see this APM-infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see APM data in infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.69029,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>infrastructure</em>, and vice versa. If you do not see this APM-<em>infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint": [
    {
      "sections": [
        "Incorrect data reported",
        "Problem",
        "Troubleshooting",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows"
      ],
      "title": "Incorrect data reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "84b37d403b5c2b8c8c9d8d9220254d77852c49ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-12-30T06:51:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows unexpected data for some of the events, metrics or attributes collected from the infrastructure agent. Troubleshooting Infrastructure supports trace-level logging that can be enabled on-demand to help troubleshooting complex scenarios. The following trace flags can be configured in order to print all events and metrics send to Telemetry Data Platform. This setting generates a lot of data very quickly, we recommend only enabling it for troubleshooting purposes. Edit the newrelic-infra.yml configuration file and add required flags. For example: verbose: 1 log_file: /path/myfile.log trace: # v3.submission enables detailed logging for events, examples: SystemSample, NetworkSample, etc. - v3.submission # dm.submission - dm.submission Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10, or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Identify the new trace log lines to confirm the data being sent to the Telemetry Data Platform. Log example when v3.submission is enabled: time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Sending events to metrics-ingest.\" component=MetricsIngestSender key=... numEvents=3 postCount=1 timestamps=\"[2021-01-01 09:27:28 +0000 UTC]\" time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Preparing metrics post.\" component=MetricsIngestSender postCount=1 time=\"2021-12-28T09:27:28Z\" level=trace msg=\"[{\\\"EntityID\\\":111,\\\"IsAgent\\\":true,\\\"Events\\\":[{\\\"eventType\\\":\\\"SystemSample\\\",\\\"timestamp\\\":1640683648,\\\"entityKey\\\":\\\"...\\\",\\\"cpuPercent\\\":0.2004008016032026, ...}]\" feature=v3.submission time=\"2021-12-28T09:27:29Z\" level=debug msg=\"Metrics post succeeded.\" component=MetricsIngestSender postCount=1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.60744,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows unexpected data for some of the events, metrics or attributes collected from the <em>infrastructure</em> agent. <em>Troubleshooting</em> <em>Infrastructure</em> supports trace-level logging that can be enabled on-demand to help <em>troubleshooting</em> complex"
      },
      "id": "61cd56e3e7b9d25b3e7f54c3"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-08T13:01:10Z",
      "updated_at": "2021-11-24T17:52:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.21191,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2022-01-08T13:06:49Z",
      "updated_at": "2021-11-13T18:53:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause In Linux and macOS, the New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent. In Windows, it resolves the domain name using internal tools.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/time-gaps-missing-data": [
    {
      "sections": [
        "Incorrect data reported",
        "Problem",
        "Troubleshooting",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows"
      ],
      "title": "Incorrect data reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "84b37d403b5c2b8c8c9d8d9220254d77852c49ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-data-reported/",
      "published_at": "2022-01-08T13:06:09Z",
      "updated_at": "2021-12-30T06:51:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows unexpected data for some of the events, metrics or attributes collected from the infrastructure agent. Troubleshooting Infrastructure supports trace-level logging that can be enabled on-demand to help troubleshooting complex scenarios. The following trace flags can be configured in order to print all events and metrics send to Telemetry Data Platform. This setting generates a lot of data very quickly, we recommend only enabling it for troubleshooting purposes. Edit the newrelic-infra.yml configuration file and add required flags. For example: verbose: 1 log_file: /path/myfile.log trace: # v3.submission enables detailed logging for events, examples: SystemSample, NetworkSample, etc. - v3.submission # dm.submission - dm.submission Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10, or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Identify the new trace log lines to confirm the data being sent to the Telemetry Data Platform. Log example when v3.submission is enabled: time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Sending events to metrics-ingest.\" component=MetricsIngestSender key=... numEvents=3 postCount=1 timestamps=\"[2021-01-01 09:27:28 +0000 UTC]\" time=\"2021-12-28T09:27:28Z\" level=debug msg=\"Preparing metrics post.\" component=MetricsIngestSender postCount=1 time=\"2021-12-28T09:27:28Z\" level=trace msg=\"[{\\\"EntityID\\\":111,\\\"IsAgent\\\":true,\\\"Events\\\":[{\\\"eventType\\\":\\\"SystemSample\\\",\\\"timestamp\\\":1640683648,\\\"entityKey\\\":\\\"...\\\",\\\"cpuPercent\\\":0.2004008016032026, ...}]\" feature=v3.submission time=\"2021-12-28T09:27:29Z\" level=debug msg=\"Metrics post succeeded.\" component=MetricsIngestSender postCount=1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.60739,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows unexpected data for some of the events, metrics or attributes collected from the <em>infrastructure</em> agent. <em>Troubleshooting</em> <em>Infrastructure</em> supports trace-level logging that can be enabled on-demand to help <em>troubleshooting</em> complex"
      },
      "id": "61cd56e3e7b9d25b3e7f54c3"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-08T13:01:10Z",
      "updated_at": "2021-11-24T17:52:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.2119,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2022-01-08T13:06:49Z",
      "updated_at": "2021-11-13T18:53:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause In Linux and macOS, the New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent. In Windows, it resolves the domain name using internal tools.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic": [
    {
      "sections": [
        "Introduction to New Relic NerdGraph, our GraphQL API",
        "What is NerdGraph?",
        "Important",
        "Use the GraphiQL explorer",
        "Requirements and endpoints",
        "What can you do with NerdGraph?",
        "NerdGraph terminology",
        "Tips on using the GraphiQL explorer",
        "Query accounts a New Relic user can access",
        "Query user, account, and NRQL in one request"
      ],
      "title": "Introduction to New Relic NerdGraph, our GraphQL API",
      "type": "docs",
      "tags": [
        "APIs",
        "NerdGraph",
        "Get started"
      ],
      "external_id": "e8e96c16cd75f494ebfacb3bc53b4ee9ccf1c727",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph/",
      "published_at": "2022-01-08T03:28:33Z",
      "updated_at": "2022-01-08T03:28:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. After you sign up for a free New Relic account and install any of our monitoring services, you can get started with NerdGraph. What is NerdGraph? New Relic has several APIs. NerdGraph is the API we recommend for querying New Relic data and for performing some specific configurations (learn more about features). NerdGraph provides a single API interface for returning data from New Relic’s various APIs and microservices. Over time, other configuration capabilities will be added to NerdGraph. Important NerdGraph isn’t used for data ingest. For that, you'd use our data ingest APIs. NerdGraph is built using GraphQL, which is an open source API format that allows you to request exactly the data needed, with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic University’s Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To get started using GraphQL, we recommend playing around with our GraphiQL explorer (GraphiQL is an open source graphical interface for using GraphQL). You can use it to explore our data schema, to read built-in object definitions, and to build and execute queries. To use GraphQL, you’ll need a user-specific New Relic API key called a user key. You can generate one or find an existing one from the GraphiQL explorer’s API key dropdown. To find the GraphiQL explorer: If your New Relic account uses an EU data center, go to api.eu.newrelic.com/graphiql. Otherwise use api.newrelic.com/graphiql. For tips on how to build queries, see Build queries. Requirements and endpoints To use NerdGraph, you need a New Relic user key, which can be generated and accessed from the GraphiQL explorer. The endpoints are: Main endpoint: https://api.newrelic.com/graphql Endpoint for accounts using EU data center: https://api.eu.newrelic.com/graphql To access the endpoint, use the following cURL command: curl -X POST https://api.newrelic.com/graphql \\ -H 'Content-Type: application/json' \\ -H 'API-Key: YOUR_NEW_RELIC_USER_KEY' \\ -d '{ \"query\": \"{ requestContext { userId apiKey } }\" } ' Copy What can you do with NerdGraph? NerdGraph functionality can be broken down into two main categories: Querying New Relic data. You can fetch data for a variety of purposes, including using it in a programmatic workflow, or building a New Relic One app for custom data visualizations. Configuring New Relic features. There are a variety of configurations available and more will be added over time. You can do things like add tags, configure workloads, or customize \"golden metrics.\" You can use NerdGraph to return a wide range of New Relic data but we’ve created some tutorials for common use cases: Topic Tutorials Your monitored entities Get data about entities Understand entity relationships and dependencies (used to build service maps) Query and configure \"golden metrics\" (important entity metrics) Querying data Query using NRQL (our query language) Tags Add and manage tags APM agents APM agent configuration Dashboards Create dashboards Export dashboards to other accounts Export dashboards as files Migrate from Insights Dashboard API to NerdGraph Alerts See all alert-related tutorials Applied Intelligence View and configure topology Workloads View and configure workloads Service levels Configure and manage service levels Manage keys Create and manage keys (license keys used for data ingest, and user keys) Manage data Convert event data to metric data Drop data Distributed tracing Query distributed tracing data Configure Infinite Tracing New Relic One apps Build a New Relic One app Cloud integrations (AWS, Azure, GCP) Configure cloud integrations Partners and resellers Manage subscriptions (only for partners using original pricing model) Data partitions Manage data partitions Date retention Manage data retention NerdGraph terminology The following are terms that originate with GraphQL (the API format NerdGraph uses). Term Definition Queries and mutations There are two classes of GraphQL operations: Queries are basic requests used only to fetch data. These queries are not static, meaning that you can ask for more data or less data, depending on your needs. For each query, you can specify exactly what data you want to retrieve, as long as it is supported by the schema. Mutations are requests that perform an action, such as creating a resource or changing configuration. Mutations require the keyword mutation, as well as the name of the mutation. Type Data in GraphQL is organized into types. Types can be scalars (like strings, numbers, or booleans) or object types. An object type is a custom type made up of a collection of fields. For example, an object type called User may represent a user in a system. Field A field represents a piece of information on an object type that can be queried. Fields can be scalars, lists, or objects. For example, a User object type could have a string field called name. Interface An interface is an abstract type that represents a collection of common fields that other object types can implement. Tips on using the GraphiQL explorer You can make queries with the NerdGraph GraphiQL explorer. The explorer provides built-in schema definitions and features, including auto-complete and query validation. Query accounts a New Relic user can access You can query for the name of an account that an actor (a New Relic authorized user) has access to: query { actor { account(id: YOUR_ACCOUNT_ID) { name } } } Copy The response will mirror the query structure you defined in the request, making it easy to ask for the specific data that you want. { \"data\": { \"actor\": { \"account\": { \"name\": \"Data Nerd\" } } } } Copy Query user, account, and NRQL in one request The graph structure shows its capabilities when queries become more complex. For example, you can query for user information, account information, and make a NRQL query with one request. With REST API, this would take three different requests to three different endpoints. query { actor { account(id: YOUR_ACCOUNT_ID) { name nrql(query: \"SELECT * FROM Transaction\") { results } } user { name id } } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.67575,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. After you sign up for a free New Relic account and install any of our monitoring services, you can <em>get</em> <em>started</em> with NerdGraph. What is NerdGraph? New Relic has several APIs. NerdGraph"
      },
      "id": "6043ff97196a67d0a0960f55"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.114235,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can <em>get</em> <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "New Relic API keys",
        "API key UI",
        "Overview of keys",
        "Keys for data ingest",
        "Recommendations for managing ingest keys",
        "Keys for querying and configuration",
        "License key",
        "Create and manage license keys",
        "User key",
        "Browser key",
        "Insights insert key",
        "Important",
        "REST API key",
        "Insights query key",
        "Admin key",
        "Account ID"
      ],
      "title": "New Relic API keys",
      "type": "docs",
      "tags": [
        "APIs",
        "Get started",
        "Intro to APIs"
      ],
      "external_id": "b373cd68cf21daeb5d912ffb4b1ae3f14f500fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/",
      "published_at": "2022-01-08T03:11:41Z",
      "updated_at": "2022-01-08T03:11:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has several different APIs that use different API keys. This resource explains our keys, what they're used for, and how to access them. Ready to get started? Make sure you have a New Relic account. It's free, forever! API key UI Most of the keys can be viewed and managed via the API keys UI page: From the account dropdown, click API keys (get a direct link to the API keys page). If you're using NerdGraph, you can manage license keys and user keys from our GraphiQL explorer. Overview of keys If you're using a specific API, see the docs for that API to understand which keys are required and recommended. To learn about our APIs, see Introduction to APIs. Our keys can be broken down into two categories: Keys used for data ingest Keys used for querying and configuration Keys for data ingest There are many ways to get data into New Relic. Here are the API keys used for data ingest: License key: our primary ingest key, used for APM ingest, infrastructure monitoring ingest, and our ingest APIs and the integrations that use them. Browser key: used for browser monitoring ingest. Mobile app token: used for mobile monitoring ingest. Insights insert key: an older key that has been mostly deprecated, it has the same functionality as the license key. We recommend using the license key instead. Recommendations for managing ingest keys Some recommendations for managing data ingest keys: Keep them safe. Because these keys are used for data ingest, we recommend you treat ingest keys securely, like you would a password. This ensures no unwanted data is sent to your New Relic account. If a data ingest key falls into the wrong hands, it would allow someone to send data to your account, which could trigger false alerts and contaminate your data so that detecting actual issues is more difficult. If you believe a data ingest key has been exposed and has generated unwanted data, work with our Support team. Make additional keys. When setting up monitoring solutions that require a data ingest key, we recommend creating a new key, if possible. The original data ingest key for an account or app cannot be deleted or edited, so in order to give you greater management control (for example, deleting a key if exposed), we recommend creating new ones and using those. Keys for querying and configuration Here are keys used for querying New Relic data or configuration of features: User key, also known as a \"personal API key\": used for NerdGraph (our GraphQL API) and for accessing REST API endpoints. REST API key: used for the REST API but we instead recommend using the user key because it has fewer restrictions. Insights query key: used with the Insights query API for querying New Relic data. We recommend using NerdGraph instead of this API. License key Our primary key used for data ingest is called the license key, also referenced in the UI and NerdGraph API as ingest - license. The license key is a 40-character hexadecimal string associated with a New Relic account. Each account in a New Relic organization has at least one license key. When you first sign up for New Relic, that creates an organization with a single account, and that account has its own license key. If more accounts are added, each account will have its own license key. An account's original license key cannot be deleted but you can create additional license keys that can be managed and deleted. The types of data ingest the license key is used for include: APM agent data. Infrastructure agent data. Data sent via our core data ingest APIs (Metric API, Trace API, Event API, Log API), and the SDKs and integrations that use those APIs. The license key is used for almost all New Relic data ingest. The main exceptions are browser monitoring data (which uses a browser key) and mobile monitoring data (which uses a mobile app token). Create and manage license keys For tips on best practices for key management, see Ingest key recommendations. To add, delete, and manage license keys: From the account dropdown, click API keys (get a direct link to the API keys page). You can also create and manage keys with our NerdGraph API. Note that you can't delete or change an account's original license key (the one generated upon account creation). For that, contact New Relic support. User key New Relic user keys, sometimes referred to as \"personal API keys\", are required for using NerdGraph and for the REST API. A user key is tied to both a specific New Relic user and a specific account, and they cannot be transferred. Our APIs that use this key let a user make queries for any accounts that user has been granted access to, not just the specific account the key was created under. If the key's user is deleted, all their user keys will be deactivated and will no longer be valid in API requests. To view and manage the user key and other API keys in the UI: From the account dropdown, click API keys (here's a direct link to the API keys page). To manage this key via API, see Manage keys with NerdGraph. You can also get or generate a user key from the NerdGraph GraphiQL explorer. Browser key One of the New Relic API keys that are used for data ingest is the browser key. The browser key allows the ingestion of data from New Relic browser monitoring. For tips on best practices for key management, see Ingest key recommendations. To view and manage this key: From the account dropdown, click API keys (here's a direct link to the API keys page). You can't manage or delete an original browser key that was created when your account was created. For that, contact New Relic support. Insights insert key Important This key is still in use but we highly recommend using the license key, which can be used for the same things and more. One of the New Relic API keys used for data ingest is the Insights insert key, also known as an \"insert key\"). Note that the license key is used for the same functionality and more, which is why we recommend the license key over this key. This key is used for the ingestion of data via our Event API, Log API, Metric API, and Trace API, or via tools that use those APIs. Tips on availability and access: Because these keys are associated with an account and not a specific user, anyone in the account with access to a key can use it. As a best practice for security purposes, we recommend you use different Insights insert keys for different applications or different data sources. To find and manage Insights insert keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click Insights insert keys. REST API key Important We highly recommend using a user key instead, because that key has fewer restrictions. The REST API key is for using our REST APIs for Alerts, APM, browser, infrastructure alerts, as well as mobile monitoring REST APIs and the API Explorer. Things to consider: We recommend using our newer NerdGraph API over the REST API, if possible. Requires admin-level user permissions. If you don't have access to the REST API key or the REST API explorer, it might be due to lack of permissions. Talk to your New Relic account manager, or use a user key instead. Each New Relic account can have only one REST API key. To find and manage REST API keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click REST API key. Before you configure or delete an API key, ensure you are doing so for the correct account. Insights query key The Insights query key is used for our Insights query API: we now recommend using NerdGraph for querying New Relic data. To find and manage Insights query keys: From the account dropdown, click API keys (get a direct link to the API keys page). Then click Insights query keys. Admin key Important As of December 4, 2020, all existing admin keys have been migrated to be user keys. You don’t need to do anything for existing admin keys to remain active. They will be automatically accessible via the API keys UI, labeled as user keys, and granted identical permissions. You can manage them as you would any user key via the same workflow. All migrated admin keys will have a note that says “Migrated from an admin user key” in the key table, so you’ll be able to find them easily. Account ID Looking for the account ID? See Account ID.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.4021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic has several different APIs that use different API keys. This resource explains our keys, what they&#x27;re used for, and how to access them. Ready to <em>get</em> <em>started</em>? Make sure you have a New Relic account. It&#x27;s free, forever! API key UI Most of the keys can be viewed and managed via the API keys"
      },
      "id": "6043fa3464441f1358378f3b"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure/prometheus-high-availability-ha": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08917,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " in the URL. We don&#x27;t recommend using it unless one of these other approaches doesn&#x27;t work in your environment. European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f7e6e410b22d67d08c56f91dc26a8e7a6f47e8cd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2022-01-09T01:49:07Z",
      "updated_at": "2021-10-24T02:43:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "617dc722e7b9d2fb55c049b6"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "b3a08d5b6e4c4c04f4046167eb836e6b45523376",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2022-01-08T08:23:23Z",
      "updated_at": "2021-10-24T02:42:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.9645,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "617dae7a196a6740e2f7e23d"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure/remote-write-drop-data": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08905,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " in the URL. We don&#x27;t recommend using it unless one of these other approaches doesn&#x27;t work in your environment. European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "c84b7e7d250ff6c9f322566015c6e0c27887c918",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2022-01-08T08:59:31Z",
      "updated_at": "2021-10-24T02:43:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "617d725064441f9ed9fbd6f3"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f7e6e410b22d67d08c56f91dc26a8e7a6f47e8cd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2022-01-09T01:49:07Z",
      "updated_at": "2021-10-24T02:43:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "617dc722e7b9d2fb55c049b6"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install in Kubernetes",
        "Docker",
        "Install",
        "Update the integration",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "a6a9f87452643ebd1e5e97b62f6db779929fe6e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-30T22:28:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements. Install in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can find the documentation of how to install Kubernetes integration (which includes the feature flag for Prometheus OpenMetric's integration) here: Kubernetes integration: install and configure Alternatively, we also offer fully manual instructions for deploying our integration using Helm. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Docker Install To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: docker rm -f nri-prometheus Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.767,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Before you <em>install</em> New Relic&#x27;s <em>Prometheus</em> <em>OpenMetrics</em> integration, review the requirements. <em>Install</em> in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can"
      },
      "id": "617dc72264441f923dfbefb9"
    },
    {
      "sections": [
        "Ignore or include Prometheus metrics",
        "Prevent billing increases",
        "Identify metrics to ignore or include",
        "Caution",
        "Filter out unwanted metrics (ignore)",
        "Include only specific metrics (except)"
      ],
      "title": "Ignore or include Prometheus metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "e46d47240f8dc0da19f05e9aff4f955ca003f04b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-30T22:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Avoid sending Prometheus OpenMetrics integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific metrics. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges, as explained in this document. Prevent billing increases We use the Prometheus discovery and scrape annotations. If you set the Prometheus OpenMetrics integration to scrape all the available targets and to send all the data that's exposed from those targets, you may exceed New Relic's platform limits and increase your billing charges. To help prevent this from happening, use the integration's filtering capabilities. For more information, see the Prometheus OpenMetrics integration requirements for Docker and Kubernetes. Also see the troubleshooting procedures for NrIntegrationError events. Identify metrics to ignore or include To decide what data to include or exclude, use New Relic's Metric API to explore your metric data. Then, refine your filters to scrape only relevant targets and send useful metrics. To filter out unwanted metrics from a target, use the ignore_metrics configuration option. To filter out targets instead of the metrics, use the scrape_enabled_label configuration option. Caution The histogram and summary metrics type filtering apply to the base name. You can't filter by the _bucket, _sum, or _count timeseries for that metric. The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg config map showing an example configuration. The integration will ignore or include metrics before executing the other functions to add, rename, or copy attributes. Filter out unwanted metrics (ignore) To ignore unwanted metrics, use the following transformation. Example: Configuration To drop all metrics that start with go_ or process_: transformations: - description: \"General processing rules\" ignore_metrics: - prefixes: - \"go_\" - \"process_\" Copy Example: Input go_goroutines 13 process_virtual_memory_bytes 2.062336e+07 mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy This is taken from the MySQL exporter. Besides the MySQL metrics, it also exposes metrics about the exporter that may not be of interest to you. Example: Output mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy Include only specific metrics (except) If you only want to include specific metrics, you can use the except list under the ignore_metrics section. As the name implies, this will ignore all the metrics except the ones that contain the with the given prefixes. Example: Configuration To drop all metrics except kube_hpa_: transformations: - description: \"General processing rules\" ignore_metrics: - except: - kube_hpa_ Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.76682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "sections": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Avoid sending <em>Prometheus</em> <em>OpenMetrics</em> integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific <em>metrics</em>. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges"
      },
      "id": "617dc6e8196a67cf3df7ce4c"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "396828c59bd1c13e4be7d9790b3c3b586f74065c",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-24T02:40:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "617dc6e9e7b9d24595c05537"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/add-rename-or-copy-prometheus-attributes": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install in Kubernetes",
        "Docker",
        "Install",
        "Update the integration",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "a6a9f87452643ebd1e5e97b62f6db779929fe6e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-30T22:28:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements. Install in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can find the documentation of how to install Kubernetes integration (which includes the feature flag for Prometheus OpenMetric's integration) here: Kubernetes integration: install and configure Alternatively, we also offer fully manual instructions for deploying our integration using Helm. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Docker Install To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: docker rm -f nri-prometheus Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.767,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Before you <em>install</em> New Relic&#x27;s <em>Prometheus</em> <em>OpenMetrics</em> integration, review the requirements. <em>Install</em> in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can"
      },
      "id": "617dc72264441f923dfbefb9"
    },
    {
      "sections": [
        "Ignore or include Prometheus metrics",
        "Prevent billing increases",
        "Identify metrics to ignore or include",
        "Caution",
        "Filter out unwanted metrics (ignore)",
        "Include only specific metrics (except)"
      ],
      "title": "Ignore or include Prometheus metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "e46d47240f8dc0da19f05e9aff4f955ca003f04b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-30T22:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Avoid sending Prometheus OpenMetrics integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific metrics. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges, as explained in this document. Prevent billing increases We use the Prometheus discovery and scrape annotations. If you set the Prometheus OpenMetrics integration to scrape all the available targets and to send all the data that's exposed from those targets, you may exceed New Relic's platform limits and increase your billing charges. To help prevent this from happening, use the integration's filtering capabilities. For more information, see the Prometheus OpenMetrics integration requirements for Docker and Kubernetes. Also see the troubleshooting procedures for NrIntegrationError events. Identify metrics to ignore or include To decide what data to include or exclude, use New Relic's Metric API to explore your metric data. Then, refine your filters to scrape only relevant targets and send useful metrics. To filter out unwanted metrics from a target, use the ignore_metrics configuration option. To filter out targets instead of the metrics, use the scrape_enabled_label configuration option. Caution The histogram and summary metrics type filtering apply to the base name. You can't filter by the _bucket, _sum, or _count timeseries for that metric. The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg config map showing an example configuration. The integration will ignore or include metrics before executing the other functions to add, rename, or copy attributes. Filter out unwanted metrics (ignore) To ignore unwanted metrics, use the following transformation. Example: Configuration To drop all metrics that start with go_ or process_: transformations: - description: \"General processing rules\" ignore_metrics: - prefixes: - \"go_\" - \"process_\" Copy Example: Input go_goroutines 13 process_virtual_memory_bytes 2.062336e+07 mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy This is taken from the MySQL exporter. Besides the MySQL metrics, it also exposes metrics about the exporter that may not be of interest to you. Example: Output mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy Include only specific metrics (except) If you only want to include specific metrics, you can use the except list under the ignore_metrics section. As the name implies, this will ignore all the metrics except the ones that contain the with the given prefixes. Example: Configuration To drop all metrics except kube_hpa_: transformations: - description: \"General processing rules\" ignore_metrics: - except: - kube_hpa_ Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.76682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "sections": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Avoid sending <em>Prometheus</em> <em>OpenMetrics</em> integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific <em>metrics</em>. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges"
      },
      "id": "617dc6e8196a67cf3df7ce4c"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "396828c59bd1c13e4be7d9790b3c3b586f74065c",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-24T02:40:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "617dc6e9e7b9d24595c05537"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install in Kubernetes",
        "Docker",
        "Install",
        "Update the integration",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "a6a9f87452643ebd1e5e97b62f6db779929fe6e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-30T22:28:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements. Install in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can find the documentation of how to install Kubernetes integration (which includes the feature flag for Prometheus OpenMetric's integration) here: Kubernetes integration: install and configure Alternatively, we also offer fully manual instructions for deploying our integration using Helm. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Docker Install To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: docker rm -f nri-prometheus Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.767,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Before you <em>install</em> New Relic&#x27;s <em>Prometheus</em> <em>OpenMetrics</em> integration, review the requirements. <em>Install</em> in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can"
      },
      "id": "617dc72264441f923dfbefb9"
    },
    {
      "sections": [
        "Ignore or include Prometheus metrics",
        "Prevent billing increases",
        "Identify metrics to ignore or include",
        "Caution",
        "Filter out unwanted metrics (ignore)",
        "Include only specific metrics (except)"
      ],
      "title": "Ignore or include Prometheus metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "e46d47240f8dc0da19f05e9aff4f955ca003f04b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-30T22:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Avoid sending Prometheus OpenMetrics integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific metrics. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges, as explained in this document. Prevent billing increases We use the Prometheus discovery and scrape annotations. If you set the Prometheus OpenMetrics integration to scrape all the available targets and to send all the data that's exposed from those targets, you may exceed New Relic's platform limits and increase your billing charges. To help prevent this from happening, use the integration's filtering capabilities. For more information, see the Prometheus OpenMetrics integration requirements for Docker and Kubernetes. Also see the troubleshooting procedures for NrIntegrationError events. Identify metrics to ignore or include To decide what data to include or exclude, use New Relic's Metric API to explore your metric data. Then, refine your filters to scrape only relevant targets and send useful metrics. To filter out unwanted metrics from a target, use the ignore_metrics configuration option. To filter out targets instead of the metrics, use the scrape_enabled_label configuration option. Caution The histogram and summary metrics type filtering apply to the base name. You can't filter by the _bucket, _sum, or _count timeseries for that metric. The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg config map showing an example configuration. The integration will ignore or include metrics before executing the other functions to add, rename, or copy attributes. Filter out unwanted metrics (ignore) To ignore unwanted metrics, use the following transformation. Example: Configuration To drop all metrics that start with go_ or process_: transformations: - description: \"General processing rules\" ignore_metrics: - prefixes: - \"go_\" - \"process_\" Copy Example: Input go_goroutines 13 process_virtual_memory_bytes 2.062336e+07 mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy This is taken from the MySQL exporter. Besides the MySQL metrics, it also exposes metrics about the exporter that may not be of interest to you. Example: Output mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy Include only specific metrics (except) If you only want to include specific metrics, you can use the except list under the ignore_metrics section. As the name implies, this will ignore all the metrics except the ones that contain the with the given prefixes. Example: Configuration To drop all metrics except kube_hpa_: transformations: - description: \"General processing rules\" ignore_metrics: - except: - kube_hpa_ Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.76682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "sections": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Avoid sending <em>Prometheus</em> <em>OpenMetrics</em> integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific <em>metrics</em>. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges"
      },
      "id": "617dc6e8196a67cf3df7ce4c"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "396828c59bd1c13e4be7d9790b3c3b586f74065c",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-24T02:40:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "617dc6e9e7b9d24595c05537"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install in Kubernetes",
        "Docker",
        "Install",
        "Update the integration",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "a6a9f87452643ebd1e5e97b62f6db779929fe6e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-30T22:28:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements. Install in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can find the documentation of how to install Kubernetes integration (which includes the feature flag for Prometheus OpenMetric's integration) here: Kubernetes integration: install and configure Alternatively, we also offer fully manual instructions for deploying our integration using Helm. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Docker Install To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: docker rm -f nri-prometheus Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.767,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Before you <em>install</em> New Relic&#x27;s <em>Prometheus</em> <em>OpenMetrics</em> integration, review the requirements. <em>Install</em> in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can"
      },
      "id": "617dc72264441f923dfbefb9"
    },
    {
      "sections": [
        "Ignore or include Prometheus metrics",
        "Prevent billing increases",
        "Identify metrics to ignore or include",
        "Caution",
        "Filter out unwanted metrics (ignore)",
        "Include only specific metrics (except)"
      ],
      "title": "Ignore or include Prometheus metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "e46d47240f8dc0da19f05e9aff4f955ca003f04b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-30T22:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Avoid sending Prometheus OpenMetrics integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific metrics. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges, as explained in this document. Prevent billing increases We use the Prometheus discovery and scrape annotations. If you set the Prometheus OpenMetrics integration to scrape all the available targets and to send all the data that's exposed from those targets, you may exceed New Relic's platform limits and increase your billing charges. To help prevent this from happening, use the integration's filtering capabilities. For more information, see the Prometheus OpenMetrics integration requirements for Docker and Kubernetes. Also see the troubleshooting procedures for NrIntegrationError events. Identify metrics to ignore or include To decide what data to include or exclude, use New Relic's Metric API to explore your metric data. Then, refine your filters to scrape only relevant targets and send useful metrics. To filter out unwanted metrics from a target, use the ignore_metrics configuration option. To filter out targets instead of the metrics, use the scrape_enabled_label configuration option. Caution The histogram and summary metrics type filtering apply to the base name. You can't filter by the _bucket, _sum, or _count timeseries for that metric. The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg config map showing an example configuration. The integration will ignore or include metrics before executing the other functions to add, rename, or copy attributes. Filter out unwanted metrics (ignore) To ignore unwanted metrics, use the following transformation. Example: Configuration To drop all metrics that start with go_ or process_: transformations: - description: \"General processing rules\" ignore_metrics: - prefixes: - \"go_\" - \"process_\" Copy Example: Input go_goroutines 13 process_virtual_memory_bytes 2.062336e+07 mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy This is taken from the MySQL exporter. Besides the MySQL metrics, it also exposes metrics about the exporter that may not be of interest to you. Example: Output mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy Include only specific metrics (except) If you only want to include specific metrics, you can use the except list under the ignore_metrics section. As the name implies, this will ignore all the metrics except the ones that contain the with the given prefixes. Example: Configuration To drop all metrics except kube_hpa_: transformations: - description: \"General processing rules\" ignore_metrics: - except: - kube_hpa_ Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.76682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "sections": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Avoid sending <em>Prometheus</em> <em>OpenMetrics</em> integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific <em>metrics</em>. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges"
      },
      "id": "617dc6e8196a67cf3df7ce4c"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "9cb9e46d99ff21fed306935fc03cce1787138f76",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-24T02:40:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00687,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "617dc6bc28ccbccb06800fc6"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install in Kubernetes",
        "Docker",
        "Install",
        "Update the integration",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "a6a9f87452643ebd1e5e97b62f6db779929fe6e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-30T22:28:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements. Install in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can find the documentation of how to install Kubernetes integration (which includes the feature flag for Prometheus OpenMetric's integration) here: Kubernetes integration: install and configure Alternatively, we also offer fully manual instructions for deploying our integration using Helm. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Docker Install To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: docker rm -f nri-prometheus Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.767,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Before you <em>install</em> New Relic&#x27;s <em>Prometheus</em> <em>OpenMetrics</em> integration, review the requirements. <em>Install</em> in Kubernetes We encourage you to use our automated installer for servers, VMs, and unprivileged environments which also covers the configuration of the monitorization of Kubernetes clusters. You can"
      },
      "id": "617dc72264441f923dfbefb9"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "396828c59bd1c13e4be7d9790b3c3b586f74065c",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-24T02:40:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "617dc6e9e7b9d24595c05537"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "9cb9e46d99ff21fed306935fc03cce1787138f76",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-24T02:40:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00687,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "617dc6bc28ccbccb06800fc6"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration": [
    {
      "sections": [
        "Ignore or include Prometheus metrics",
        "Prevent billing increases",
        "Identify metrics to ignore or include",
        "Caution",
        "Filter out unwanted metrics (ignore)",
        "Include only specific metrics (except)"
      ],
      "title": "Ignore or include Prometheus metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "e46d47240f8dc0da19f05e9aff4f955ca003f04b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-30T22:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Avoid sending Prometheus OpenMetrics integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific metrics. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges, as explained in this document. Prevent billing increases We use the Prometheus discovery and scrape annotations. If you set the Prometheus OpenMetrics integration to scrape all the available targets and to send all the data that's exposed from those targets, you may exceed New Relic's platform limits and increase your billing charges. To help prevent this from happening, use the integration's filtering capabilities. For more information, see the Prometheus OpenMetrics integration requirements for Docker and Kubernetes. Also see the troubleshooting procedures for NrIntegrationError events. Identify metrics to ignore or include To decide what data to include or exclude, use New Relic's Metric API to explore your metric data. Then, refine your filters to scrape only relevant targets and send useful metrics. To filter out unwanted metrics from a target, use the ignore_metrics configuration option. To filter out targets instead of the metrics, use the scrape_enabled_label configuration option. Caution The histogram and summary metrics type filtering apply to the base name. You can't filter by the _bucket, _sum, or _count timeseries for that metric. The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg config map showing an example configuration. The integration will ignore or include metrics before executing the other functions to add, rename, or copy attributes. Filter out unwanted metrics (ignore) To ignore unwanted metrics, use the following transformation. Example: Configuration To drop all metrics that start with go_ or process_: transformations: - description: \"General processing rules\" ignore_metrics: - prefixes: - \"go_\" - \"process_\" Copy Example: Input go_goroutines 13 process_virtual_memory_bytes 2.062336e+07 mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy This is taken from the MySQL exporter. Besides the MySQL metrics, it also exposes metrics about the exporter that may not be of interest to you. Example: Output mysql_global_status_commands_total{command=\"ha_close\"} 0 mysql_global_status_commands_total{command=\"ha_open\"} 0 Copy Include only specific metrics (except) If you only want to include specific metrics, you can use the except list under the ignore_metrics section. As the name implies, this will ignore all the metrics except the ones that contain the with the given prefixes. Example: Configuration To drop all metrics except kube_hpa_: transformations: - description: \"General processing rules\" ignore_metrics: - except: - kube_hpa_ Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.76682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "sections": "Ignore or include <em>Prometheus</em> <em>metrics</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Avoid sending <em>Prometheus</em> <em>OpenMetrics</em> integration data that is not relevant to your monitoring needs. Instead, use filters to ignore or include specific <em>metrics</em>. This will help you control the amount and types of data you send to New Relic. This will also help you avoid additional billing charges"
      },
      "id": "617dc6e8196a67cf3df7ce4c"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "396828c59bd1c13e4be7d9790b3c3b586f74065c",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2022-01-08T09:46:08Z",
      "updated_at": "2021-10-24T02:40:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "617dc6e9e7b9d24595c05537"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "9cb9e46d99ff21fed306935fc03cce1787138f76",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2022-01-08T09:45:25Z",
      "updated_at": "2021-10-24T02:40:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.00687,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "617dc6bc28ccbccb06800fc6"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration": [
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "c84b7e7d250ff6c9f322566015c6e0c27887c918",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2022-01-08T08:59:31Z",
      "updated_at": "2021-10-24T02:43:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1968.0598,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can configure your <em>remote</em> <em>write</em> <em>integration</em> so that New"
      },
      "id": "617d725064441f9ed9fbd6f3"
    },
    {
      "image": "https://docs.newrelic.com/static/d2a9c929c7541b67b6fe4c87844fc01b/ae694/prometheus_grafana_dashboard.png",
      "url": "https://docs.newrelic.com/whats-new/2020/08/create-grafana-dashboards-prometheus-data-stored-new-relic/",
      "sections": [
        "Create Grafana dashboards with Prometheus data stored in New Relic",
        "Step 1: Get data flowing into New Relic with the Prometheus remote write integration",
        "Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic"
      ],
      "published_at": "2022-01-08T07:20:23Z",
      "title": "Create Grafana dashboards with Prometheus data stored in New Relic",
      "updated_at": "2021-10-19T05:58:32Z",
      "type": "docs",
      "external_id": "da09ab47a2ac806ad3ed1fa67e3a02dd54394383",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We’ve teamed up with Grafana Labs so you can use our platform as a data source for Prometheus metrics and see them in your existing dashboards, seamlessly tapping into the reliability, scale, and security provided by New Relic. Follow the steps below or use this more detailed walkthrough to send Prometheus data to New Relic, so that Grafana can populate your existing Prometheus-specific dashboards with that data. This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. You’ll also need to sign up for New Relic. Here's an example of how these Grafana dashboards with Prometheus data look in our new dark mode. Step 1: Get data flowing into New Relic with the Prometheus remote write integration Go to Instrument Everything – US or Instrument Everything – EU, then click the Prometheus tile. You can also go to the Prometheus remote write setup page to get your remote_write URL. For more information on how to set up the Prometheus remote write integration, check out our docs. Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic For more information on how to configure New Relic as a Prometheus data source for Grafana, check out our docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1963.6504,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create Grafana dashboards with <em>Prometheus</em> data stored in New Relic",
        "sections": "Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "body": " dashboards with <em>Prometheus</em> data look in our new dark mode. Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> Go to Instrument Everything – US or Instrument Everything – EU, then click the <em>Prometheus</em> tile. You can also go to the <em>Prometheus</em> <em>remote</em> <em>write</em> setup page to get"
      },
      "id": "60445821e7b9d23b585799e4"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Prometheus OpenMetrics integration for Kubernetes or Docker",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "aaeaf025175ef14ba33549b5a315caab72c929d0",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2022-01-08T08:59:32Z",
      "updated_at": "2021-10-24T02:39:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Examine the benefits, reminders, and recommendations for each option below. Prometheus remote write integration Benefits: Easy access to your combined metrics in New Relic if you already have Prometheus servers. Access only takes one line of yaml in your Prometheus configuration. Access your metrics through both New Relic and Prometheus without making additional adjustments in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Reminders: You will need to manage your Prometheus servers. You can reduce your storage retention. Fewer query loads to the server. Recommendations: Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Prometheus OpenMetrics integration for Kubernetes or Docker Benefits: Best for an alternative to Prometheus servers Store all your metrics directly in New Relic No need to manage any Prometheus servers yourself. No need for local storage. Reminders: Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. New Relic's database can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free — we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relic’s Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1962.5546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "This page provides an overview of New Relic&#x27;s <em>Prometheus</em> <em>integration</em> options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. <em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>? We currently offer two"
      },
      "id": "6174c75c28ccbcbd0cc6bde8"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 320.08878,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " in the URL. We don&#x27;t recommend using it unless one of these other approaches doesn&#x27;t work in your environment. European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "c84b7e7d250ff6c9f322566015c6e0c27887c918",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2022-01-08T08:59:31Z",
      "updated_at": "2021-10-24T02:43:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "617d725064441f9ed9fbd6f3"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "b3a08d5b6e4c4c04f4046167eb836e6b45523376",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2022-01-08T08:23:23Z",
      "updated_at": "2021-10-24T02:42:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96448,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "617dae7a196a6740e2f7e23d"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration": [
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "c84b7e7d250ff6c9f322566015c6e0c27887c918",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2022-01-08T08:59:31Z",
      "updated_at": "2021-10-24T02:43:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "617d725064441f9ed9fbd6f3"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f7e6e410b22d67d08c56f91dc26a8e7a6f47e8cd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2022-01-09T01:49:07Z",
      "updated_at": "2021-10-24T02:43:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "617dc722e7b9d2fb55c049b6"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "b3a08d5b6e4c4c04f4046167eb836e6b45523376",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2022-01-08T08:23:23Z",
      "updated_at": "2021-10-24T02:42:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.96448,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "617dae7a196a6740e2f7e23d"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/compare-rw-data-sent-billed-bytes": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 241.02164,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> <em>data</em> flowing in New Relic with just a few simple steps. This page covers basic setup for the <em>remote</em> <em>write</em> <em>integration</em>, as well as a few common troubleshooting topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Prometheus OpenMetrics integration for Kubernetes or Docker",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "aaeaf025175ef14ba33549b5a315caab72c929d0",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2022-01-08T08:59:32Z",
      "updated_at": "2021-10-24T02:39:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Examine the benefits, reminders, and recommendations for each option below. Prometheus remote write integration Benefits: Easy access to your combined metrics in New Relic if you already have Prometheus servers. Access only takes one line of yaml in your Prometheus configuration. Access your metrics through both New Relic and Prometheus without making additional adjustments in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Reminders: You will need to manage your Prometheus servers. You can reduce your storage retention. Fewer query loads to the server. Recommendations: Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Prometheus OpenMetrics integration for Kubernetes or Docker Benefits: Best for an alternative to Prometheus servers Store all your metrics directly in New Relic No need to manage any Prometheus servers yourself. No need for local storage. Reminders: Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. New Relic's database can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free — we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relic’s Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.48611,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric <em>data</em> to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " <em>write</em> <em>integration</em> The <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> allows you to forward <em>telemetry</em> <em>data</em> from your existing <em>Prometheus</em> servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw <em>data</em> to queries, dashboards, and more. Scale your <em>data</em> and get"
      },
      "id": "6174c75c28ccbcbd0cc6bde8"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next"
      ],
      "published_at": "2022-01-08T09:46:08Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-10-24T02:42:19Z",
      "type": "docs",
      "external_id": "6f7d739c53aa8b1dd7674347d2af44e677084e99",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 185.19563,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "<em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "body": " scrape <em>data</em> from <em>Prometheus</em> endpoints, the <em>remote</em> <em>write</em> <em>integration</em> allows you to forward <em>telemetry</em> <em>data</em> from your existing <em>Prometheus</em> servers to New Relic. You can leverage the full range of options for setup and management, from raw <em>data</em> to queries and dashboards and beyond. With the <em>Prometheus</em>"
      },
      "id": "617d57fb64441f2704fbcb24"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/debug-issues-data-sent-metric-api-prometheus-integration": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.2951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172516,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "803a1c1d838ee988bec78bf4ba69d7b46b4c5372",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2022-01-08T09:10:40Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.13313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "617d51fe28ccbcd2fd7ff5e0"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.2951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172516,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "803a1c1d838ee988bec78bf4ba69d7b46b4c5372",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2022-01-08T09:10:40Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.13313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "617d51fe28ccbcd2fd7ff5e0"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/get-logs-prometheus-integration": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.29506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172485,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "803a1c1d838ee988bec78bf4ba69d7b46b4c5372",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2022-01-08T09:10:40Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.13313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "617d51fe28ccbcd2fd7ff5e0"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/get-scraper-metrics-prometheus-integration": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.29506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172485,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "803a1c1d838ee988bec78bf4ba69d7b46b4c5372",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2022-01-08T09:10:40Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.13313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "617d51fe28ccbcd2fd7ff5e0"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.29506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172485,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0fb0d18fde3e5329639abb3dfb90d4e78f576d14",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2022-01-08T09:00:19Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.03335,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "617da8b064441fb7a9fbca71"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.295,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "803a1c1d838ee988bec78bf4ba69d7b46b4c5372",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2022-01-08T09:10:40Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.133125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "617d51fe28ccbcd2fd7ff5e0"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.295,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "803a1c1d838ee988bec78bf4ba69d7b46b4c5372",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2022-01-08T09:10:40Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.133125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "617d51fe28ccbcd2fd7ff5e0"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.295,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "You can get <em>Prometheus</em> data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common <em>troubleshooting</em> topics. For information on integrating <em>Prometheus</em> servers in a high availability (HA) configuration, see our"
      },
      "id": "617d515264441fc9eafbe18f"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.172455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Troubleshooting</em>",
        "body": " monitoring Windows Kubernetes <em>Prometheus</em> On-host <em>integrations</em> (for services like NGINX, StatsD, MySQL, etc.) AWS cloud <em>integrations</em> Azure cloud <em>integrations</em> Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn&#x27;t require installation, except"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "803a1c1d838ee988bec78bf4ba69d7b46b4c5372",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2022-01-08T09:10:40Z",
      "updated_at": "2021-10-24T02:47:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.133125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "617d51fe28ccbcd2fd7ff5e0"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/view-query-data/supported-promql-features": [
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "a6d9e2f685b835d6d540d1b4a1bc30a99ab3bd92",
      "image": "https://docs.newrelic.com/static/PROMQL-query-2-4f5bf0fc14fc06c4e85f7bf7c4937401.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2022-01-08T13:08:12Z",
      "updated_at": "2021-12-20T04:23:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\". NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.22647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "617daa50e7b9d2dab7c060c2"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Generate histograms and calculate percentiles"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "6a77103c9d0898b9b0219a6ada7165498e5cab54",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2022-01-08T09:11:28Z",
      "updated_at": "2021-12-04T16:00:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Generate histograms and calculate percentiles Using Prometheus remote write or version 2.0.0 and higher of the Prometheus OpenMetrics Integration (POMI), you can generate histograms and calculate percentiles from your data. For Prometheus histograms, a bucket <basename>_bucket{le=\"42\"} will be sent as the metric <basename>_bucket, and the dimension will be {histogram.bucket.le=\"42\"}. NRQL has two functions that work on Prometheus histograms ingested via remote write or the Prometheus OpenMetrics Integration (starting with version 2.0.0): bucketPercentile(), and histogram(). The links include query examples.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.7551,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "617d54dee7b9d20ef9c06101"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.63913,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "?<em>prometheus</em>_server=YOUR_<em>DATA</em>_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your <em>Prometheus</em> server. <em>View</em> your <em>data</em> in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map <em>Prometheus</em> and New Relic metric types The <em>Prometheus</em> remote"
      },
      "id": "617d515264441fc9eafbe18f"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/view-query-data/translate-promql-queries-nrql": [
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Generate histograms and calculate percentiles"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "6a77103c9d0898b9b0219a6ada7165498e5cab54",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2022-01-08T09:11:28Z",
      "updated_at": "2021-12-04T16:00:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Generate histograms and calculate percentiles Using Prometheus remote write or version 2.0.0 and higher of the Prometheus OpenMetrics Integration (POMI), you can generate histograms and calculate percentiles from your data. For Prometheus histograms, a bucket <basename>_bucket{le=\"42\"} will be sent as the metric <basename>_bucket, and the dimension will be {histogram.bucket.le=\"42\"}. NRQL has two functions that work on Prometheus histograms ingested via remote write or the Prometheus OpenMetrics Integration (starting with version 2.0.0): bucketPercentile(), and histogram(). The links include query examples.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.7551,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "617d54dee7b9d20ef9c06101"
    },
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d1385a3b86dec8af0d1a59e666079b15ed3ab93a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2022-01-08T13:07:30Z",
      "updated_at": "2021-10-24T02:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.87935,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "617daa0d28ccbc49757fecfd"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.63913,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "?<em>prometheus</em>_server=YOUR_<em>DATA</em>_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your <em>Prometheus</em> server. <em>View</em> your <em>data</em> in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map <em>Prometheus</em> and New Relic metric types The <em>Prometheus</em> remote"
      },
      "id": "617d515264441fc9eafbe18f"
    }
  ],
  "/docs/infrastructure/prometheus-integrations/view-query-data/view-query-your-prometheus-data": [
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "a6d9e2f685b835d6d540d1b4a1bc30a99ab3bd92",
      "image": "https://docs.newrelic.com/static/PROMQL-query-2-4f5bf0fc14fc06c4e85f7bf7c4937401.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2022-01-08T13:08:12Z",
      "updated_at": "2021-12-20T04:23:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\". NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.22647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "617daa50e7b9d2dab7c060c2"
    },
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d1385a3b86dec8af0d1a59e666079b15ed3ab93a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2022-01-08T13:07:30Z",
      "updated_at": "2021-10-24T02:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.87935,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "617daa0d28ccbc49757fecfd"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "2b83e518967d4375d0530d239067a0c49c42ad3a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2022-01-08T09:47:40Z",
      "updated_at": "2022-01-04T10:57:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: Prometheus v2.26 and newer remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME authorization: credentials: YOUR_LICENSE_KEY Copy Prometheus older than v2.26 remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR Any Prometheus version remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy This approach passes credentials in the URL. We don't recommend using it unless one of these other approaches doesn't work in your environment. European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.63913,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> remote write <em>integration</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "?<em>prometheus</em>_server=YOUR_<em>DATA</em>_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your <em>Prometheus</em> server. <em>View</em> your <em>data</em> in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map <em>Prometheus</em> and New Relic metric types The <em>Prometheus</em> remote"
      },
      "id": "617d515264441fc9eafbe18f"
    }
  ],
  "/docs/instrumentation-editor-instrument-net-ui": [
    {
      "image": "https://docs.newrelic.com/static/6decabb9d8cd5dc3e18f2f647f7c7cdd/c1b63/arrow-step-diagram-trans.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/quick-start/",
      "sections": [
        "Distributed tracing setup options"
      ],
      "published_at": "2022-01-08T03:17:11Z",
      "title": "Distributed tracing setup options",
      "updated_at": "2022-01-07T21:54:46Z",
      "type": "docs",
      "external_id": "44df1a2d07693a41fa23c9bba9473ce8ebabe47e",
      "document_type": "page",
      "popularity": 1,
      "body": "We recommend you do an initial setup of distributed tracing and consider the advanced Infinite Tracing feature if you are not getting the data you need. Also, if you are currently using New Relic APM agents and would like to enable distributed tracing, see our planning guide. Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! To set up distributed tracing, you'll complete three general steps: Identify services: Identify and write down the endpoints, services, languages, and systems that are used to complete this request (you'll need this information in the next step). If you have an environment diagram like the following, you could use it to create a list of services handling requests: Instrument services: Instrument each service you identify so it can send your trace data. Some tools, such as APM agents, instrument services automatically, while other tools require you to insert some code in the services. Click the icon below for instrumentation steps: APM: C APM: Golang APM: Java APM: .NET APM: Node.js APM: PHP APM: Python APM: Ruby Browser monitoring Mobile monitoring AWS Lambda Functions Kamon OpenTelemetry X-Ray Trace API: Zipkin format Trace API: generic format View traces: After you instrument the services, generate some traffic in your application, and then go to the New Relic UI to see your trace data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.00803,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " automatically, while other tools require you to insert some code in the services. Click the icon below for <em>instrumentation</em> steps: APM: C APM: Golang APM: Java APM: .<em>NET</em> APM: Node.js APM: PHP APM: Python APM: Ruby Browser monitoring Mobile monitoring AWS Lambda Functions Kamon OpenTelemetry X-Ray Trace API"
      },
      "id": "61d8b6a664441fbe9700cc16"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Review New Relic settings for exports",
        "Important",
        "Complete the export configuration steps",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "View our OpenTelemetry examples",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "067b2e00bd167f4d78a1398575acd6f3ac76e069",
      "image": "",
      "url": "https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2022-01-08T08:32:45Z",
      "updated_at": "2022-01-08T08:32:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic. If you prefer to export your data first to an OpenTelemetry collector, we have separate instructions. Here's an example of sending data from your service directly to New Relic. To complete this third step, first familiarize yourself with some required New Relic settings, and then complete the steps in the OTLP exporter documentation for your language. Review New Relic settings for exports Before you go to the external OTLP exporter documentation, consult the table below so you're ready to do the following: Configure the OTLP exporter to add a header (api-key) whose value is the license key for the New Relic account you want to send data to. Based on your region, configure the endpoint where the exporter sends data to New Relic. Region gRPC HTTP/1.1 Endpoint API header name API header value TLS encryption required US ✅ ❌ https://otlp.nr-data.net:4317 api-key License key ✅ EU ✅ ❌ https://otlp.eu01.nr-data.net:4317 api-key License key ✅ Important If you have FedRamp compliance constraints, you will need to use https://gov-otlp.nr-data.net:4317. Please see FedRAMP-compliant endpoints for further information. Important In Node.js, the opentelemetry-collector-grpc library requires additional options to enable TLS. Complete the export configuration steps Click on the link below for your language and complete the configuration steps. When you're done, return here to complete Step 4. View your data in the New Relic UI. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. In this case, instead of exporting data directly to New Relic, you export through a collector that you set up. In the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector after you make the following changes: Replace OTLP_ENDPOINT_HERE with the appropriate endpoint, using the <hostname>:<port> format without http(s):// (for example, otlp.nr-data.net:4317). Replace YOUR_KEY_HERE with your account's license key. export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector \\ --config otel-config.yaml Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One user interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options or how to make sure your data appears in the UI, see View your OpenTelemetry data in New Relic. View our OpenTelemetry examples View some of our examples for using OpenTelemetry with New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.83492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 2. <em>Instrument</em> your service with OpenTelemetry",
        "body": " your telemetry data to New Relic View your data in the New Relic <em>UI</em> Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. <em>Instrument</em> your service with OpenTelemetry To get started, you <em>instrument</em> your"
      },
      "id": "617dca7464441f8186fbc951"
    },
    {
      "sections": [
        ".NET agent configuration",
        "Configuration overview",
        "Important",
        "Configuration methods and precedence levels",
        "Required environment variables",
        "Caution",
        ".NET Framework environment variables",
        ".NET Core environment variables",
        "Profiler conflict explanation",
        "Optional environment variables",
        "Setup options, newrelic.config",
        "Configuration element",
        "agentEnabled",
        "maxStackTraceLines",
        "timingPrecision",
        "Service element",
        "licenseKey (required)",
        "sendEnvironmentInfo",
        "syncStartup",
        "sendDataOnExit",
        "sendDataOnExitThreshold",
        "completeTransactionsOnThread",
        "requestTimeout",
        "autoStart",
        "Obscuring key element",
        "Proxy element",
        "host",
        "port",
        "uriPath",
        "domain",
        "user",
        "password",
        "passwordObfuscated",
        "Log element",
        "level",
        "auditLog",
        "console",
        "directory",
        "fileName",
        "Application element (required)",
        "name",
        "disableSamplers",
        "Data transmission element",
        "putForDataSend",
        "Host name",
        "Set using config file",
        "Set using environment variable",
        "Cloud platform utilization",
        "detectAws",
        "detectAzure",
        "detectGcp",
        "detectPcf",
        "detectDocker",
        "detectKubernetes",
        "Instrumentation options",
        "Instrumentation element",
        "Applications element (instrumentation)",
        "Attributes element",
        "enabled",
        "include",
        "exclude",
        "Feature options",
        "App pools",
        "defaultBehavior",
        "applicationPool",
        "Cross application traces",
        "Error collection",
        "Tip",
        "captureEvents",
        "maxEventSamplesStored",
        "ignoreClasses",
        "ignoreMessages",
        "ignoreErrors (obsolete)",
        "ignoreStatusCodes",
        "expectedClasses",
        "expectedMessages",
        "expectedStatusCodes",
        "attributes",
        "High security mode",
        "Strip exception messages",
        "Transaction events",
        "maximumSamplesStored",
        "Custom events",
        "Custom parameters",
        "Labels (tags)",
        "Browser instrumentation",
        "autoInstrument",
        "requestPathsExcluded",
        "Slow queries",
        "Transaction traces",
        "transactionThreshold",
        "recordSql",
        "explainEnabled",
        "explainThreshold",
        "maxSegments",
        "maxExplainPlans",
        "maxStackTrace",
        "Datastore tracer",
        "instanceReporting",
        "databaseNameReporting",
        "queryParameters",
        "Distributed tracing",
        "excludeNewrelicHeader",
        "Disable span events via config file",
        "Disable span events via environment variable",
        "Infinite Tracing",
        "trace_observer",
        "Span events",
        "Capture HTTP Request Headers",
        "Settings in app.config or web.config",
        "Enable and disable the agent",
        "Application name",
        "License key",
        "Change newrelic.config location",
        "Settings in appsettings.json"
      ],
      "title": ".NET agent configuration",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Configuration"
      ],
      "external_id": "fd9643d4551ea4fd76f3275edc05251e6dc52f5c",
      "image": "https://docs.newrelic.com/static/cffd7eb2d22c8e338531c38f35208c7c/c1b63/net-agent-config-settings-precedence_0.png",
      "url": "https://docs.newrelic.com/docs/apm/agents/net-agent/configuration/net-agent-configuration/",
      "published_at": "2022-01-08T06:49:16Z",
      "updated_at": "2022-01-08T06:49:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You'll be able to configure our .NET agent to suit your environnment after you create a New Relic account (it's free, forever) and install the .NET agent. Configuration overview APM agent configuration options allow you to control some aspects of how the agent behaves. Some of these config options are part of the basic install process (like setting your license key and app name), but most are more advanced settings, such as setting a log level, setting up proxy host access, excluding certain attributes, and enabling distributed tracing. The .NET agent gets its configuration from the newrelic.config file, which is generated as part of the install process. By default, only a global newrelic.config file is created, but you can also create app-local newrelic.config files for finer control over a multi-app system. Other ways to set config options include: using environment variables, or setting server-side configuration from the UI. For more on the various config options and what overrides what, see Config settings precedence. Support for both .NET Framework and .NET Core use the same configuration options and have the same APM features, unless otherwise stated. If you make changes to the config file and want to validate that it's in the right format, you can check it against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET Agent\\newrelic.xsd for Windows) with any XSD validator. Important For IIS: after you change your newrelic.config or app.config file, perform an IISRESET from an administrative command prompt. Log level adjustments do not require a reset. Configuration methods and precedence levels Upon installation, the .NET agent's configuration file (newrelic.config) applies to all monitored applications, but you can configure the agent in other ways. Here's a diagram showing how different configuration options take precedence over one another: This diagram explains the order of precedence for different ways you might configure the .NET agent. Here are details about the configuration methods shown in the diagram, and their precedence levels: .NET configuration Details and precedence web.config or app.config or appsettings.json Configuration settings set in these files take highest precedence. However, if the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled settings in these files will be ignored. Environment variables Second-highest precedence. For more about these, see .NET environment variables. Server-side configuration Third-highest precedence. A limited number of server-side configuration settings are available; the other settings will come from other configuration sources. App-local newrelic.config Fourth-highest precedence. You can create app-local newrelic.config files to configure individual apps on a multi-app system. These local configuration files override settings in the global newrelic.config file. The agent looks for app-local config files in the following directories, in this order: A directory specified in your web.config or app.config file with the NewRelic.ConfigFile property The web app's root directory (with the app.config or web.config) The directory containing your app's executable file Note that the app-local config file must be complete and validate against the XSD file (for example, at C:\\ProgramData\\New Relic\\.NET Agent\\newrelic.xsd for Windows). Default (global) newrelic.config Default source and the lowest precedence. Will configure all applications on a host in the absence of other config files. The global config file is located in the New Relic agent home directory: %PROGRAMDATA%\\New Relic\\.NET Agent Required environment variables New Relic's .NET agent relies on environment variables to tell the .NET Common Language Runtime (CLR) to attach New Relic to your processes. Some .NET agent install procedures (like the MSI installer) will automatically set these variables for you; some procedures will require you to manually set them. Caution Security recommendation: You should consider what users can set system environment variables. You should also secure the accounts under which your applications execute to prevent user environment variables overriding system environment variables .NET Framework environment variables For .NET Framework, the following variables are required: COR_ENABLE_PROFILING=1 COR_PROFILER={71DA0A04-7777-4EC6-9643-7D28B46A8A41} NEWRELIC_INSTALL_PATH=path\\to\\agent\\directory Copy The .NET agent installer will add these to IIS or as system-wide environment variables. .NET Core environment variables For .NET Core, the following variables are required: Linux: CORECLR_ENABLE_PROFILING=1 CORECLR_PROFILER={36032161-FFC0-4B61-B559-F6C5D41BAE5A} CORECLR_NEWRELIC_HOME=path/to/agent/directory CORECLR_PROFILER_PATH=\"${CORECLR_NEWRELIC_HOME}/libNewRelicProfiler.so\" Copy Windows: CORECLR_ENABLE_PROFILING=1 CORECLR_PROFILER={36032161-FFC0-4B61-B559-F6C5D41BAE5A} NEWRELIC_INSTALL_PATH=path\\to\\agent\\directory CORECLR_NEWRELIC_HOME=path\\to\\agent\\directory Copy The .NET agent installer will add these to IIS or as system-wide environment variables. If your system has previously used monitoring services (non-New Relic), you may have a \"profiler conflict\" when trying to install and use the New Relic agent. More details: Profiler conflict explanation New Relic’s .NET agents rely on environment variables to tell the .NET Common Language Runtime (CLR) to load New Relic into your processes. The install-related environment variables are Microsoft variables, not New Relic variables. They can be used by other .NET profilers, and only one profiler can be attached to a process at a time. For this reason, if you have used previous application monitoring products, you may have profiler conflicts. For specific install instructions, see the .NET agent install documentation. Optional environment variables Some configuration options in New Relic's .NET agent can be set via environment variables as an alternative to setting them in a config file. Below is a list of environment variables recognized by the .NET agent with example values. NEW_RELIC_LICENSE_KEY=XXXXXXXX NEW_RELIC_LOG=MyApp.log NEW_RELIC_APP_NAME=Descriptive Name MAX_TRANSACTION_SAMPLES_STORED=500 MAX_EVENT_SAMPLES_STORED=500 NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_SPAN_EVENTS_ENABLED=false NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED=2000 NEW_RELIC_LABELS=foo:bar;zip:zap NEW_RELIC_CONFIG_OBSCURING_KEY=XXXXXXXX NEW_RELIC_DISABLE_SAMPLERS=true NEWRELIC_PROFILER_LOG_DIRECTORY=path\\to\\a\\directory (not configurable via config file) NEWRELIC_LOG_DIRECTORY=path\\to\\a\\directory (Insert a directory where you want to put the agent and profiler logs. You can't set this directory for both agent and profiler logs in the configuration file.) NEWRELIC_LOG_LEVEL=off|error|warn|info|debug|finest|all Copy If you're using New Relic APM and CodeStream, see how to associate repositories and how to associate build SHAs or release tags with errors inbox. Setup options, newrelic.config Use these options to setup and configure your agent via the newrelic.config file. The New Relic .NET agent supports the following categories of setup options: Configuration element Service element Obscuring key element Proxy element Log element Application element (configuration) Data transmission element Host name Configuration element The root element of the configuration document is a configuration element. <configuration xmlns=\"urn:newrelic-config\" agentEnabled=\"true\" maxStackTraceLines=\"50\" timingPrecision=\"low\"> Copy The configuration element supports the following attributes: agentEnabled Type Boolean Default true Enable or disable the New Relic agent. maxStackTraceLines Type Integer Default 80 The maximum number of stack frames to trace in any stack dump. timingPrecision Type String Default low Controls the precision of the timers. High precision will provide better data, but at a lower execution speed. Possible values are high and low. Service element The first child of the configuration element is a service element. The service element configures the agent's connection to the New Relic service. <service licenseKey=\"YOUR_LICENSE_KEY\" sendEnvironmentInfo=\"true\" syncStartup=\"false\" sendDataOnExit=\"false\" sendDataOnExitThreshold=\"60000\" autoStart=\"true\"/> Copy The service element supports the following attributes: licenseKey (required) Type String Default (none) Your New Relic license key. New Relic uses the license key to match your app's data to the correct account in the UI. Set the license key via environment variable. Alternatively, set the NEW_RELIC_LICENSE_KEY environment variable in the application's environment. NEW_RELIC_LICENSE_KEY=XXXXXXXX Copy sendEnvironmentInfo Type Boolean Default true Instructs the agent to record execution environment information. Environment information includes operating system, agent version, and which assemblies are available. syncStartup Type Boolean Default false Block application startup until the agent connects to New Relic. If set to true, the first transaction may take substantially longer to complete, because it is blocked until the connection to New Relic is finished. sendDataOnExit Type Boolean Default false Block application shutdown while the agent initiates a final harvest cycle and sends all data to New Relic. sendDataOnExitThreshold Type Integer Default 60000 Unit Milliseconds The minimum amount of time the process must run before the agent blocks it from shutting down. This setting only applies when sendDataOnExit is true. completeTransactionsOnThread Type Boolean Default false If false, the agent uses a pool thread to complete the transaction processing. If true, the agent will complete transaction processing on the request thread. requestTimeout Type Integer Default 2000 (sendDataOnExit enabled) 120000 (sendDataOnExit disabled) Unit Milliseconds The agent's request timeout when communicating with New Relic. autoStart Type Boolean Default True Automatically start the .NET agent when the first instrumented method is hit. Obscuring key element The obscuringKey element is an optional child of the service element. The .NET Agent uses this value to deobfuscate supported configuration values. For example, when an obfuscated proxy password is supplied, it will be deobfuscated using this key. <service licenseKey=\"YOUR_LICENSE_KEY\"> <obscuringKey>OBSCURING_KEY</obscuringKey> </service> Copy The obscuring key may also be configured by setting the NEW_RELIC_CONFIG_OBSCURING_KEY environment variable. Caution Security recommendation: The placement of the obscuring Key in the same configuration file as an obfuscated value may pose a security risk. Consider placing the obscuring key in an environment variable and limiting access to environment variables within your environment. Proxy element The proxy element is an optional child of the service element. The proxy element is used when the agent communicates to the New Relic back-end service via a proxy. <service licenseKey=\"YOUR_LICENSE_KEY\"> <proxy host=\"hostname\" port=\"PROXY_PORT\" uriPath=\"path/to/something.aspx\" domain=\"mydomain.com\" user=\"PROXY_USERNAME\" password=\"PROXY_PASSWORD\" passwordObfuscated=\"OBFUSCATED_PROXY_PASSWORD\"/> </service> Copy The proxy element supports the following attributes: host Type String Default (none) Defines the proxy host. port Type Integer Default 8080 Defines the proxy port. uriPath Type String Default (none) Optionally define a proxy URI path. domain Type String Default (none) Optionally define a domain to use when authenticating with the proxy server. user Type String Default (none) Optionally define a user name for authentication. password Type String Default (none) Optionally define a password for authentication. passwordObfuscated Type String Default (none) For additional security, the .NET Agent supports the use of an obfuscated proxy password with the passwordObfuscated attribute. The obfuscated proxy password is generated using the following New Relic CLI command: newrelic agent config obfuscate --key OBSCURING_KEY --value \"CLEAR_TEXT_PROXY_PASSWORD\" Copy Important When using an obfuscated proxy password, the obscuring key must also be configured. Log element The log element is a child of the configuration element. The log element configures New Relic's logging . The agent generates its own log file to keep its logging information separate from your application's logs. <log level=\"info\" auditLog=\"false\" console=\"false\" directory=\"PATH\\TO\\LOG\\DIRECTORY\" fileName=\"FILENAME.log\" /> Copy The log element supports the following attributes: level Type String Default info Defines the level of detail recorded in the log file. Possible values, in increasing order of detail, are: off error warn info debug finest all Alternatively, set the NEWRELIC_LOG_LEVEL environment variable in the application's environment. Important Increasing the log level will increase New Relic's performance impact. auditLog Type Boolean Default false Records all data sent to and received from New Relic in both an auditlog log file and the standard log file. console Type Boolean Default false Send log messages to the console, in addition to the log file. directory Type String Default C:\\ProgramData\\New Relic\\.NET Agent\\Logs The directory to hold log files generated by the agent. If this is omitted, then a directory named logs in the New Relic agent install area will be used by default. fileName Type String Default (none) Defines a name for the log file. If you do not define a fileName, the name is derived from the name of the monitored process. Alternatively, set the NEW_RELIC_LOG environment variable in the application's environment. NEW_RELIC_LOG=MyApp.log Copy Application element (required) The application element is a child of the configuration element. This required element defines your application name, and disables or enables sampling. name Type String Default My Application The name of your .NET application is a child of the application element. New Relic will aggregate your data according to this name. For example, if you have two running applications named AppA and AppB, you will see two applications in the New Relic interface: AppA and AppB. You can also assign up to three names to your app. The first name is the primary name. For example: <application> <name>MY APPLICATION PRIMARY</name> <name>SECOND APP NAME</name> <name>THIRD APP NAME</name> </application> Copy Alternatively, set the NEW_RELIC_APP_NAME environment variable in the application's environment. NEW_RELIC_APP_NAME=Descriptive Name Copy disableSamplers Type Boolean Default false Samplers collect information about memory and CPU consumption. Set this to true to disable sampling. Alternatively, set the NEW_RELIC_DISABLE_SAMPLERS environment variable in the application's environment. NEW_RELIC_DISABLE_SAMPLERS=true Copy Data transmission element The dataTransmission element is a child of the configuration element. This element affects how data is sent to New Relic and can be used if you have specific data transmission requirements. <dataTransmission putForDataSend=\"false\" compressedContentEncoding=\"deflate\"/> Copy The dataTransmission element supports the following attributes: putForDataSend Type Boolean Default false Defines the HTTP method used when sending data to New Relic. Set this to true to enable using the PUT method when sending data. The POST method is used by default. Host name If the default host name label in the APM UI is not useful, you can decorate that name in the New Relic UI with a display name. After the application process is restarted and the .NET agent is reporting again, the display name will appear in the Servers drop-down list. This host name setting does not affect the list of hosts on your application's Summary page. To set a display name, choose one of the following options. The environment variable takes precedence over the config file value. Then restart your application to see your changes in the New Relic UI. Set using config file Set the displayName attribute in the processHost element in newrelic.config. The processHost element is a child of the configuration element. <configuration . . . > <processHost displayName=\"CUSTOM_NAME\" /> </configuration> Copy Set using environment variable Set the NEW_RELIC_PROCESS_HOST_DISPLAY_NAME environment variable: NEW_RELIC_PROCESS_HOST_DISPLAY_NAME = \"CUSTOM_NAME\" Copy Cloud platform utilization Configures the utilization configuration element to control how the agent collects utilization information and sends it to the New Relic service to determine pricing. The agent can collect information from Amazon Web Services (AWS) EC2 instances, Docker containers, Azure, Google Cloud Platform, Pivotal Cloud Foundry, and Kubernetes. detectAws Type Boolean Default true Determines whether the agent polls AWS metadata API. detectAzure Type Boolean Default true Determines whether the agent polls Azure metadata API. detectGcp Type Boolean Default true Determines whether the agent polls GCP metadata API. detectPcf Type Boolean Default true Determines whether the agent polls PCF information from environment variables. detectDocker Type Boolean Default true Determines whether the agent reads Docker information from the file system. detectKubernetes Type Boolean Default true Determines whether the agent polls Kubernetes information from environment variables. Instrumentation options Use these options to configure which elements of your application and environment to instrument. New Relic for .NET supports the following categories of instrumentation options: Instrumentation element Applications element (instrumentation) Attributes element Instrumentation element The instrumentation element is a child of the configuration element. By default, the .NET agent instruments IIS asp worker processes and Azure web and worker roles. To instrument other processes, see Instrumenting custom applications. Applications element (instrumentation) The applications element is a child of the instrumentation element. The applications element specifies which non-web apps to instrument. It contains a name attribute. Important This is not the same as the application (configuration) element, which is a child of the configuration element. <instrumentation> <applications> <application name=\"MyService1.exe\" /> <application name=\"MyService2.exe\" /> <application name=\"MyService3.exe\" /> </applications> </instrumentation> Copy Attributes element An attribute is a key/value pair that determines the properties of an event or transaction. Each attribute is sent to APM transaction traces, APM error traces, Transaction events, TransactionError events, or PageView events. The primary attributes element enables or disables attribute collection for the .NET agent, and defines specific attributes to collect or exclude. You can also configure attribute settings based on their destination: Error collection, transaction traces, browser instrumentation, and transaction events. In this example, the agent excludes all attributes whose key begins with myApiKey (myApiKey.bar, myApiKey.value), but collects the custom attribute myApiKey.foo. <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> Copy You can view the .NET APM attributes on the .NET agent attributes page. You can also define custom attributes with the agent API call AddCustomAttribute. enabled Type Boolean Default true Enable or disable attribute collection. When set to false in the primary attribute element, this setting overrides all attribute settings for individual destinations. include Type String Default (none) If attributes are enabled, the agent will collect all attribute keys specified in this list. To specify multiple attribute keys, specify each individually. You can also use a * wildcard character at the end of a key to match multiple attributes (for example, myApiKey.*). For more information, see Attribute rules. exclude Type String Default (none) If attributes are enabled, the agent will not collect attribute keys specified in this list. To specify multiple attribute keys, specify each individually. You can also use a * wildcard character at the end of a key to match multiple attributes (for example, myApiKey.*). For more information, see Attribute rules. Feature options Use these options to enable, disable, and configure New Relic features. New Relic for .NET allows you to configure the following features: App pools Cross application traces Error collection High security mode Strip exception messages Transaction events Custom events Custom parameters Tags/labels Browser instrumentation Slow Queries Transaction traces Datastore tracer Distributed tracing Span events Capture HTTP Request Headers App pools Important This is only applicable to a system's global config file. The applicationPools element is a child of the configuration element. The applicationPools element specifies for the profiler exactly which application pools to instrument and uses the same name as the IIS application pool name. This configuration element is useful when you may need to instrument only a small subset of your app pools. For example, a given server might have several hundred application pools, but only a few of those pools need to be instrumented by the .NET agent. Here is an example of disabling instrumentation for specific application pools: <applicationPools> <applicationPool name=\"Foo\" instrument=\"false\"/> <applicationPool name=\"Bar\" instrument=\"false\"/> </applicationPools> Copy Here is an example of disabling instrumentation for all application pools currently executing on the server and enabling instrumentation for specific application pools: <applicationPools> <defaultBehavior instrument=\"false\"/> <applicationPool name=\"Foo\" instrument=\"true\"/> <applicationPool name=\"Bar\" instrument=\"true\"/> </applicationPools> Copy The applicationPools element supports the following elements: defaultBehavior Type Boolean Default false Defines how the .NET agent will behave on a \"global\" level for application pools served via IIS. The .NET agent instruments all application pools by default. When true, application pools listed under applicationPool with an instrument attribute set to false will not be instrumented. Essentially, when set to false, the application pool list acts as an allow list. When set to true, the application pool list acts as a deny list. applicationPool Defines instrumentation behavior for a specific application pool. The name attribute is the name of an application pool. Enable or disable profiling in the instrument attribute. Define this application in the name attribute. Cross application traces The crossApplicationTracer element is a child of the configuration element. crossApplicationTracer links transaction traces across applications. When linked in a service-oriented architecture, all instrumented applications that communicate with each other via HTTP will now \"link\" transaction traces with the applications that they call and the applications they are called by. Cross application tracing makes it easier to understand the performance relationship between services and applications. <crossApplicationTracer enabled=\"true\"/> Copy The crossApplicationTracer element supports the following attribute: enabled Type Boolean Default true Enable or disable cross application tracing Error collection The errorCollector element is a child of the configuration element. errorCollector configures error collection, which captures information about uncaught exceptions and sends them to New Relic. <errorCollector enabled=\"true\" captureEvents=\"true\" maxEventSamplesStored=\"100\"> <ignoreClasses> <errorClass>System.IO.FileNotFoundException</errorClass> <errorClass>System.Threading.ThreadAbortException</errorClass> </ignoreClasses> <ignoreMessages> <errorClass name=\"System.Exception\"> <message>Ignore message</message> <message>Ignore too</message> </errorClass> </ignoreMessages> <ignoreStatusCodes> <code>401</code> <code>404</code> </ignoreStatusCodes> <expectedClasses> <errorClass>System.ArgumentNullException</errorClass> <errorClass>System.ArgumentOutOfRangeException</errorClass> </expectedClasses> <expectedMessages> <errorClass name=\"System.Exception\"> <message>Expected message</message> <message>Expected too</message> </errorClass> </expectedMessages> <expectedStatusCodes>403,500-505</expectedStatusCodes> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </errorCollector> Copy Tip For an overview of error configuration in APM, see Manage errors in APM. Important expectedClasses, expectedMessages, and expectedStatusCodes configuration settings require .NET agent version 8.31.0.0 or higher. The errorCollector element supports the following elements and attributes: enabled Type Boolean Default true Enable or disable the error collector. captureEvents Type Boolean Default true Enable or disable the capturing of error events. maxEventSamplesStored Type Integer Default 100 Reservoir limit for error events. ignoreClasses A list of fully qualified class names to be ignored. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used ignoreMessages An optional map of fully qualified class names to list of strings matching a substring of the message of an error. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used ignoreErrors (obsolete) Type String Default (none) Lists specific exceptions to not report to New Relic. The full name of the exception should be used, such as System.IO.FileNotFoundException. ignoreStatusCodes Type String Default (none) Lists specific HTTP error codes to not report to New Relic. You can use standard integral HTTP error codes, such as just 401, or you may use Microsoft full status codes with decimal points, such as 401.4 or 403.18. The status codes should be equal to or greater than 400. expectedClasses A list of fully qualified class names to be marked as expected. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used expectedMessages An optional map of fully qualified class names to list of strings matching a substring of the message of an error. The maximum number of error class and message combinations that SHOULD be reported is 50. If more than 50 are listed, then only the first 50 SHOULD be used expectedStatusCodes A comma separated list of status codes. The list may include integer ranges, using a single dash (-) and will be inclusive of both the starting and ending integer in the range. attributes Use this sub-element to customize your agent attribute settings for error traces. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. High security mode The highSecurity element is a child of the configuration element. To enable high security mode, set this property to true and enable the high security property in the New Relic user interface. Enabling high security turns SSL on; request parameters, custom parameters and HTTP request headers are not collected; strip exception messages is enabled; and queries can't be sent to New Relic in their raw form. enabled Type Boolean Default false Enable or disable high security mode. Example: <highSecurity enabled=\"true\"/> Copy Strip exception messages The stripExceptionMessages element is a child of the configuration element. To enable strip exception messages, set this property to true. By default, this is set to false, which means that the agent sends messages from all exceptions to the New Relic collector. If you enable high security mode, this is automatically changed to true, and the agent strips the messages from exceptions. enabled Type Boolean Default false Enable or disable strip exception messages. Example: <stripExceptionMessages enabled=\"true\"/> Copy Transaction events The transactionEvents element is a child of the configuration element. Use transactionEvents to configure transaction events. <transactionEvents enabled=\"true\" maximumSamplesStored=\"10000\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </transactionEvents> Copy The transactionEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Integer Default 10000 The maximum number of samples to store in memory at once. Alternatively, set the MAX_TRANSACTION_SAMPLES_STORED environment variable in the application's environment. MAX_TRANSACTION_SAMPLES_STORED=500 Copy attributes Use this sub-element to customize your agent attribute settings for transaction events. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Tip These attribute settings are specific to transaction events. Attribute settings can be applied globally to all event types to with this configuration setting. Caution When distributed tracing and/or Infinite Tracing are enabled, information from transaction events is applied to the root Span Event of the transaction. Consider applying any attribute settings for transaction events to span events and/or apply them as Global Attribute settings. Custom events The customEvents element is a child of the configuration element. Use customEvents to configure custom events. <customEvents enabled=\"true\" maximumSamplesStored=\"10000\"/> Copy The CustomEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Integer Default 10000 The maximum number of samples to store in memory at once. Alternatively, set the MAX_EVENT_SAMPLES_STORED environment variable in the application's environment. MAX_EVENT_SAMPLES_STORED=500 Copy Custom parameters The customParameters element is a child of the configuration element. Use customParameters to configure custom parameters. <customParameters enabled=\"true\" /> Copy The CustomParameters element supports the following attributes: enabled Type Boolean Default true Enable or disable the capture of custom parameters. Labels (tags) The labels element is a child of the configuration element. This sets tag names and values. The list is a semicolon delimited list of colon-separated name and value pairs. You can also use with the NEW_RELIC_LABELS environment variable. Example: <labels>foo:bar;zip:zap</labels> Copy Browser instrumentation The browserMonitoring element is a child of the configuration element. browserMonitoring configures browser monitoring in your .NET application. Browser gives you insight your end users' performance experience. This is accomplished by measuring the time it takes for your users' browsers to download and render your webpages by injecting a small amount of JavaScript code into the header and footer of each page. // If you use both the Exclude and Attribute elements // the Exclude element must be listed first. <browserMonitoring autoInstrument=\"true\"> <requestPathsExcluded> <path regex=\"url-regex-1\"/> <path regex=\"url-regex-2\"/> ... <path regex=\"url-regex-n\"/> </requestPathsExcluded> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </browserMonitoring> Copy The browserMonitoring element supports the following attributes: autoInstrument Type Boolean Default true By default the agent automatically injects the browser agent JavaScript. To turn off automatic injection, set this attribute to false. attributes Use this sub-element to customize your agent attribute settings for browser monitoring. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. requestPathsExcluded Use this sub-element to prevent the browser agent from being injected in specific pages. The element is used as follows: <requestPathsExcluded> <path regex=\"url-regex-1\"/> <path regex=\"url-regex-2\"/> ... <path regex=\"url-regex-n\"/> </requestPathsExcluded> Copy The agent will not inject the browser agent into pages whose URL matches one of the specified regular expressions. The regular expression should follow Microsoft guidelines for the Regex class. It is a reference to the virtual directory of the path in your application and not the full URL of the path you wish to exclude. For example, to exclude the pages in https://www.mywebsite.com/mywebpages/ you would simply insert /mywebpages/ as the path regex value. The requestPathsExcluded element should be used in cases where it is impossible or undesirable to use the DisableBrowserMonitoring() call. To minimize a possible performance impact try to use as few regular expressions as possible and keep them as simple as possible. Slow queries The slowSql element is a child of the configuration element. slowSql configures capturing information about slow query executions, and captures and obfuscates explain plans for these queries. <slowSql enabled=\"true\"/> Copy The slowSql element supports the following attribute: enabled Type Boolean Default true Enable or disable slow query tracing. Transaction traces The transactionTracer element is a child of the configuration element. transactionTracer configures transaction traces. Included in the trace is the exact call sequence of the transactions, including any query statements issued. <transactionTracer enabled=\"true\" transactionThreshold=\"apdex_f\" recordSql=\"obfuscated\" explainEnabled=\"true\" explainThreshold=\"500\" maxSegments=\"3000\" maxExplainPlans=\"20\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </transactionTracer> Copy The transactionTracer element supports the following attributes: enabled Type Boolean Default true Enable or disable transaction traces. transactionThreshold Type String Default apdex_f Defines the threshold for transaction traces. If a transaction takes longer than the threshold, it is eligible for being traced. See transaction trace basics for more about the rules governing traces. The default value is apdex_f, which sets the threshold to four times the application's apdex_t value. For more information about apdex_t, see Apdex. You can also set the threshold to be a specific time value in milliseconds. recordSql Type String Default obfuscated Select a query tracing policy. Options are off, which records nothing; obfuscated, which records an obfuscated version of the query; or raw, which records the query exactly as it is issued to the database. Caution Recording raw queries may capture sensitive information. explainEnabled Type Boolean Default false When true, the agent captures EXPLAIN statements for slow queries. explainThreshold Type Integer Default 500 Unit Milliseconds The agent collects slow query data for queries that exceed this threshold, along with any available explain plans, as part of transaction traces. maxSegments Type Integer Default 3000 The maximum number of segments to collect in a transaction trace. maxExplainPlans Type Integer Default 20 The maximum number of explain plans to collect during a harvest cycle. maxStackTrace Type Integer Default 0 By default maxStackTrace is set to 0, which disables stack traces as part of a transaction trace. If this value is set greater than 0, then stack traces will be captured for transaction traces. attributes Use this sub-element to customize your agent attribute settings for transaction traces. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Datastore tracer The datastoreTracer element is a child of the configuration element. <datastoreTracer> <instanceReporting enabled=\"true\" /> <databaseNameReporting enabled=\"true\" /> <queryParameters enabled=\"false\" /> </datastoreTracer> Copy The datastoreTracer element supports the following sub-elements: instanceReporting Use this sub-element to enable collection of datastore instance metrics (such as the host and port) for some database drivers. These are reported on slow query traces and transaction traces. The default value of attribute enabled is true. databaseNameReporting Use this sub-element to enable collection of the database name on slow query traces and transaction traces for some database drivers. The default value of attribute enabled is true. queryParameters Use this sub-element to enable collection of the SQL query parameters on slow query traces. The default value of attribute enabled is false. Caution Recording query parameters may capture sensitive information. The transactionTracer.recordSql configuration option must be set to raw or this option is ignored. Distributed tracing The distributedTracing element is a child of the configuration element. <distributedTracing enabled=\"false\" excludeNewrelicHeader=\"false\"/> Copy Distributed tracing lets you see the path that a request takes as it travels through a distributed system. Enabling distributed tracing disables cross application tracing, and has other effects on APM features. Before enabling, read the planning guide. Important Requires .NET agent version 8.6.45.0 or higher. The distributedTracing element supports the following attributes: To enable or disable, see Enable distributed tracing. enabled Type Boolean Default false Alternatively, enable distributed tracing via the NEW_RELIC_DISTRIBUTED_TRACING_ENABLED environment variable in the application's environment. NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy excludeNewrelicHeader Type Boolean Default false By default, supported versions of the agent utilize both the newrelic header and W3C Trace Context headers for distributed tracing. The newrelic distributed tracing header allows interoperability with older agents that don't support W3C Trace Context headers. Agent versions that support W3C Trace Context headers will prioritize them over newrelic headers for distributed tracing. If you do not want to utilize the newrelic header, setting this to true will result in the agent excluding the newrelic header and only using W3C Trace Context headers for distributed tracing. Distributed tracing reports span events. Span event reporting is enabled by default, but distributed tracing must be enabled for spans to be reported. To disable span events, choose one of the following options: Disable span events via config file Set the <spanEvents> element to false to disable via the newrelic.config file. This element is a child of the <configuration> element. <configuration . . . > <spanEvents enabled=\"false\" /> </configuration> Copy Disable span events via environment variable Set the NEW_RELIC_SPAN_EVENTS_ENABLED environment variable in the application's environment. NEW_RELIC_SPAN_EVENTS_ENABLED=false Copy Infinite Tracing Infinite Tracing extends the distributed tracing service by employing a trace observer that is external to the agent. It observes 100% of your application traces across various services and provides actionable data so you can solve issues faster. To turn on Infinite Tracing, make sure you have .NET agent version 8.30 or higher, and enable distributed tracing. Then add the following additional settings: <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Important Infinite Tracing spans can be limited by the transactionTracer.maxSegments setting. The infiniteTracing element supports the following elements: trace_observer The trace_observer element identifies an observer host that is independent from the agent. For help getting a valid Infinite Tracing trace observer host entry, see Find or create a trace observer endpoint. The trace observer may be configured using the NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST environment variable as well. Important When configuring the trace observer, you should not supply the protocol as part of the host. For example, use myhost.infinitetracing.com instead of https://myhost.infinitetracing.com. Span events The spanEvents element is a child of the configuration element. Use spanEvents to configure span events. <spanEvents enabled=\"true\"> <attributes enabled=\"true\"> <exclude>myApiKey.*</exclude> <include>myApiKey.foo</include> </attributes> </spanEvents> Copy The spanEvents element supports the following attributes: enabled Type Boolean Default true Enable or disable the event recorder. maximumSamplesStored Type Int Default 2000 The maximum number of samples to store in memory at a time. This may be configured using the NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED environment variable as well. Important This configuration option is only available in the .NET Agent v9.0 or higher. attributes Use this sub-element to customize your agent attribute settings for span events. This sub-element uses the same settings as the primary attributes element: enabled, include, and exclude. Tip These attribute settings are specific to span events. Attribute settings may be applied globally to all event types to with this configuration setting. Capture HTTP Request Headers The allowAllHeaders element is a child of the configuration element. Set this to true to allow the .NET Agent to capture all HTTP request headers as request.headers.{http-header-name} attributes. Set this to false to only allow the .NET agent to collect the following HTTP request headers: request.headers.referer request.headers.accept request.headers.content-length request.headers.host request.headers.user-agent Copy enabled Type Boolean Default false Enable or disable HTTP request headers capture. Example: <allowAllHeaders enabled=\"true\" /> <attributes enabled=\"true\"> <include>request.headers.*</include> </attributes> Copy Important The allowAllHeaders setting is only available in the .NET Agent version 8.40.0+. When using allowAllHeaders to capture attributes, the captured request header attributes are still being controlled by the root level and destination level attributes settings. Without setting the request.header.* in the include list under the attributes element (see the following), the .NET Agent still filters out all header attributes. The default newrelic.config is set to include the request.header.*. <allowAllHeaders enabled=\"true\" /> <attributes enabled=\"true\"> <include>request.headers.*</include> ... </attributes> Copy The default newrelic.config is also set to explicitly exclude the following HTTP request headers to prevent the .NET Agent collecting unwanted data. <attributes enabled=\"true\"> <exclude>request.headers.cookie</exclude> <exclude>request.headers.authorization</exclude> <exclude>request.headers.proxy-authorization</exclude> <exclude>request.headers.x-*</exclude> </attributes> Copy Settings in app.config or web.config For ASP.NET and .NET Framework console apps you can also configure the following settings in your app's app.config or web.config, within the outermost element, <configuration>: Enable and disable the agent <appSettings> <add key = \"NewRelic.AgentEnabled\" value=\"false\" /> </appSettings> Copy Important If the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled settings in these files will be ignored. Application name For more information, see Name your .NET application. <appSettings> <add key = \"NewRelic.AppName\" value =\"Descriptive Name\" /> </appSettings> Copy License key <appSettings> <add key = \"NewRelic.LicenseKey\" value =\"XXXXXXXX\" /> </appSettings> Copy Change newrelic.config location Designates an alternative location for the config file outside of the local root of the app or global config location. The location entered must be an absolute path. <appSettings> <add key = \"NewRelic.ConfigFile\" value=\"C:\\Path-to-alternate-config-dir\\newrelic.config\" /> </appSettings> Copy Settings in appsettings.json For .NET Core apps, you can configure the following settings in appsettings.json if the following is true: The appsettings.json file must be located in the current working directory of the application. The application must have the following dependencies: Microsoft.Extensions.Configuration Microsoft.Extensions.Configuration.Json Microsoft.Extensions.Configuration.EnvironmentVariables Enable and disable the agent { \"NewRelic.AgentEnabled\":\"false\" } Copy Important If the agent is disabled in the local or global newrelic.config, the NewRelic.AgentEnabled setting in this file will be ignored. Application name For more information, see Name your .NET application. { \"NewRelic.AppName\": \"Descriptive Name\" } Copy License key { \"NewRelic.LicenseKey\": \"XXXXXXXX\" } Copy Change newrelic.config location Designates an alternative location for the config file outside of the local root of the app or global config location. The location entered must be an absolute path. { \"NewRelic.ConfigFile\": \"C:\\\\Path-to-alternate-config-dir\\\\newrelic.config\" } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 145.10414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".<em>NET</em> agent configuration",
        "sections": ".<em>NET</em> agent configuration",
        "tags": "<em>NET</em> agent",
        "body": ", or setting server-side configuration <em>from</em> the <em>UI</em>. For more on the various config options and what overrides what, see Config settings precedence. Support for both .<em>NET</em> Framework and .<em>NET</em> Core use the same configuration options and have the same APM features, unless otherwise stated. If you make changes"
      },
      "id": "617e9587196a67924df7e55e"
    }
  ],
  "/docs/kubernetes-pixie/auto-telemetry-pixie/auto-telemetry-pixie-data-model": [
    {
      "sections": [
        "Auto-telemetry with Pixie for instant Kubernetes observability",
        "Quickly start observing and debugging Kubernetes clusters",
        "Important",
        "Explore your cluster",
        "Tip",
        "Investigate usage spikes using the flame graph",
        "Debug live"
      ],
      "title": "Auto-telemetry with Pixie for instant Kubernetes observability",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "1dc7474aedbf84c51593c152fc39fda0a46b4f48",
      "image": "https://docs.newrelic.com/static/521c1e907520f4bc8da33ad27ab47289/c1b63/flamegraph.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/get-started-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:10:54Z",
      "updated_at": "2021-12-30T09:39:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When we say auto-telemetry, we’re not talking about cars — we're talking about instant baseline visibility into your Kubernetes clusters. With the New Relic One integration with Pixie, you get similar data to traditional language agents, but without manually instrumenting your code or redeploying your application. Pixie auto-telemetry is powered by eBPF, a virtual machine-like construct that enables Pixie to seamlessly collect fine-grained telemetry data — service-level metrics, unsampled requests, and more. With one install command, you get deeper insight into your Kubernetes clusters and workloads. No language agents required. Live debugging with Pixie shows a service graph listing the namespaces and the node that are available on the current cluster. Simply put, Auto-telemetry with Pixie offers the quickest option for getting observability into your Kubernetes services. Ready to get started? You'll be able to configure Pixie to suit your environnment after you create a New Relic account (it's free, forever!) and install our Pixie integration. Quickly start observing and debugging Kubernetes clusters Our Pixie integration gives you the best of both worlds: Pixie’s fast and simple Kubernetes observability coupled with New Relic One’s incident correlation, intelligent alerting, and long-term retention. You’ll get visibility into HTTP services using golden signals, HTTP transactions, database transactions, distributed tracing, and JVM metrics. You can operate, debug, and scale your Kubernetes clusters based on the information you learn about how your clusters and services are running. Using the New Relic Explorer, you can see key metrics and events at every level, starting with the cluster, and diving down into namespaces, deployments, and pods. You can quickly spot anomalous behavior, and where it’s happening. And then dive deeper using embedded visualizations of your Pixie data. Quickly identify hot spots with a flame graph display. On the Live debugging with Pixie tab, answer questions like what SQL requests your app is making or which services are talking to each other. This short video (approx. 6:20 minutes) shows how Pixie and Kubernetes work together in New Relic One so you can debug faster with code-level insights: Important Auto-Telemetry with Pixie leverages Community Cloud with Pixie, a separate platform from New Relic One. Use of Community Cloud with Pixie is subject to separate terms of service. Explore your cluster Access the Pixie UI via New Relic's Live debugging with Pixie area of your Kubernetes clusters. The cluster explorer provides a quick overview of the nodes in your cluster, including CPU, memory, and storage, as well as the status of each pod (healthy, warning, or critical). You can also find out what services are running in each container, their latency, throughput, and error rate. For more information about using the cluster explorer, see Navigate the Kubernetes cluster explorer. Note that you cannot log directly into the Pixie UI unless you have created a separate Pixie login. Tip Containers might be listed for up to four hours after they get decommissioned. You can query the Pixie data in New Relic One and create dashboards for at-a-glance monitoring. For more information, see our documentation about the data model and sample queries. Investigate usage spikes using the flame graph Debugging is orders of magnitude easier when you can quickly see what your application is doing. The Pixie flame graph display requires no instrumentation, redeploying, or recompiling. It works for compiled languages like Go, C+, Rust, to name a few. And at a glance, the flame graph tells you what functions your application is spending time on and where you have hot spots. Flame graphs are especially useful for hierarchical resource use, like disk usage and CPU utilization. For more information on how to read a flame graph, see the Pixie flame graph docs. Debug live On the Live debugging with Pixie tab, run PxL scripts — scripts written in Pixie's PxL language — to view live data captured through eBPF. Select the script drop-down and then select a script to run in the tab. (For best results, select a time range that is recent in the time picker.) Scripts enable you to debug: Traffic in multiple formats: HTTP and HTTPs (including encrypted), DNS, Postgres, MySQL, Cassandra, Redis (currently supporting SQL and HTTP in beta). Learn more: Request tracing tutorial. Database request performance. Learn more: Database Query Profiling tutorial. Service maps to learn which services are talking to each other. Learn more: Service Performance tutorial. Network traffic maps to learn which nodes are talking to each other. Learn more: Network Monitoring tutorial. Monitor resource usage by Node and Pod. Learn more: Infra health tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 508.02475,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "sections": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": " your application. <em>Pixie</em> <em>auto</em>-<em>telemetry</em> is powered by <em>eBPF</em>, a virtual machine-like construct that enables <em>Pixie</em> to seamlessly collect fine-grained <em>telemetry</em> data — <em>service</em>-level metrics, unsampled requests, and more. With one install command, you get deeper insight into your <em>Kubernetes</em> clusters"
      },
      "id": "6174ca7364441f0e385fdea5"
    },
    {
      "sections": [
        "Install Auto-telemetry with Pixie",
        "General prerequisites for using Pixie",
        "Setup steps depend on your account status",
        "Install from the beginning of the guided install process",
        "Install from the Configure the HELM command/manifest (yaml) file",
        "Important",
        "Helm method",
        "manifest method",
        "If you link the wrong Pixie and New Relic account"
      ],
      "title": "Install Auto-telemetry with Pixie",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "bb957763e579e39ef2bbeafeebc46ab3a111bca2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/install-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:49:26Z",
      "updated_at": "2022-01-08T03:49:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To get up and running with Auto-telemetry with Pixie, you start with our guided installation. The guided installation deploys Pixie with New Relic's Kubernetes integration on your cluster. You don't need to do any further configuration or installation to start using Pixie. If you want to install Auto-telemetry with Pixie on multiple clusters, re-run the guided install for each additional cluster. General prerequisites for using Pixie Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! You must be a New Relic full platform user. Other user-related requirements: Users on our New Relic One user model must be assigned to a group that has a role with Pixie-related capabilities. Users on our original user model cannot be Restricted. In addition: Review this Pixie data security overview for actions to take to secure your data. Make sure you have sufficient memory. Pixie requires at least 2Gb of memory on each node in your cluster. More memory may be required for larger clusters. Pixie needs to run in privileged mode. Review the other Pixie technical requirements. Setup steps depend on your account status Use the following table to find out where to start installing Auto-telemetry with Pixie. Where you start the installation depends on whether you already have a New Relic or Pixie account, or both. New Relic Pixie Next steps Start the guided install at the beginning of the process. If you already have both types of accounts, and used the same email address for each of them, click the New Relic icon in the Pixie UI. This brings you to the Configure the HELM command/manifest (yaml) file section of the guided installation. Then, follow the steps. If you're using different email addresses in Pixie and New Relic, create a new account for either Pixie or New Relic to match email addresses across both products. You can also contact New Relic support to manually link your existing New Relic account with your Pixie account. If you follow a link to New Relic from the Pixie UI and do not have a New Relic account, you must first create one. Click the New Relic icon in the Pixie UI, and follow the steps to create a New Relic account. When you do so, your Pixie account is linked to it. Then, continue the guided install process with these steps. Sign up for a free New Relic account. Then, start the guided install at the beginning of the process. Install from the beginning of the guided install process Open our New Relic One guided install. Select the account you want to use for the guided install, and click Continue. Note: if you have a single account, you won't see this option. Select Kubernetes and then continue with step one in the next section. Install from the Configure the HELM command/manifest (yaml) file If you arrived in the guided installation process by following a link from Pixie or from within New Relic, your steps begin here. Select the account and cluster for the install. If needed, select a namespace. Important Currently, Pixie performs best on clusters with up to 100 nodes (exceeding 100 nodes can lead to excessive memory usage and scripts failing to run). Friendly reminder: autoscaling can quickly drive up your node numbers. Click Continue. Select the data you want to gather, observe, and debug, and click Continue. On the Choose install method page, select either Helm or manifest. Helm method Run the provided Helm command on your command line. If you're concerned about the amount of Pixie data you'll ingest, check out strategies for reducing ingest. Helm installs a bundle containing the New Relic infrastructure agent, an integration to gather Prometheus metrics and Kubernetes events, and the Pixie integration. The deployment takes a few minutes to complete. To see the status of the install to the cluster, run kubectl get pods -n newrelic. For general information about installing a Kubernetes integration, see this Helm install info. manifest method Run the provided command in your console, and insert the path to your downloaded manifest. If you're running your Kubernetes cluster in the cloud, see the additional steps in the Kubernetes docs. Click Continue to open the Listening for data page. When you get the message, See your data, click Kubernetes Cluster Explorer to see your cluster. Auto-telemetry with Pixie might restart after installation. This is caused by the auto update feature. If you link the wrong Pixie and New Relic account Contact support to unlink a Pixie account from your New Relic account. Be aware that if you unlink a Pixie account that was created automatically through the guided install, you'll lose access to that Pixie account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 491.92688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "sections": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": "To get up and running with <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>, you start with our guided installation. The guided installation deploys <em>Pixie</em> with New Relic&#x27;s <em>Kubernetes</em> integration on your cluster. You don&#x27;t need to do any further configuration or installation to start using <em>Pixie</em>. If you want to install"
      },
      "id": "6174ca14e7b9d26ba513cd12"
    },
    {
      "sections": [
        "Auto-telemetry with Pixie data and security",
        "Control who has access to Pixie data",
        "Manage auto-update and two-way communication",
        "Helm option",
        "newrelic-manifest.yaml option"
      ],
      "title": "Auto-telemetry with Pixie data and security",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry data",
        "Service monitoring",
        "Kubernetes",
        "eBPF",
        "Pixie data"
      ],
      "external_id": "acec5042b8735d73fa255829401e8708bd6d0595",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/pixie-data-security-overview/",
      "published_at": "2022-01-08T07:41:45Z",
      "updated_at": "2021-10-24T02:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Auto-telemetry with Pixie is New Relic One's integration of Community Cloud for Pixie, a managed version of Pixie open source software. Auto-telemetry with Pixie therefore benefits from Pixie's approach to keeping data secure. The data that Pixie collects is stored entirely within your Kubernetes cluster. This data does not persist outside of your environment, and will never be stored by Community Cloud for Pixie. This means that your sensitive data remains within your environment and control. Community Cloud for Pixie makes queries directly to your Kubernetes cluster to access the data. In order for the query results to be shown in the Community Cloud for Pixie UI, CLI, and API, the data is sent to the client from your cluster using a reverse proxy. Community Cloud for Pixie’s reverse proxy is designed to ensure: Data is ephemeral. It only passes through the Community Cloud for Pixie's cloud proxy in transit. This ensures data locality. Data is encrypted while in transit. Only you are able to read your data. New Relic One fetches and stores data that related to an application's performance. With Auto-telemetry with Pixie, a predefined subset of data persists outside of your cluster. This data is stored in our database, in your selected region. This data persists in order to give you long-term storage, alerting, correlation with additional data, and the ability to use advanced New Relic platform capabilities, such as anomaly detection. The persisted performance metrics include, but are not limited to: Golden metrics (throughput, latency, error rate) for HTTP-based services HTTP transaction data Database transaction data (for MySQL & PostgreSQL) Distributed tracing JVM metrics The data you view on the Live debugging tab comes through Community Cloud for Pixie, and is therefore potentially sensitive. It is not stored by New Relic and is ephemeral and queryable for less than 24 hours. Control who has access to Pixie data If you want to manage which members of your organization can view Pixie data in New Relic One, as well as install and delete Pixie links, you can create a custom role. Note that this option is available only to Enterprise and Pro level customers. For more information, see New Relic's user model. Manage auto-update and two-way communication Pixie maintains an active two-way communication channel from your host system to Community Cloud with Pixie at withpixie.ai. Pixie uses this communication channel to query data, push updates, and retrieve metadata and health checks about Pixie and your Kubernetes cluster. By default, Pixie queries withpixie.ai to check if new updates have been pushed and then automatically installs them if they’re present. To disable auto updates, you must set a flag prior to the install process using either Helm or in the newrelic-manifest.yaml file. To disable automatic updates, choose one: Helm option Add --set pixie-chart.disableAutoUpdate=true to your Helm command. newrelic-manifest.yaml option in your newrelic-manifest.yaml file under the pl-cluster-config section, add PL_DISABLE_AUTO_UPDATE: \"true\" to the data directive. Example: --- apiVersion: v1 data: PL_CUSTOM_ANNOTATIONS: \"\" PL_CUSTOM_LABELS: \"\" PL_DISABLE_AUTO_UPDATE: \"true\" PL_ETCD_OPERATOR_ENABLED: \"false\" PL_MD_ETCD_SERVER: \"https://etcd.newrelic.svc:2379\" PX_MEMORY_LIMIT: \"\" kind: ConfigMap metadata: annotations: creationTimestamp: null labels: name: pl-cluster-config namespace: newrelic --- Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.02283,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data and security",
        "sections": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data and security",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em> data",
        "body": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> is New Relic One&#x27;s integration of Community Cloud for <em>Pixie</em>, a managed version of <em>Pixie</em> open source software. <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> therefore benefits from <em>Pixie</em>&#x27;s approach to keeping data secure. The data that <em>Pixie</em> collects is stored entirely within your <em>Kubernetes</em>"
      },
      "id": "6174ca40e7b9d203d413d744"
    }
  ]
}